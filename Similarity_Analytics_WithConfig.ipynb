{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd583e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write config file\n",
    "import json\n",
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/\"\n",
    "\n",
    "config = {\n",
    "    \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "    \"DataPreprocessing\":{\n",
    "    \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "    \"word_contractions\": {\"enable\": True},\n",
    "    \"lowercase\": {\"enable\": True},\n",
    "    \"remove_htmltag_url\": {\"enable\":True},\n",
    "    \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "    \"remove_num\":{\"enable\":True},\n",
    "    \"remove_multwhitespace\":{\"enable\":True},\n",
    "    \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "    \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "    \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "    \"custom_taxo\":{\"enable\":True,\"remove_taxo\":[\"gio\",\"fields\",\"test\"],\"include_taxo\":[\"test suite execution\",\"kpi metric\"]},\n",
    "    \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "    \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "    },\n",
    "    \"UnsupervisedLearning\":{\n",
    "        \"kmeans_clustering\":{\"enable\":True,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None,\"n_clusters\":None,\"max_n_clusters\":10}\n",
    "        \"lda\":{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\"=None}\n",
    "        \"nmf\"{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\"=None,\"fe_type\":None}\n",
    "    },:\n",
    "    \"SupervisedLearning\":{\n",
    "        \"supervised_lng\":{\"enable\":True,\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"model_type\":None,\"ascend\":None,\"save_path\":None}\n",
    "        \"deep_lng\":{\"enable\":True,\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"hidden_layer_sizes\":(5,5),\"activation\":None,\"solver\":None,\"learning_rate\":None,\"max_iter\":None,\"ascend\":None,\"save_path\":None}\n",
    "    },\n",
    "    \"SimilarityMetrics\":{\n",
    "        \"cosinesimilarity\"{\"enable\":True,\"threshold\":None,\"total_rows\":10,\"base_row\":None,\"ngram_range\":None,\"fe_type\":None,\"ascending\":None}\n",
    "        \"jaccard_similarity\"{\"enable\":True,\"threshold\":None,\"total_rows\":10,\"base_row\":None,\"ascending\":None}\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "with open(json_path+'config.json', 'w') as f:\n",
    "    json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63866130",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74fa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file and call the other functions\n",
    "def main(json_path,out_path):\n",
    "    \n",
    "    import json     \n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(json_path+'config.json') as config_file:\n",
    "        data = json.load(config_file)\n",
    "    \n",
    "    #---------DATA LOADING----------#\n",
    "    dl = data[\"DataLoading\"]\n",
    "    path,start_date,stop_date = dl['path'],dl['start_date'],dl['stop_date']  \n",
    "    df = data_loading(path=path,start_date=start_date,stop_date=stop_date)\n",
    "\n",
    "    #---------DATA PREPROCESSING----------# \n",
    "    #df_manipulation\n",
    "    dm = data[\"DataPreprocessing\"][\"df_manipulation\"]\n",
    "    col_selection,keep,subset = dm['col_selection'],dm['keep'],dm['subset']  \n",
    "    df = df_manipulation(df,col_selection=col_selection,keep=keep,subset=subset)\n",
    "    df_all = df.copy() #id, raw text -> data preprocessing final file\n",
    "    df_out = df.copy() #id, raw text -> ml output\n",
    "    df = df.drop(\"id\",axis=1) #raw text\n",
    "    \n",
    "    #word_contractions\n",
    "    wordcont = data[\"DataPreprocessing\"][\"word_contractions\"][\"enable\"]    \n",
    "    if wordcont:\n",
    "        df = word_contractions(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1) #raw text, raw text after contractions\n",
    "            \n",
    "    #lowercase\n",
    "    lower = data[\"DataPreprocessing\"][\"lowercase\"][\"enable\"]      \n",
    "    if lower:\n",
    "        df = lowercase(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "            \n",
    "   #Remove html tag and url\n",
    "    tagrem = data[\"DataPreprocessing\"][\"remove_htmltag_url\"][\"enable\"] \n",
    "    if tagrem:\n",
    "        df = remove_htmltag_url(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    #Remove irrelevant characters and punctuation\n",
    "    pc = data[\"DataPreprocessing\"][\"remove_irrchar_punc\"]\n",
    "    puncrem,char = pc[\"enable\"],pc[\"char\"] \n",
    "    if puncrem:\n",
    "        df = remove_irrchar_punc(df,char=char)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    #Remove numbers \n",
    "    numrem = data[\"DataPreprocessing\"][\"remove_num\"][\"enable\"]\n",
    "    if numrem:\n",
    "        df = remove_num(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove multiple whitespace\n",
    "    wsrem = data[\"DataPreprocessing\"][\"remove_multwhitespace\"][\"enable\"]\n",
    "    if wsrem:\n",
    "        df = remove_multwhitespace(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stoprem = data[\"DataPreprocessing\"][\"remove_stopwords\"][\"enable\"]\n",
    "    if stoprem:\n",
    "        df = remove_stopwords(df,extra_sw=None,remove_sw=None)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Remove frequent words\n",
    "    fw = data[\"DataPreprocessing\"][\"remove_freqwords\"]\n",
    "    freqrem,n = fw[\"enable\"],fw[\"n\"]\n",
    "    if freqrem:\n",
    "        df = remove_freqwords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    # Remove rare words\n",
    "    rw = data[\"DataPreprocessing\"][\"remove_rarewords\"]\n",
    "    rarerem,n = rw[\"enable\"],rw[\"n\"]    \n",
    "    if rarerem:\n",
    "        df = remove_rarewords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Custom taxonomy\n",
    "    ct = data[\"DataPreprocessing\"][\"custom_taxo\"]\n",
    "    taxo,remove_taxo,include_taxo = ct[\"enable\"], ct[\"remove_taxo\"], ct[\"include_taxo\"]\n",
    "    if taxo:\n",
    "        df = custom_taxo(df,remove_taxo,include_taxo)\n",
    "        df_all = pd.concat([df_all,df],axis=1)     \n",
    "        \n",
    "    # Stemming\n",
    "    st = data[\"DataPreprocessing\"][\"stem_words\"]\n",
    "    stem,stemmer_type = st[\"enable\"],st[\"stemmer_type\"]     \n",
    "    if stem:\n",
    "        df = stem_words(df,stemmer_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)            \n",
    "    \n",
    "    #Lemmatization\n",
    "    lem = data[\"DataPreprocessing\"][\"lemmatize_words\"]\n",
    "    lemma,lemma_type = lem[\"enable\"], lem[\"lemma_type\"]      \n",
    "    if lemma:\n",
    "        df = lemmatize_words(df,lemma_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    display(df_all)    \n",
    "    df_all.to_csv(out_path+\"preprocessed_text.csv\",index=False) #save after data preprocessing   \n",
    "    \n",
    "    #---------------ML module---------------# \n",
    "    #Unsupervised Learning\n",
    "    #k-means clustering \n",
    "    km= data[\"UnsupervisedLearning\"][\"kmeans_clustering\"]\n",
    "    kmeans = km[\"enable\"]\n",
    "    if kmeans:\n",
    "        top_n_terms,ngram_range,fe_type,n_clusters,max_n_clusters= km[\"top_n_terms\"],km[\"ngram_range\"],km[\"fe_type\"],km[\"n_clusters\"],km[\"max_n_clusters\"]        \n",
    "        df_out[\"cluster\"]=kmeans_clustering(column=df,top_n_terms=top_n_terms,ngram_range=ngram_range,fe_type=fe_type,n_clusters=n_clusters,max_n_clusters=max_n_clusters)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(out_path+\"kmeans_output.csv\",index=False)      \n",
    "        \n",
    "    #LDA\n",
    "    lda_m= data[\"UnsupervisedLearning\"][\"lda\"]\n",
    "    LatentDirichletAllocation = lda_m[\"enable\"]\n",
    "    if LatentDirichletAllocation:\n",
    "        n_components,top_n_terms,ngram_range= lda_m[\"n_components\"],lda_m[\"top_n_terms\"],lda_m[\"ngram_range\"]       \n",
    "        df_out[\"topic\"]=lda(column=df,n_components=n_components,top_n_terms=top_n_terms,ngram_range=ngram_range)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(out_path+\"LatentDirichletAllocation_output.csv\",index=False)       \n",
    "        \n",
    "    #NMF Factorization\n",
    "    nmf_m= data[\"UnsupervisedLearning\"][\"nmf\"]\n",
    "    NonNegativeMatrixFactorization = nmf_m[\"enable\"]\n",
    "    if NonNegativeMatrixFactorization:\n",
    "        top_n_terms,ngram_range,fe_type,n_clusters,max_n_clusters= nmf_m[\"n_components\"],nmf_m[\"top_n_terms\"],nmf_m[\"fe_type\"],nmf_m[\"ngram_range\"]\n",
    "        df_out[\"topic\"]=nmf(column=df,n_components=n_components,top_n_terms=top_n_terms,fe_type=fe_type,ngram_range=ngram_range)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(out_path+\"NonNegativeMatrixFactorization_output.csv\",index=False) \n",
    "        \n",
    "    #Supervised Learning\n",
    "    #Machine Learning\n",
    "    sl= data[\"SupervisedLearning\"][\"supervised_lng\"]\n",
    "    SupervisedLearning = sl[\"enable\"]\n",
    "    if SupervisedLearning:\n",
    "        test_size,ngram_range,fe_type,model_type,ascend,save_path= sl[\"test_size\"],sl[\"ngram_range\"],sl[\"fe_type\"],sl[\"model_type\"],sl[\"ascend\"],sl[\"save_path\"]\n",
    "        supervised_lng(X=df,y=df,test_size=test_size,ngram_range=ngram_range,fe_type=fe_type,model_type=model_type,ascend=ascend,save_path=save_path)\n",
    "    \n",
    "    #Deep Learning\n",
    "    dl= data[\"SupervisedLearning\"][\"deep_lng\"]\n",
    "    DeepLearning = dl[\"enable\"]\n",
    "    if DeepLearning:\n",
    "        test_size,ngram_range,fe_type,hidden_layer_sizes,activation,solver,learning_rate,max_iter,ascend,save_path= dl[\"test_size\"],dl[\"ngram_range\"],dl[\"fe_type\"],dl[\"hidden_layer_sizes\"],dl[\"activation\"],dl[\"solver\"],dl[\"learning_rate\"],dl[\"max_iter\"],dl[\"ascend\"],dl[\"save_path\"]\n",
    "        deep_lng(X=df,y=df,test_size=test_size,ngram_range=ngram_range,fe_type=fe_type,hidden_layer_sizes=hidden_layer_sizes,activation=activation,solver=solver,learning_rate=learning_rate,max_iter=max_iter,ascend=ascend,save_path=save_path)\n",
    "        \n",
    "    #Similarity Metrics\n",
    "    #Cosine Similarity\n",
    "    cs= data[\"UnsupervisedLearning\"][\"cosine_similarity\"]\n",
    "    cosinesimilarity = cs[\"enable\"]\n",
    "    if cosinesimilarity:\n",
    "        threshold,total_rows,base_row,ngram_range,fe_type,ascending= cs[\"threshold\"],cs[\"total_rows\"],cs[\"base_row\"],cs[\"ngram_range\"],cs[\"fe_type\"],cs[\"ascending\"]        \n",
    "        cosinesimilarity(column=df,threshold=threshold,total_rows=total_rows,base_row=base_row,ngram_range=ngram_range,fe_type=fe_type,ascending=ascending)\n",
    "        \n",
    "    #Jaccard Similarity\n",
    "    js= data[\"UnsupervisedLearning\"][\"jaccard_similarity\"]\n",
    "    jaccardsimilarity = js[\"enable\"]\n",
    "    if jaccardsimilarity:\n",
    "        threshold,total_rows,base_row,ascending= js[\"threshold\"],js[\"total_rows\"],js[\"base_row\"],js[\"ascending\"]\n",
    "        jaccardsimilarity(column=df,threshold=threshold,total_rows=total_rows,base_row=base_row,ascending=ascending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ee69fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read: ['C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)1_firstSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)3_secondSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-10-11)3_secondSet_1.json']\n",
      "Shape of df before manipulation: (2712, 15)\n",
      "Shape of df after selecting columns: (2712, 2)\n",
      "Number of null values in df:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of null values in df after NA imputation:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of duplicates in the df: 1808\n",
      "Shape of df after manipulation: (904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_contract</th>\n",
       "      <th>description_contract_lower</th>\n",
       "      <th>description_contract_lower_tagrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_taxo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update     from git re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;test suite execution finished before execut...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;i am trying to clone defects from another t...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another   cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;hello,&amp;...</td>\n",
       "      <td>hello, please import time global domain: time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi_metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team thank you for providing kpi metric...</td>\n",
       "      <td>hi   team thank you for providing kpi metric f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;the schedule test suite allow for the use...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule   suite allow for the user to clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team thank you for providing kpi featur...</td>\n",
       "      <td>hi   team thank you for providing kpi feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>converting to enhancement... would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        description  \\\n",
       "0     1308651592  Please provide a way to update GIO fields from...   \n",
       "1     1308671310  <p>Test suite execution finished before execut...   \n",
       "2     1308673361  <p>I am trying to clone defects from another t...   \n",
       "3     1507656633                        Retest some function again.   \n",
       "4     1507656638               enter the support needed at here ...   \n",
       "..           ...                                                ...   \n",
       "899  22012641037  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  22012645565  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  22012704243  <div>The schedule test suite allow for the use...   \n",
       "902  22012765885  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  22013190829  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                                  description_contract  \\\n",
       "0    Please provide a way to update GIO fields from...   \n",
       "1    <p>Test suite execution finished before execut...   \n",
       "2    <p>I am trying to clone defects from another t...   \n",
       "3                          Retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  <div>The schedule test suite allow for the use...   \n",
       "902  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                            description_contract_lower  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    <p>test suite execution finished before execut...   \n",
       "2    <p>i am trying to clone defects from another t...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">hello,&...   \n",
       "900  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "901  <div>the schedule test suite allow for the use...   \n",
       "902  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "903  <p>converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                     description_contract_lower_tagrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  hello, please import time global domain: time ...   \n",
       "900  hi gio team, thank you for providing kpi_metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team, thank you for providing kpi featu...   \n",
       "903  converting to enhancement... would like the ab...   \n",
       "\n",
       "             description_contract_lower_tagrem_puncrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "      description_contract_lower_tagrem_puncrem_numrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem  \\\n",
       "0    please provide a way to update gio fields from...       \n",
       "1    test suite execution finished before executing...       \n",
       "2    i am trying to clone defects from another test...       \n",
       "3                          retest some function again        \n",
       "4                    enter the support needed at here        \n",
       "..                                                 ...       \n",
       "899  hello please import time global domain time kp...       \n",
       "900  hi gio team thank you for providing kpi metric...       \n",
       "901  the schedule test suite allow for the user to ...       \n",
       "902  hi gio team thank you for providing kpi featur...       \n",
       "903  converting to enhancement would like the abili...       \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_taxo  \n",
       "0    please provide a way to update     from git re...           \n",
       "1    test suite execution finished before executing...           \n",
       "2    i am trying to clone defects from another   cy...           \n",
       "3                          retest some function again            \n",
       "4                    enter the support needed at here            \n",
       "..                                                 ...           \n",
       "899  hello please import time global domain time kp...           \n",
       "900  hi   team thank you for providing kpi metric f...           \n",
       "901  the schedule   suite allow for the user to clo...           \n",
       "902  hi   team thank you for providing kpi feature ...           \n",
       "903  converting to enhancement would like the abili...           \n",
       "\n",
       "[904 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The silhouette_score is : 0.017\n",
      "For n_clusters = 3 The silhouette_score is : 0.013\n",
      "For n_clusters = 4 The silhouette_score is : 0.013\n",
      "For n_clusters = 5 The silhouette_score is : 0.011\n",
      "For n_clusters = 6 The silhouette_score is : 0.011\n",
      "For n_clusters = 7 The silhouette_score is : 0.014\n",
      "For n_clusters = 8 The silhouette_score is : 0.015\n",
      "For n_clusters = 9 The silhouette_score is : 0.015\n",
      "For n_clusters = 10 The silhouette_score is : 0.016\n",
      "\n",
      "The optimal number of clusters selected is 2 with silhouette_score of 0.017 \n",
      "\n",
      "Top 10 terms per cluster:\n",
      "Cluster 0:\n",
      "['project', 'dng', 'to', 'program', 'sve', 'requirement', 'create', 'yocto', 'link', 'request']\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      "['the', 'to', 'in', 'is', 'and', 'for', 'cycle', 'not', 'of', 'this']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        description  cluster\n",
       "0     1308651592  Please provide a way to update GIO fields from...        1\n",
       "1     1308671310  <p>Test suite execution finished before execut...        1\n",
       "2     1308673361  <p>I am trying to clone defects from another t...        1\n",
       "3     1507656633                        Retest some function again.        1\n",
       "4     1507656638               enter the support needed at here ...        1\n",
       "..           ...                                                ...      ...\n",
       "899  22012641037  <div><span style=\"font-size: 12.18px;\">Hello,&...        0\n",
       "900  22012645565  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...        1\n",
       "901  22012704243  <div>The schedule test suite allow for the use...        1\n",
       "902  22012765885  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...        1\n",
       "903  22013190829  <p>Converting to enhancement...</p><p><br /></...        1\n",
       "\n",
       "[904 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "main(json_path,out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f2344",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "def data_loading(path,start_date=None,stop_date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    User can choose to \n",
    "    a) Load all json files following the agreed filename format\n",
    "    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and\n",
    "    stop_date must be used together)\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    \n",
    "    start_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files starting from start_date\n",
    "    - None: no start_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files starting from start_date will be loaded\n",
    "    \n",
    "    stop_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files until stop_date\n",
    "    - None: no stop_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files until stop_date will be loaded\n",
    "    '''\n",
    "    from datetime import datetime,timedelta\n",
    "    import pandas as pd\n",
    "    import glob, os, json\n",
    "    import re\n",
    "    \n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    date_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_date == None and stop_date == None :\n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "\n",
    "            pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "            match = re.search(pattern,file)\n",
    "                \n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "        print(\"Files read:\",file_list)                   \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)              \n",
    "                \n",
    "    else:\n",
    "        #convert start and stop string to datetime\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        stop = datetime.strptime(stop_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "        #iterate from start to stop dates by day and store dates in list\n",
    "        while start <= stop:\n",
    "            date_list.append(start)\n",
    "            start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "        #convert datetime objects to string\n",
    "        string_list =[d.strftime(\"%Y-%m-%d\") for d in date_list]\n",
    "#         print(string_list)\n",
    "        \n",
    "        for file in filenames: \n",
    "            \n",
    "            # search agreed file format pattern in the filename\n",
    "            for date in string_list: \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "                match = re.search(pattern,file)\n",
    "                \n",
    "                #if match is found\n",
    "                if match:\n",
    "                    pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "        print(\"Files read:\",file_list)     \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f9fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "def df_manipulation(df,col_selection=None,keep=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep columns in dataframe\n",
    "    2) Data impute: Impute NA rows with empty string\n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe     \n",
    "    col_selection [None/list]: - None [Default]: Keep all columns in dataframe \n",
    "                               - List: List of columns to keep in dataframe                      \n",
    "                                 \n",
    "    keep[None/string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. \n",
    "                      # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                      # - False : Drop all duplicates.                 \n",
    "    subset[list/None]: Subset of columns for identifying duplicates, use None if no column to select\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select column(s) \n",
    "    if col_selection != None:\n",
    "        df = df[col_selection]\n",
    "    \n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with empty string\n",
    "    impute_value = \"\"\n",
    "    df = df.fillna(impute_value)\n",
    "    print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    if keep==None:\n",
    "        keep=\"first\"\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def word_contractions(df):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \"\"\"\n",
    "    import contractions\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda text: \" \".join([contractions.fix(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_contract')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lowercase(df):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    df[dataframe]: input dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    df = df.add_suffix('_lower')\n",
    "        \n",
    "    return df \n",
    "\n",
    "def remove_htmltag_url(df):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    #remove html tag\n",
    "    df = df.applymap(lambda text:BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True))\n",
    "    #remove url\n",
    "    df = df.replace('https?[://%]*\\S+',' ', regex=True) \n",
    "    \n",
    "    df = df.add_suffix('_tagrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_irrchar_punc(df,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex\n",
    "    format.    \n",
    "    params:    \n",
    "    df [dataframe]: input dataframe \n",
    "    characters[string]: input regex of characters to be removed  \n",
    "    \n",
    "    \"\"\"\n",
    "    import re \n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        df = df.replace(char,' ',regex = True)\n",
    "            \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    df = df.replace(r'\\\\+x[\\d\\D][\\d\\D]',' ',regex = True)\n",
    "        \n",
    "    #Remove special characters and punctuation\n",
    "    df = df.replace('[^\\w\\s]',' ',regex = True)\n",
    "    df = df.replace(r'_',' ',regex = True)\n",
    "    \n",
    "    df = df.add_suffix('_puncrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_num(df):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df=df.replace('\\d+',' ', regex=True) \n",
    "    df = df.add_suffix('_numrem')\n",
    "    \n",
    "    return df \n",
    "\n",
    "def remove_multwhitespace(df):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    df = df.add_suffix('_wsrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(df,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        df = df.replace(pattern,' ', regex=True)\n",
    "    \n",
    "    df = df.add_suffix('_stoprem')\n",
    "                   \n",
    "    return df \n",
    "\n",
    "def remove_freqwords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "    df = df.add_suffix('_freqrem')\n",
    "    return df\n",
    "\n",
    "def remove_rarewords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n",
    "    df = df.add_suffix('_rarerem')\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_taxo(df,remove_taxo,include_taxo):\n",
    "    \"\"\"\n",
    "    User provides taxonomy to be removed or remained in the text\n",
    "    params:\n",
    "    df [dataframe]: input dataframe\n",
    "    remove_taxo[list]: list of taxonomy to be removed from text\n",
    "    include_taxo[list]: list of taxonomy to be maintained in text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    def taxo(text,remove_taxo,include_taxo):\n",
    "        for w in remove_taxo:\n",
    "        #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "            if all(phrase not in text for phrase in include_taxo): \n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text) \n",
    "            #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "            else: \n",
    "                if all(w not in phrase for phrase in include_taxo):\n",
    "                    pattern = r'\\b'+w+r'\\b'\n",
    "                    text = re.sub(pattern,' ', text) \n",
    "        return text \n",
    "    \n",
    "    df = df.applymap(lambda text: taxo(text,remove_taxo,include_taxo))    \n",
    "    df = df.add_suffix('_taxo')\n",
    "                \n",
    "    return df    \n",
    "\n",
    "def stem_words(df,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    stemmer_type[None/string]: input stemming method \n",
    "                                - None for Porter Stemmer\n",
    "                                - \"Lancaster\" for Lancaster Stemmer \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    \n",
    "    if stemmer_type == None:\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    df = df.applymap(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_stem')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def lemmatize_words(df,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    lemma_type[None/string]: input lemmatization method\n",
    "                            - None for WordNetLemmatizer\n",
    "                            - \"Spacy\" for Spacy    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    if lemma_type == None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df = df.applymap(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "        \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        df = df.applymap(lambda text: \" \".join([word.lemma_ for word in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        df = df.applymap(lambda s:s.lower() if type(s) == str else s) \n",
    "        \n",
    "    df = df.add_suffix('_lemma')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def feature_extraction(column,ngram_range=None,ascending=None,fe_type=None):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - TF-IDF(default choice) or Bag of words\n",
    "     \n",
    "    params:\n",
    "    column [series/DataFrame]: column selected for feature extraction \n",
    "                        - series: only one column is selected for feature extraction (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for feature extraction (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "                    \n",
    "    if ngram_range == None: #set ngram range as unigram by default\n",
    "        ngram_range=(1,1)\n",
    "        \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == None: #tfidf\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dce514a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsupervised learning\n",
    "\n",
    "#K-means Clustering\n",
    "import pandas as pd\n",
    "def kmeans_clustering(column,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for clustering \n",
    "                        - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"   \n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn import metrics\n",
    "\n",
    "    silhouette_avg_list = []\n",
    "    n_clusters_list = []\n",
    "    dicts = {}\n",
    "    \n",
    "    #call feature extraction function    \n",
    "    ascending = None \n",
    "    X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "    X = X.drop(index='sum')\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "    #user provides the number of clusters        \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "        print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "        \n",
    "            \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "    print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "    terms = vec_type.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "        print(\"\\n\")\n",
    "   \n",
    "               \n",
    "    return labels\n",
    "\n",
    "#Latent Dirichlet Allocation\n",
    "def lda(column,n_components,top_n_terms,ngram_range=None):\n",
    "    \"\"\"\n",
    "    LDA for unsupervised learning. Bag of words is selected for feature extraction\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for lda\n",
    "                        - series: only one column is selected for lda (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for lda (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in the lda_model\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "    #feature extraction\n",
    "    ascending = None\n",
    "    fe_type = \"bagofwords\"\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the LDA class \n",
    "    lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "    lda_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = lda_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "    \n",
    "    return topic_results.argmax(axis=1)\n",
    "\n",
    "#Non-negative Matrix Factorization \n",
    "\n",
    "def nmf(column,n_components,top_n_terms,fe_type,ngram_range=None):\n",
    "    \"\"\"\n",
    "    Non-negative matrix factorization for unsupervised learning.\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for NMF \n",
    "                        - series: only one column is selected for NMF (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for NMF (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in NMF\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import NMF\n",
    "    \n",
    "    #feature extraction\n",
    "    ngram_range = None\n",
    "    ascending = None\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the NMF class \n",
    "    nmf_model = NMF(n_components,random_state=42)\n",
    "    nmf_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = nmf_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervised learning\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import numpy as np\n",
    "def supervised_lng(X,y,test_size,ngram_range=None,fe_type=None,model_type=None,ascend=None,save_path=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Consists of 3 supervised machine learning methods: RandomForest (Default), Naive Bayes(optional, SVM (optional)\n",
    "    \n",
    "    X[series/DataFrame]: column(s) of text for supervised learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    y[series]: target \n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       -[DEFAULT] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for default tfidf method\n",
    "    model_type[None/string]: Choose ML algorithm \n",
    "                            - None (Default algorithm is Random Forest)\n",
    "                            - 'NB'(To choose Naive Bayes as ML algorithm), \n",
    "                            - 'SVM'(To choose Support Vector Machine as ML algorithm)\n",
    "    ascend[True/False/None]:  - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                              - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                              - False(Confusion matrix arranged in descending order of accuracy % per label)  \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)\n",
    "        \n",
    "    \"\"\"\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import svm\n",
    "    \n",
    "    #TRAIN-TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "#         display(X_train)\n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "#         display(X_test)\n",
    "    \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train       \n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    if model_type == None:\n",
    "        #random forest is chosen by default\n",
    "        model = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "    if model_type == \"NB\":\n",
    "        model = MultinomialNB()\n",
    "                   \n",
    "    if model_type == \"SVM\":\n",
    "        model = svm.SVC(random_state = 42)\n",
    "    \n",
    "    model.fit(X_train, y_train) \n",
    "    \n",
    "    #MODEL SAVING\n",
    "    if save_path != None:\n",
    "        joblib.dump(model, path + \"model.joblib\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    # print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    # print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(save_path+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(save_path+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(out_path+\"Confusion_Matrix.csv\",index=False)     \n",
    "\n",
    "#Deep Learning \n",
    "\n",
    "def deep_lng(X,y,test_size,ngram_range,fe_type,hidden_layer_sizes=None,activation=None,solver=None,learning_rate=None,max_iter=None,ascend=None,save_path=None):\n",
    "    \"\"\"\n",
    "     Deep learning method: MultiLayer Perceptron\n",
    "\n",
    "    X[series/DataFrame]: column(s) of text for deep learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])   \n",
    "    y[series]: target\n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    hidden_layer_sizes[tuple],default = (100): To set the number of layers and the number of nodes.\n",
    "                                               Each element in the tuple represents the number of nodes,\n",
    "                                               length of tuple denotes the total number of hidden layers in the network\n",
    "    activation[\"identity\", \"logistic\", \"tanh\",\"relu\"], default=\"relu\": Activation function for the hidden layer.\n",
    "    solver[\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\": The solver for weight optimization.\n",
    "    learning_rate[\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\": Learning rate schedule for weight updates\n",
    "    max_iter[int], default=200: Maximum number of iterations. The solver iterates until convergence or this number of iterations.\n",
    "    ascend [True/False/None]: - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                                 - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                                 - False(Confusion matrix arranged in descending order of accuracy % per label)                            \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)    \n",
    "    \"\"\"\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    #train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "        \n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train\n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    #default hypermarameters\n",
    "    if hidden_layer_sizes == None:\n",
    "        hidden_layer_sizes = (100)\n",
    "    if activation == None:\n",
    "        activation = \"relu\"\n",
    "    if solver == None:\n",
    "        solver = \"adam\"\n",
    "    if learning_rate == None:\n",
    "        learning_rate = \"constant\"\n",
    "    if max_iter == None:\n",
    "        max_iter = 200\n",
    "    \n",
    "    print(\"Hidden layer sizes: \", hidden_layer_sizes,\", Activation: \",activation,\", Solver: \",solver,\", Learning rate: \",learning_rate,\", Max iteration: \",max_iter)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter,verbose = False,random_state=42)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    #MODEL SAVING\n",
    "    if save_path != None:\n",
    "        joblib.dump(model, path + \"mlpmodel.joblib\")\n",
    "        print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    # print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    # print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(save_path+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(save_path+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(out_path+\"Confusion_Matrix.csv\",index=False)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity metrics\n",
    "\n",
    "#Cosine Similarity \n",
    "\n",
    "def cosinesimilarity(column,threshold=None,total_rows = None,base_row=None,ngram_range=None,fe_type=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between rows of texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])  \n",
    "    threshold[None/float]: cut off value for the cosine similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for tfidf\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "                \n",
    "    #feature extraction              \n",
    "    X = feature_extraction(column=column,ngram_range=ngram_range,ascending=None,fe_type=fe_type)[0]\n",
    "    X = X.drop([\"sum\"],axis = 0)\n",
    "    \n",
    "    #Get cosine similarity matrix\n",
    "    similarity_matrix = pd.DataFrame(cosine_similarity(X))\n",
    "    \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "       \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        for base in range(total_rows): \n",
    "            print (\"\")\n",
    "            print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "            \n",
    "            #Create empty df\n",
    "            column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)\n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                \n",
    "                if similarity_matrix.iloc[base,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {'Index':i, 'Similarity Score':round(similarity_matrix.iloc[base,i],4), 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                    if ascending != None:            \n",
    "                        results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)\n",
    "                        \n",
    "            display(results)\n",
    "#             print(results['Similarity Score'].mean())\n",
    "           \n",
    "\n",
    "    if base_row !=None: #fix base_row index for comparison with all indexes\n",
    "        print (\"Using index \" + str(base_row) + \" as base:\") #fix one index as base\n",
    "        \n",
    "        #Create empty df\n",
    "        column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)\n",
    "        \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            if similarity_matrix.iloc[base_row,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {'Index':i, 'Similarity Score':round(similarity_matrix.iloc[base_row,i],4), 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "                    \n",
    "        results.to_csv(out_path+\"Cosine_Similarity.csv\",index=False)  \n",
    "\n",
    "#Jaccard Similarity \n",
    "\n",
    "def jaccard_similarity(column,threshold=None,total_rows = None,base_row=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the jaccard similarity between texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:\n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]]) \n",
    "    threshold[None/float]: cut off value for the jaccard similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"     \n",
    "            \n",
    "    #jaccard score computation\n",
    "    def get_jaccard_sim(str1, str2):        \n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "       \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "        \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        for base in range(total_rows): \n",
    "            print (\"\")\n",
    "            print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "            \n",
    "            #Create empty df\n",
    "            column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                jac_score =  round(get_jaccard_sim(column.iloc[base],column.iloc[i]),4)\n",
    "                if jac_score > threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "                    \n",
    "            display(results) \n",
    "        \n",
    "    if base_row != None: #fix base_row index for comparison with all indexes\n",
    "       \n",
    "        print (\"Using index \" + str(base_row) + \" as base row:\") #fix one index as base_row\n",
    "        #Create empty df\n",
    "        column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            jac_score = round(get_jaccard_sim(column.iloc[base_row],column.iloc[i]),4)\n",
    "            if jac_score >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "            if ascending != None:            \n",
    "                results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "\n",
    "        results.to_csv(out_path+\"Jaccard_Similarity.csv\",index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 1: DL -> DP -> Supervised learning\n",
    "# config 2: DL -> DP -> Unsupervised learning\n",
    "# config 3: DL -> DP -> Similarity metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8335bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file\n",
    "# import json\n",
    "# json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "\n",
    "# with open(json_path+'config.json') as config_file:\n",
    "#     data = json.load(config_file)\n",
    "\n",
    "# path = data[\"DataLoading\"]['path']\n",
    "# start_date = data[\"DataLoading\"]['start_date']\n",
    "# stop_date = data[\"DataLoading\"]['stop_date']\n",
    "# print(path)\n",
    "# print(start_date)\n",
    "# print(stop_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11973277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     method #1\n",
    "#     [\"title\",\"Desc\",\"comment\"]\n",
    "    \n",
    "#     wordcont = data[\"df_manipulation\"][\"word_contractions\"]\n",
    "#     if wordcont:\n",
    "#         df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "#         df[\"desc_cont\"] = [word_contractions(text) for text in df[\"desc\"]]\n",
    "#         df[\"comment_cont\"] = [word_contractions(text) for text in df[\"comment\"]\n",
    "    \n",
    "#     method #2\n",
    "#     df (title,desc,comments) -> df(title,desc,comments) + df (title_cont,desc_cont,comments_cont)\n",
    "#     df (title,desc,comments) -> concat df(title,desc,comments) [df1]\n",
    "#                               columnbind(df,df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = {\n",
    "#     \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "#     \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "#     \"word_contractions\": {\"enable\": True},\n",
    "#     \"lowercase\": {\"enable\": True},\n",
    "#     \"remove_htmltag_url\": {\"enable\":True},\n",
    "#     \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "#     \"remove_num\":{\"enable\":True},\n",
    "#     \"remove_multwhitespace\":{\"enable\":True},\n",
    "#     \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "#     \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "#     \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "#     \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "#     \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
