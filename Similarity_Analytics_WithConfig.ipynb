{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cd583e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write config file\n",
    "import json\n",
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/\"\n",
    "\n",
    "config = {\n",
    "    \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "    \"DataPreprocessing\":{\n",
    "    \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "    \"word_contractions\": {\"enable\": True},\n",
    "    \"lowercase\": {\"enable\": True},\n",
    "    \"remove_htmltag_url\": {\"enable\":True},\n",
    "    \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "    \"remove_num\":{\"enable\":True},\n",
    "    \"remove_multwhitespace\":{\"enable\":True},\n",
    "    \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "    \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "    \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "    \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "    \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "\n",
    "                        }\n",
    "\n",
    "}\n",
    "\n",
    "with open(json_path+'config.json', 'w') as f:\n",
    "    json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "74fa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file and call the other functions\n",
    "def main(json_path):\n",
    "    \n",
    "    import json     \n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(json_path+'config.json') as config_file:\n",
    "        data = json.load(config_file)\n",
    "    \n",
    "    #---------DATA LOADING----------#\n",
    "    path = data[\"DataLoading\"]['path']  \n",
    "    start_date = data[\"DataLoading\"]['start_date']\n",
    "    stop_date = data[\"DataLoading\"]['stop_date']\n",
    "    df = data_loading(path=path,start_date=start_date,stop_date=stop_date)\n",
    "\n",
    "    #---------DATA PREPROCESSING----------# \n",
    "    #df_manipulation\n",
    "    col_selection = data[\"DataPreprocessing\"][\"df_manipulation\"]['col_selection'] \n",
    "    keep = data[\"DataPreprocessing\"][\"df_manipulation\"]['keep'] \n",
    "    subset = data[\"DataPreprocessing\"][\"df_manipulation\"]['subset']\n",
    "    df = df_manipulation(df,col_selection=col_selection,keep=keep,subset=subset)\n",
    "    df_all = df.copy()\n",
    "    df = df.drop(\"id\",axis=1)\n",
    "    \n",
    "    #word_contractions\n",
    "    wordcont = data[\"DataPreprocessing\"][\"word_contractions\"][\"enable\"]    \n",
    "    if wordcont:\n",
    "        df = word_contractions(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "            \n",
    "    #lowercase\n",
    "    lower = data[\"DataPreprocessing\"][\"lowercase\"][\"enable\"]      \n",
    "    if lower:\n",
    "        df = lowercase(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "            \n",
    "   #Remove html tag and url\n",
    "    tagrem = data[\"DataPreprocessing\"][\"remove_htmltag_url\"][\"enable\"] \n",
    "    if tagrem:\n",
    "        df = remove_htmltag_url(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    #Remove irrelevant characters and punctuation\n",
    "    puncrem = data[\"DataPreprocessing\"][\"remove_irrchar_punc\"][\"enable\"] \n",
    "    char = data[\"DataPreprocessing\"][\"remove_irrchar_punc\"][\"char\"]\n",
    "    if puncrem:\n",
    "        df = remove_irrchar_punc(df,char=char)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "        \n",
    "    #Remove numbers \n",
    "    numrem = data[\"DataPreprocessing\"][\"remove_num\"][\"enable\"]\n",
    "    if numrem:\n",
    "        df = remove_num(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove multiple whitespace\n",
    "    wsrem = data[\"DataPreprocessing\"][\"remove_multwhitespace\"][\"enable\"]\n",
    "    if wsrem:\n",
    "        df = remove_multwhitespace(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stoprem = data[\"DataPreprocessing\"][\"remove_stopwords\"][\"enable\"]\n",
    "    if stoprem:\n",
    "        df = remove_stopwords(df,extra_sw=None,remove_sw=None)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "        \n",
    "    # Remove frequent words\n",
    "    freqrem = data[\"DataPreprocessing\"][\"remove_freqwords\"][\"enable\"]\n",
    "    n= data[\"DataPreprocessing\"][\"remove_freqwords\"][\"n\"]\n",
    "    if freqrem:\n",
    "        df = remove_freqwords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    # Remove rare words\n",
    "    rarerem = data[\"DataPreprocessing\"][\"remove_rarewords\"][\"enable\"]\n",
    "    n= data[\"DataPreprocessing\"][\"remove_rarewords\"][\"n\"]\n",
    "    if rarerem:\n",
    "        df = remove_freqwords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "            \n",
    "    # Stemming\n",
    "    stem = data[\"DataPreprocessing\"][\"stem_words\"][\"enable\"]\n",
    "    stemmer_type = data[\"DataPreprocessing\"][\"stem_words\"][\"stemmer_type\"]    \n",
    "    if stem:\n",
    "        df = stem_words(df,stemmer_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        display(df_all)      \n",
    "    \n",
    "    #Lemmatization\n",
    "    lemma = data[\"DataPreprocessing\"][\"lemmatize_words\"][\"enable\"]\n",
    "    lemma_type = data[\"DataPreprocessing\"][\"lemmatize_words\"][\"lemma_type\"]    \n",
    "    if lemma:\n",
    "        df = lemmatize_words(df,lemma_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        display(df_all)      \n",
    "    \n",
    "    #---------ML MODULE----------# \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6ee69fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read: ['C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)1_firstSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)3_secondSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-10-11)3_secondSet_1.json']\n",
      "Shape of df before manipulation: (2712, 15)\n",
      "Shape of df after selecting columns: (2712, 2)\n",
      "Number of null values in df:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of null values in df after NA imputation:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of duplicates in the df: 1808\n",
      "Shape of df after manipulation: (904, 2)\n",
      "Frequent words that are removed: {('cases', 290), ('project', 246), ('test', 1632), ('pass', 297), ('link', 230), ('gio', 1209), ('result', 246), ('case', 334), ('cycle', 420), ('execution', 255)}\n",
      "Frequent words that are removed: {('client', 218), ('group', 170), ('run', 194), ('user', 224), ('suite', 211), ('please', 167), ('report', 182), ('c', 177), ('new', 184), ('id', 166)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_contract</th>\n",
       "      <th>description_contract_lower</th>\n",
       "      <th>description_contract_lower_tagrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_freqrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_freqrem_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide   way   update gio fields   git...</td>\n",
       "      <td>please provide way update fields git repo file...</td>\n",
       "      <td>provide way update fields git repo files means...</td>\n",
       "      <td>provide way update field git repo file mean wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;test suite execution finished before execut...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished   executing   te...</td>\n",
       "      <td>suite finished executing tests error observed ...</td>\n",
       "      <td>finished executing tests error observed log al...</td>\n",
       "      <td>finished executing test error observed log als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;i am trying to clone defects from another t...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>trying   clone defects   another test cycl...</td>\n",
       "      <td>trying clone defects another get message cloni...</td>\n",
       "      <td>trying clone defects another get message cloni...</td>\n",
       "      <td>trying clone defect another get message clonin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest   function</td>\n",
       "      <td>retest function</td>\n",
       "      <td>retest function</td>\n",
       "      <td>retest function</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter   support needed</td>\n",
       "      <td>enter support needed</td>\n",
       "      <td>enter support needed</td>\n",
       "      <td>enter support needed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;hello,&amp;...</td>\n",
       "      <td>hello, please import time global domain: time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "      <td>hello import time global domain time kpis ehl ...</td>\n",
       "      <td>hello import time global domain time kpis ehl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi_metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team thank you for providing kpi metric...</td>\n",
       "      <td>hi gio team thank     providing kpi metric fea...</td>\n",
       "      <td>hi team thank providing kpi metric feature sto...</td>\n",
       "      <td>hi team thank providing kpi metric feature sto...</td>\n",
       "      <td>hi team thank providing kpi metric feature sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;the schedule test suite allow for the use...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>schedule test suite allow     user   clone t...</td>\n",
       "      <td>schedule suite allow user clone suites recipes...</td>\n",
       "      <td>schedule allow clone suites recipes already fi...</td>\n",
       "      <td>schedule allow clone suite recipe already fill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team thank you for providing kpi featur...</td>\n",
       "      <td>hi gio team thank     providing kpi feature   ...</td>\n",
       "      <td>hi team thank providing kpi feature plot kpi m...</td>\n",
       "      <td>hi team thank providing kpi feature plot kpi m...</td>\n",
       "      <td>hi team thank providing kpi feature plot kpi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>converting to enhancement... would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "      <td>converting   enhancement would like   ability ...</td>\n",
       "      <td>converting enhancement would like ability down...</td>\n",
       "      <td>converting enhancement would like ability down...</td>\n",
       "      <td>converting enhancement would like ability down...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        description  \\\n",
       "0     1308651592  Please provide a way to update GIO fields from...   \n",
       "1     1308671310  <p>Test suite execution finished before execut...   \n",
       "2     1308673361  <p>I am trying to clone defects from another t...   \n",
       "3     1507656633                        Retest some function again.   \n",
       "4     1507656638               enter the support needed at here ...   \n",
       "..           ...                                                ...   \n",
       "899  22012641037  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  22012645565  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  22012704243  <div>The schedule test suite allow for the use...   \n",
       "902  22012765885  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  22013190829  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                                  description_contract  \\\n",
       "0    Please provide a way to update GIO fields from...   \n",
       "1    <p>Test suite execution finished before execut...   \n",
       "2    <p>I am trying to clone defects from another t...   \n",
       "3                          Retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  <div>The schedule test suite allow for the use...   \n",
       "902  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                            description_contract_lower  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    <p>test suite execution finished before execut...   \n",
       "2    <p>i am trying to clone defects from another t...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">hello,&...   \n",
       "900  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "901  <div>the schedule test suite allow for the use...   \n",
       "902  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "903  <p>converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                     description_contract_lower_tagrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  hello, please import time global domain: time ...   \n",
       "900  hi gio team, thank you for providing kpi_metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team, thank you for providing kpi featu...   \n",
       "903  converting to enhancement... would like the ab...   \n",
       "\n",
       "             description_contract_lower_tagrem_puncrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "      description_contract_lower_tagrem_puncrem_numrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem  \\\n",
       "0    please provide a way to update gio fields from...       \n",
       "1    test suite execution finished before executing...       \n",
       "2    i am trying to clone defects from another test...       \n",
       "3                          retest some function again        \n",
       "4                    enter the support needed at here        \n",
       "..                                                 ...       \n",
       "899  hello please import time global domain time kp...       \n",
       "900  hi gio team thank you for providing kpi metric...       \n",
       "901  the schedule test suite allow for the user to ...       \n",
       "902  hi gio team thank you for providing kpi featur...       \n",
       "903  converting to enhancement would like the abili...       \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem  \\\n",
       "0    please provide   way   update gio fields   git...               \n",
       "1    test suite execution finished   executing   te...               \n",
       "2        trying   clone defects   another test cycl...               \n",
       "3                                 retest   function                  \n",
       "4                          enter   support needed                    \n",
       "..                                                 ...               \n",
       "899  hello please import time global domain time kp...               \n",
       "900  hi gio team thank     providing kpi metric fea...               \n",
       "901    schedule test suite allow     user   clone t...               \n",
       "902  hi gio team thank     providing kpi feature   ...               \n",
       "903  converting   enhancement would like   ability ...               \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem  \\\n",
       "0    please provide way update fields git repo file...                       \n",
       "1    suite finished executing tests error observed ...                       \n",
       "2    trying clone defects another get message cloni...                       \n",
       "3                                      retest function                       \n",
       "4                                 enter support needed                       \n",
       "..                                                 ...                       \n",
       "899  hello please import time global domain time kp...                       \n",
       "900  hi team thank providing kpi metric feature sto...                       \n",
       "901  schedule suite allow user clone suites recipes...                       \n",
       "902  hi team thank providing kpi feature plot kpi m...                       \n",
       "903  converting enhancement would like ability down...                       \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_freqrem  \\\n",
       "0    provide way update fields git repo files means...                               \n",
       "1    finished executing tests error observed log al...                               \n",
       "2    trying clone defects another get message cloni...                               \n",
       "3                                      retest function                               \n",
       "4                                 enter support needed                               \n",
       "..                                                 ...                               \n",
       "899  hello import time global domain time kpis ehl ...                               \n",
       "900  hi team thank providing kpi metric feature sto...                               \n",
       "901  schedule allow clone suites recipes already fi...                               \n",
       "902  hi team thank providing kpi feature plot kpi m...                               \n",
       "903  converting enhancement would like ability down...                               \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_freqrem_lemma  \n",
       "0    provide way update field git repo file mean wh...                                    \n",
       "1    finished executing test error observed log als...                                    \n",
       "2    trying clone defect another get message clonin...                                    \n",
       "3                                      retest function                                    \n",
       "4                                 enter support needed                                    \n",
       "..                                                 ...                                    \n",
       "899  hello import time global domain time kpis ehl ...                                    \n",
       "900  hi team thank providing kpi metric feature sto...                                    \n",
       "901  schedule allow clone suite recipe already fill...                                    \n",
       "902  hi team thank providing kpi feature plot kpi m...                                    \n",
       "903  converting enhancement would like ability down...                                    \n",
       "\n",
       "[904 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "main(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f2344",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f1ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(path,start_date=None,stop_date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    User can choose to \n",
    "    a) Load all json files following the agreed filename format\n",
    "    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and\n",
    "    stop_date must be used together)\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    \n",
    "    start_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files starting from start_date\n",
    "    - None: no start_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files starting from start_date will be loaded\n",
    "    \n",
    "    stop_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files until stop_date\n",
    "    - None: no stop_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files until stop_date will be loaded\n",
    "    '''\n",
    "    from datetime import datetime,timedelta\n",
    "    import pandas as pd\n",
    "    import glob, os, json\n",
    "    import re\n",
    "    \n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    date_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_date == None and stop_date == None :\n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "\n",
    "            pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "            match = re.search(pattern,file)\n",
    "                \n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "        print(\"Files read:\",file_list)                   \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)              \n",
    "                \n",
    "    else:\n",
    "        #convert start and stop string to datetime\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        stop = datetime.strptime(stop_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "        #iterate from start to stop dates by day and store dates in list\n",
    "        while start <= stop:\n",
    "            date_list.append(start)\n",
    "            start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "        #convert datetime objects to string\n",
    "        string_list =[d.strftime(\"%Y-%m-%d\") for d in date_list]\n",
    "#         print(string_list)\n",
    "        \n",
    "        for file in filenames: \n",
    "            \n",
    "            # search agreed file format pattern in the filename\n",
    "            for date in string_list: \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "                match = re.search(pattern,file)\n",
    "                \n",
    "                #if match is found\n",
    "                if match:\n",
    "                    pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "        print(\"Files read:\",file_list)     \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f9fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(df,col_selection=None,keep=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep columns in dataframe\n",
    "    2) Data impute: Impute NA rows with empty string\n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe     \n",
    "    col_selection [None/list]: - None [Default]: Keep all columns in dataframe \n",
    "                               - List: List of columns to keep in dataframe                      \n",
    "                                 \n",
    "    keep[None/string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. \n",
    "                      # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                      # - False : Drop all duplicates.                 \n",
    "    subset[list/None]: Subset of columns for identifying duplicates, use None if no column to select\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select column(s) \n",
    "    if col_selection != None:\n",
    "        df = df[col_selection]\n",
    "    \n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with empty string\n",
    "    impute_value = \"\"\n",
    "    df = df.fillna(impute_value)\n",
    "    print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    if keep==None:\n",
    "        keep=\"first\"\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dce514a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_contractions(df):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \"\"\"\n",
    "    import contractions\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda text: \" \".join([contractions.fix(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_contract')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ea172ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(df):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    df[dataframe]: input dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    df = df.add_suffix('_lower')\n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "560c20f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_htmltag_url(df):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    #remove html tag\n",
    "    df = df.applymap(lambda text:BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True))\n",
    "    #remove url\n",
    "    df = df.replace('https?[://%]*\\S+',' ', regex=True) \n",
    "    \n",
    "    df = df.add_suffix('_tagrem')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "87b9aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrchar_punc(df,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex\n",
    "    format.    \n",
    "    params:    \n",
    "    df [dataframe]: input dataframe \n",
    "    characters[string]: input regex of characters to be removed  \n",
    "    \n",
    "    \"\"\"\n",
    "    import re \n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        df = df.replace(char,' ',regex = True)\n",
    "            \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    df = df.replace(r'\\\\+x[\\d\\D][\\d\\D]',' ',regex = True)\n",
    "        \n",
    "    #Remove special characters and punctuation\n",
    "    df = df.replace('[^\\w\\s]',' ',regex = True)\n",
    "    df = df.replace(r'_',' ',regex = True)\n",
    "    \n",
    "    df = df.add_suffix('_puncrem')\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8fbcfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(df):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df=df.replace('\\d+',' ', regex=True) \n",
    "    df = df.add_suffix('_numrem')\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "48e9a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multwhitespace(df):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    df = df.add_suffix('_wsrem')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "173e8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(df,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        df = df.replace(pattern,' ', regex=True)\n",
    "    \n",
    "    df = df.add_suffix('_stoprem')\n",
    "                   \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5b78e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freqwords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "    df = df.add_suffix('_freqrem')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "fcfa2136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rarewords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n",
    "    df = df.add_suffix('_rarerem')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "457a04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(df,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    stemmer_type[None/string]: input stemming method \n",
    "                                - None for Porter Stemmer\n",
    "                                - \"Lancaster\" for Lancaster Stemmer \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    \n",
    "    if stemmer_type == None:\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    df = df.applymap(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_stem')\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "20abedc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(df,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    lemma_type[None/string]: input lemmatization method\n",
    "                            - None for WordNetLemmatizer\n",
    "                            - \"Spacy\" for Spacy    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    if lemma_type == None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df = df.applymap(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "        \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        df = df.applymap(lambda text: \" \".join([word.lemma_ for word in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        df = df.applymap(lambda s:s.lower() if type(s) == str else s) \n",
    "        \n",
    "    df = df.add_suffix('_lemma')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 1: DL -> DP -> Supervised learning\n",
    "# config 2: DL -> DP -> Unsupervised learning\n",
    "# config 3: DL -> DP -> Similarity metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8335bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file\n",
    "# import json\n",
    "# json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "\n",
    "# with open(json_path+'config.json') as config_file:\n",
    "#     data = json.load(config_file)\n",
    "\n",
    "# path = data[\"DataLoading\"]['path']\n",
    "# start_date = data[\"DataLoading\"]['start_date']\n",
    "# stop_date = data[\"DataLoading\"]['stop_date']\n",
    "# print(path)\n",
    "# print(start_date)\n",
    "# print(stop_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11973277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     method #1\n",
    "#     [\"title\",\"Desc\",\"comment\"]\n",
    "    \n",
    "#     wordcont = data[\"df_manipulation\"][\"word_contractions\"]\n",
    "#     if wordcont:\n",
    "#         df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "#         df[\"desc_cont\"] = [word_contractions(text) for text in df[\"desc\"]]\n",
    "#         df[\"comment_cont\"] = [word_contractions(text) for text in df[\"comment\"]\n",
    "    \n",
    "#     method #2\n",
    "#     df (title,desc,comments) -> df(title,desc,comments) + df (title_cont,desc_cont,comments_cont)\n",
    "#     df (title,desc,comments) -> concat df(title,desc,comments) [df1]\n",
    "#                               columnbind(df,df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = {\n",
    "#     \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "#     \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "#     \"word_contractions\": {\"enable\": True},\n",
    "#     \"lowercase\": {\"enable\": True},\n",
    "#     \"remove_htmltag_url\": {\"enable\":True},\n",
    "#     \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "#     \"remove_num\":{\"enable\":True},\n",
    "#     \"remove_multwhitespace\":{\"enable\":True},\n",
    "#     \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "#     \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "#     \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "#     \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "#     \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
