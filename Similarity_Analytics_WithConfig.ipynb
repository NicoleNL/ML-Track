{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f25a5fc4",
   "metadata": {},
   "source": [
    "### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd583e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write config file \n",
    "# import json\n",
    "# json_path ='C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/'\n",
    "# # path = json_path + \"VICE_JSON/\"\n",
    "# path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/\"\n",
    "# config = {   \n",
    "#     \"Email\":\"abc@intel.com\", #for logging purpose\n",
    "#     \"user_outpath\" :\"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/user_outpath/\", \n",
    "#     \"log_path\" : \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/user_outpath/\",\n",
    "#     \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "#     \"DataPreprocessing\":{    \n",
    "#     \"df_manipulation\": {\"how\":None,\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "#     \"target\":{\"enable\":False,\"column\":\"problem_area\"}, #for supervised learning only\n",
    "#     \"word_contractions\": {\"enable\": True},\n",
    "#     \"lowercase\": {\"enable\": True},\n",
    "#     \"remove_htmltag_url\": {\"enable\":True},\n",
    "#     \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "#     \"remove_num\":{\"enable\":True},\n",
    "#     \"remove_multwhitespace\":{\"enable\":True},\n",
    "#     \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "#     \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "#     \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "#     \"custom_taxo\":{\"enable\":True,\"remove_taxo\":r'test \\w+',\"include_taxo\":[\"test suite execution\"]},\n",
    "#     \"stem_words\":{\"enable\":True,\"stemmer_type\":None},\n",
    "#     \"lemmatize_words\":{\"enable\":False,\"lemma_type\":None}\n",
    "#     },\n",
    "#    \"UnsupervisedLearning\":{\n",
    "#         \"Output\": {\"User\":False,\"ELK\":True},\n",
    "#         \"elk_outpath\" : \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/elk_outpath/\",\n",
    "#         \"kmeans_clustering\":{\"enable\":True,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None,\"n_clusters\":None,\"max_n_clusters\":5},\n",
    "#         \"lda\":{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\":None},\n",
    "#         \"nmf\":{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None}\n",
    "#     },   \n",
    "#     \"SupervisedLearning\":{\n",
    "#         \"supervised_lng\":{\"enable\":False,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"model_type\":None,\"ascend\":None},\n",
    "#         \"deep_lng\":{\"enable\":False,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"hidden_layer_sizes\":(5),\"activation\":None,\"solver\":None,\"learning_rate\":None,\"max_iter\":None,\"ascend\":None}\n",
    "#     },\n",
    "#     \"SimilarityMetrics\":{\n",
    "#         \"cosinesimilarity\":{\"enable\":True,\"threshold\":0,\"total_rows\":10,\"base_row\":None,\"ngram_range\":None,\"fe_type\":None,\"ascending\":None},\n",
    "#         \"jaccardsimilarity\":{\"enable\":True,\"threshold\":0,\"total_rows\":10,\"base_row\":None,\"ascending\":None}\n",
    "#     }\n",
    "\n",
    "# }\n",
    "\n",
    "# with open(json_path+'config.json', 'w') as f:\n",
    "#     json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edea8ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write config file \n",
    "import json\n",
    "json_path ='C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/'\n",
    "# path = json_path + \"VICE_JSON/\"\n",
    "path = \"/nfs/png/home/nchong/Data/\"\n",
    "config = {   \n",
    "    \"Email\":\"abc@intel.com\", #for logging purpose\n",
    "    \"user_outpath\" :\"/nfs/png/home/nchong/\", \n",
    "    \"log_path\" : \"/nfs/png/home/nchong/\",\n",
    "    \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "    \"DataPreprocessing\":{    \n",
    "    \"df_manipulation\": {\"how\":None,\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "    \"target\":{\"enable\":False,\"column\":\"problem_area\"}, #for supervised learning only\n",
    "    \"word_contractions\": {\"enable\": True},\n",
    "    \"lowercase\": {\"enable\": True},\n",
    "    \"remove_htmltag_url\": {\"enable\":True},\n",
    "    \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "    \"remove_num\":{\"enable\":True},\n",
    "    \"remove_multwhitespace\":{\"enable\":True},\n",
    "    \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "    \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "    \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "    \"custom_taxo\":{\"enable\":True,\"remove_taxo\":r'test \\w+',\"include_taxo\":[\"test suite execution\"]},\n",
    "    \"stem_words\":{\"enable\":True,\"stemmer_type\":None},\n",
    "    \"lemmatize_words\":{\"enable\":False,\"lemma_type\":None}\n",
    "    },\n",
    "   \"UnsupervisedLearning\":{\n",
    "        \"Output\": {\"User\":False,\"ELK\":True},\n",
    "        \"elk_outpath\" : \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/elk_outpath/\",\n",
    "        \"kmeans_clustering\":{\"enable\":True,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None,\"n_clusters\":None,\"max_n_clusters\":5},\n",
    "        \"lda\":{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\":None},\n",
    "        \"nmf\":{\"enable\":True,\"n_components\":5,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None}\n",
    "    },   \n",
    "    \"SupervisedLearning\":{\n",
    "        \"supervised_lng\":{\"enable\":False,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"model_type\":None,\"ascend\":None},\n",
    "        \"deep_lng\":{\"enable\":False,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"hidden_layer_sizes\":(5),\"activation\":None,\"solver\":None,\"learning_rate\":None,\"max_iter\":None,\"ascend\":None}\n",
    "    },\n",
    "    \"SimilarityMetrics\":{\n",
    "        \"cosinesimilarity\":{\"enable\":True,\"threshold\":0,\"total_rows\":10,\"base_row\":None,\"ngram_range\":None,\"fe_type\":None,\"ascending\":None},\n",
    "        \"jaccardsimilarity\":{\"enable\":True,\"threshold\":0,\"total_rows\":10,\"base_row\":None,\"ascending\":None}\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "with open(json_path+'config.json', 'w') as f:\n",
    "    json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0afad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "/nfs/png/home/nchong/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fea505f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read: ['C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/VICE_JSON/(2021-08-25)1_VICE_1.json']\n",
      "Shape of df before manipulation: (2078, 20)\n",
      "Shape of df after selecting columns: (2078, 3)\n",
      "Number of null values in df:\n",
      " id              0\n",
      "description     0\n",
      "problem_area    0\n",
      "dtype: int64\n",
      "NA is imputed with empty string\n",
      "Number of null values in df after NA imputation:\n",
      " id              0\n",
      "description     0\n",
      "problem_area    0\n",
      "dtype: int64\n",
      "Number of duplicates in the df: 0\n",
      "Shape of df after manipulation: (2078, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>problem_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22062762</td>\n",
       "      <td>When executing the Get Port Bandwidth command ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220152061</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220421258</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220634430</td>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220634437</td>\n",
       "      <td>&lt;p&gt;Trying Recipe sent by Tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>14010346691</td>\n",
       "      <td>&lt;div style=\"direction:ltr\"&gt;\\n\\n&lt;table border=\"...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>14010350584</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&amp;nbsp; 4_fpga_usbxcoe_top_tgph1p...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>14010378525</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This test is...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>14010414934</td>\n",
       "      <td>&lt;p&gt;SVOS Kernel (BUSTER) :&amp;nbsp;4.19.60&lt;br /&gt;&lt;/...</td>\n",
       "      <td>noise.validation_tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>14010451524</td>\n",
       "      <td>&lt;p&gt;Seeing an issue where the controller enters...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                        description  \\\n",
       "0        22062762  When executing the Get Port Bandwidth command ...   \n",
       "1       220152061  <pre style=\"word-wrap: break-word; white-space...   \n",
       "2       220421258  <pre style=\"word-wrap: break-word; white-space...   \n",
       "3       220634430  <p style=\"font-size: 12.18px;\"><span style=\"fo...   \n",
       "4       220634437  <p>Trying Recipe sent by Tamir</p><p>pci_confi...   \n",
       "...           ...                                                ...   \n",
       "2073  14010346691  <div style=\"direction:ltr\">\\n\\n<table border=\"...   \n",
       "2074  14010350584  <p>FPGA Image:&nbsp; 4_fpga_usbxcoe_top_tgph1p...   \n",
       "2075  14010378525  <p>FPGA Image:</p><p><br /></p><p>This test is...   \n",
       "2076  14010414934  <p>SVOS Kernel (BUSTER) :&nbsp;4.19.60<br /></...   \n",
       "2077  14010451524  <p>Seeing an issue where the controller enters...   \n",
       "\n",
       "                problem_area  \n",
       "0                             \n",
       "1                             \n",
       "2     noise.cannot_reproduce  \n",
       "3     noise.cannot_reproduce  \n",
       "4            noise.non-issue  \n",
       "...                      ...  \n",
       "2073                          \n",
       "2074                          \n",
       "2075                          \n",
       "2076  noise.validation_tools  \n",
       "2077                   other  \n",
       "\n",
       "[2078 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When executing the Get Port Bandwidth command ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;p&gt;Trying Recipe sent by Tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>&lt;div style=\"direction:ltr\"&gt;\\n\\n&lt;table border=\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>&lt;p&gt;FPGA Image:&amp;nbsp; 4_fpga_usbxcoe_top_tgph1p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>&lt;p&gt;FPGA Image:&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This test is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>&lt;p&gt;SVOS Kernel (BUSTER) :&amp;nbsp;4.19.60&lt;br /&gt;&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>&lt;p&gt;Seeing an issue where the controller enters...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description\n",
       "0     When executing the Get Port Bandwidth command ...\n",
       "1     <pre style=\"word-wrap: break-word; white-space...\n",
       "2     <pre style=\"word-wrap: break-word; white-space...\n",
       "3     <p style=\"font-size: 12.18px;\"><span style=\"fo...\n",
       "4     <p>Trying Recipe sent by Tamir</p><p>pci_confi...\n",
       "...                                                 ...\n",
       "2073  <div style=\"direction:ltr\">\\n\\n<table border=\"...\n",
       "2074  <p>FPGA Image:&nbsp; 4_fpga_usbxcoe_top_tgph1p...\n",
       "2075  <p>FPGA Image:</p><p><br /></p><p>This test is...\n",
       "2076  <p>SVOS Kernel (BUSTER) :&nbsp;4.19.60<br /></...\n",
       "2077  <p>Seeing an issue where the controller enters...\n",
       "\n",
       "[2078 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>noise.validation_tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                problem_area\n",
       "0                           \n",
       "1                           \n",
       "2     noise.cannot_reproduce\n",
       "3     noise.cannot_reproduce\n",
       "4            noise.non-issue\n",
       "...                      ...\n",
       "2073                        \n",
       "2074                        \n",
       "2075                        \n",
       "2076  noise.validation_tools\n",
       "2077                   other\n",
       "\n",
       "[2078 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent words that are removed: {('u', 6887), ('b', 7516), ('h', 6339), ('p', 7860), ('c', 6867), ('v', 6994), ('r', 6319), ('f', 7379), ('x', 12523), ('n', 6332)}\n",
      "Rare words that are removed: {('conform', 1), ('narrowing', 1), ('environmental', 1), ('reimaged', 1), ('tnolfpsresponsetimeout', 1), ('responsive', 1), ('resolving', 1), ('involved', 1), ('sbrt', 1), ('elapses', 1)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_area</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>execut get port bandwidth command msle ssp tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>current see dpm issu unit usb port usb port mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>basin fall use server deriv cpu kbp pch design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>xhci debug devic fail cnl platform fail seed c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noise.non-issue</td>\n",
       "      <td>tri recip sent tamir pci config regist access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td></td>\n",
       "      <td>hardwar asu prime z hap snp daughter card sier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td></td>\n",
       "      <td>fpga imag fpga usbxco top tgph visa ww e e upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td></td>\n",
       "      <td>fpga imag intermitt pass fail suspect issu des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>noise.validation_tools</td>\n",
       "      <td>svo kernel buster svf version svf modul svo de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>other</td>\n",
       "      <td>see issu control enter partial fulli hung non ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                problem_area  \\\n",
       "0                              \n",
       "1                              \n",
       "2     noise.cannot_reproduce   \n",
       "3     noise.cannot_reproduce   \n",
       "4            noise.non-issue   \n",
       "...                      ...   \n",
       "2073                           \n",
       "2074                           \n",
       "2075                           \n",
       "2076  noise.validation_tools   \n",
       "2077                   other   \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo_stem  \n",
       "0     execut get port bandwidth command msle ssp tra...                                    \n",
       "1     current see dpm issu unit usb port usb port mi...                                    \n",
       "2     basin fall use server deriv cpu kbp pch design...                                    \n",
       "3     xhci debug devic fail cnl platform fail seed c...                                    \n",
       "4     tri recip sent tamir pci config regist access ...                                    \n",
       "...                                                 ...                                    \n",
       "2073  hardwar asu prime z hap snp daughter card sier...                                    \n",
       "2074  fpga imag fpga usbxco top tgph visa ww e e upd...                                    \n",
       "2075  fpga imag intermitt pass fail suspect issu des...                                    \n",
       "2076  svo kernel buster svf version svf modul svo de...                                    \n",
       "2077  see issu control enter partial fulli hung non ...                                    \n",
       "\n",
       "[2078 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_area</th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_cont</th>\n",
       "      <th>description_cont_lower</th>\n",
       "      <th>description_cont_lower_tagrem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo</th>\n",
       "      <th>description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>22062762</td>\n",
       "      <td>When executing the Get Port Bandwidth command ...</td>\n",
       "      <td>When executing the Get Port Bandwidth command ...</td>\n",
       "      <td>when executing the get port bandwidth command ...</td>\n",
       "      <td>when executing the get port bandwidth command ...</td>\n",
       "      <td>when executing the get port bandwidth command ...</td>\n",
       "      <td>when executing the get port bandwidth command ...</td>\n",
       "      <td>when executing the get port bandwidth command ...</td>\n",
       "      <td>executing   get port bandwidth command   msl...</td>\n",
       "      <td>executing get port bandwidth command msle ssp ...</td>\n",
       "      <td>executing get port bandwidth command msle ssp ...</td>\n",
       "      <td>executing get port bandwidth command msle ssp ...</td>\n",
       "      <td>execut get port bandwidth command msle ssp tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>220152061</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>currently we are seeing 50dpm on this issue, w...</td>\n",
       "      <td>currently we are seeing 50dpm on this issue  w...</td>\n",
       "      <td>currently we are seeing  dpm on this issue  we...</td>\n",
       "      <td>currently we are seeing dpm on this issue we h...</td>\n",
       "      <td>currently     seeing dpm     issue     unit   ...</td>\n",
       "      <td>currently seeing dpm issue unit usb port usb p...</td>\n",
       "      <td>currently seeing dpm issue unit usb port usb p...</td>\n",
       "      <td>currently seeing dpm issue unit usb port usb p...</td>\n",
       "      <td>current see dpm issu unit usb port usb port mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>220421258</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>basin falls uses a server derived cpu with a k...</td>\n",
       "      <td>basin falls uses a server derived cpu with a k...</td>\n",
       "      <td>basin falls uses a server derived cpu with a k...</td>\n",
       "      <td>basin falls uses a server derived cpu with a k...</td>\n",
       "      <td>basin falls uses   server derived cpu     kbp ...</td>\n",
       "      <td>basin falls uses server derived cpu kbp pch de...</td>\n",
       "      <td>basin falls uses server derived cpu kbp pch de...</td>\n",
       "      <td>basin falls uses server derived cpu kbp pch de...</td>\n",
       "      <td>basin fall use server deriv cpu kbp pch design...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>220634430</td>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "      <td>xhci_debug_device test failed on cnl b0 platfo...</td>\n",
       "      <td>xhci debug device test failed on cnl b0 platfo...</td>\n",
       "      <td>xhci debug device test failed on cnl b  platfo...</td>\n",
       "      <td>xhci debug device test failed on cnl b platfor...</td>\n",
       "      <td>xhci debug device test failed   cnl b platform...</td>\n",
       "      <td>xhci debug device test failed cnl platform fai...</td>\n",
       "      <td>xhci debug device test failed cnl platform fai...</td>\n",
       "      <td>xhci debug device   failed cnl platform failin...</td>\n",
       "      <td>xhci debug devic fail cnl platform fail seed c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>noise.non-issue</td>\n",
       "      <td>220634437</td>\n",
       "      <td>&lt;p&gt;Trying Recipe sent by Tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "      <td>&lt;p&gt;Trying Recipe sent by Tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "      <td>&lt;p&gt;trying recipe sent by tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "      <td>trying recipe sent by tamir pci_config_registe...</td>\n",
       "      <td>trying recipe sent by tamir pci config registe...</td>\n",
       "      <td>trying recipe sent by tamir pci config registe...</td>\n",
       "      <td>trying recipe sent by tamir pci config registe...</td>\n",
       "      <td>trying recipe sent   tamir pci config register...</td>\n",
       "      <td>trying recipe sent tamir pci config registers ...</td>\n",
       "      <td>trying recipe sent tamir pci config registers ...</td>\n",
       "      <td>trying recipe sent tamir pci config registers ...</td>\n",
       "      <td>tri recip sent tamir pci config regist access ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td></td>\n",
       "      <td>14010346691</td>\n",
       "      <td>&lt;div style=\"direction:ltr\"&gt;\\n\\n&lt;table border=\"...</td>\n",
       "      <td>&lt;div style=\"direction:ltr\"&gt; &lt;table border=\"1\" ...</td>\n",
       "      <td>&lt;div style=\"direction:ltr\"&gt; &lt;table border=\"1\" ...</td>\n",
       "      <td>hardware asus prime z390-a haps80 – 2 snps dau...</td>\n",
       "      <td>hardware asus prime z390 a haps80   2 snps dau...</td>\n",
       "      <td>hardware asus prime z  a haps      snps daught...</td>\n",
       "      <td>hardware asus prime z a haps snps daughter car...</td>\n",
       "      <td>hardware asus prime z   haps snps daughter car...</td>\n",
       "      <td>hardware asus prime z haps snps daughter card ...</td>\n",
       "      <td>hardware asus prime z haps snps daughter card ...</td>\n",
       "      <td>hardware asus prime z haps snps daughter card ...</td>\n",
       "      <td>hardwar asu prime z hap snp daughter card sier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td></td>\n",
       "      <td>14010350584</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&amp;nbsp; 4_fpga_usbxcoe_top_tgph1p...</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&amp;nbsp; 4_fpga_usbxcoe_top_tgph1p...</td>\n",
       "      <td>&lt;p&gt;fpga image:&amp;nbsp; 4_fpga_usbxcoe_top_tgph1p...</td>\n",
       "      <td>fpga image:  4_fpga_usbxcoe_top_tgph1p0_visa_1...</td>\n",
       "      <td>fpga image   4 fpga usbxcoe top tgph1p0 visa 1...</td>\n",
       "      <td>fpga image     fpga usbxcoe top tgph p  visa  ...</td>\n",
       "      <td>fpga image   fpga usbxcoe top tgph p visa ww b...</td>\n",
       "      <td>fpga image   fpga usbxcoe top tgph p visa ww b...</td>\n",
       "      <td>fpga image fpga usbxcoe top tgph visa ww e e u...</td>\n",
       "      <td>fpga image fpga usbxcoe top tgph visa ww e e u...</td>\n",
       "      <td>fpga image fpga usbxcoe top tgph visa ww e e u...</td>\n",
       "      <td>fpga imag fpga usbxco top tgph visa ww e e upd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td></td>\n",
       "      <td>14010378525</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This test is...</td>\n",
       "      <td>&lt;p&gt;FPGA Image:&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;This test is...</td>\n",
       "      <td>&lt;p&gt;fpga image:&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;this test is...</td>\n",
       "      <td>fpga image: this test is intermittently passin...</td>\n",
       "      <td>fpga image  this test is intermittently passin...</td>\n",
       "      <td>fpga image  this test is intermittently passin...</td>\n",
       "      <td>fpga image this test is intermittently passing...</td>\n",
       "      <td>fpga image   test   intermittently passing   f...</td>\n",
       "      <td>fpga image test intermittently passing failing...</td>\n",
       "      <td>fpga image test intermittently passing failing...</td>\n",
       "      <td>fpga image   intermittently passing failing su...</td>\n",
       "      <td>fpga imag intermitt pass fail suspect issu des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>noise.validation_tools</td>\n",
       "      <td>14010414934</td>\n",
       "      <td>&lt;p&gt;SVOS Kernel (BUSTER) :&amp;nbsp;4.19.60&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;SVOS Kernel (BUSTER) :&amp;nbsp;4.19.60&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;svos kernel (buster) :&amp;nbsp;4.19.60&lt;br /&gt;&lt;/...</td>\n",
       "      <td>svos kernel (buster) : 4.19.60 svfs version: s...</td>\n",
       "      <td>svos kernel  buster    4 19 60 svfs version  s...</td>\n",
       "      <td>svos kernel  buster          svfs version  svf...</td>\n",
       "      <td>svos kernel buster   svfs version  svfs module...</td>\n",
       "      <td>svos kernel buster   svfs version  svfs module...</td>\n",
       "      <td>svos kernel buster svfs version svfs modules s...</td>\n",
       "      <td>svos kernel buster svfs version svfs modules s...</td>\n",
       "      <td>svos kernel buster svfs version svfs modules s...</td>\n",
       "      <td>svo kernel buster svf version svf modul svo de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>other</td>\n",
       "      <td>14010451524</td>\n",
       "      <td>&lt;p&gt;Seeing an issue where the controller enters...</td>\n",
       "      <td>&lt;p&gt;Seeing an issue where the controller enters...</td>\n",
       "      <td>&lt;p&gt;seeing an issue where the controller enters...</td>\n",
       "      <td>seeing an issue where the controller enters a ...</td>\n",
       "      <td>seeing an issue where the controller enters a ...</td>\n",
       "      <td>seeing an issue where the controller enters a ...</td>\n",
       "      <td>seeing an issue where the controller enters a ...</td>\n",
       "      <td>seeing   issue     controller enters   partial...</td>\n",
       "      <td>seeing issue controller enters partially fully...</td>\n",
       "      <td>seeing issue controller enters partially fully...</td>\n",
       "      <td>seeing issue controller enters partially fully...</td>\n",
       "      <td>see issu control enter partial fulli hung non ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2078 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                problem_area           id  \\\n",
       "0                                22062762   \n",
       "1                               220152061   \n",
       "2     noise.cannot_reproduce    220421258   \n",
       "3     noise.cannot_reproduce    220634430   \n",
       "4            noise.non-issue    220634437   \n",
       "...                      ...          ...   \n",
       "2073                          14010346691   \n",
       "2074                          14010350584   \n",
       "2075                          14010378525   \n",
       "2076  noise.validation_tools  14010414934   \n",
       "2077                   other  14010451524   \n",
       "\n",
       "                                            description  \\\n",
       "0     When executing the Get Port Bandwidth command ...   \n",
       "1     <pre style=\"word-wrap: break-word; white-space...   \n",
       "2     <pre style=\"word-wrap: break-word; white-space...   \n",
       "3     <p style=\"font-size: 12.18px;\"><span style=\"fo...   \n",
       "4     <p>Trying Recipe sent by Tamir</p><p>pci_confi...   \n",
       "...                                                 ...   \n",
       "2073  <div style=\"direction:ltr\">\\n\\n<table border=\"...   \n",
       "2074  <p>FPGA Image:&nbsp; 4_fpga_usbxcoe_top_tgph1p...   \n",
       "2075  <p>FPGA Image:</p><p><br /></p><p>This test is...   \n",
       "2076  <p>SVOS Kernel (BUSTER) :&nbsp;4.19.60<br /></...   \n",
       "2077  <p>Seeing an issue where the controller enters...   \n",
       "\n",
       "                                       description_cont  \\\n",
       "0     When executing the Get Port Bandwidth command ...   \n",
       "1     <pre style=\"word-wrap: break-word; white-space...   \n",
       "2     <pre style=\"word-wrap: break-word; white-space...   \n",
       "3     <p style=\"font-size: 12.18px;\"><span style=\"fo...   \n",
       "4     <p>Trying Recipe sent by Tamir</p><p>pci_confi...   \n",
       "...                                                 ...   \n",
       "2073  <div style=\"direction:ltr\"> <table border=\"1\" ...   \n",
       "2074  <p>FPGA Image:&nbsp; 4_fpga_usbxcoe_top_tgph1p...   \n",
       "2075  <p>FPGA Image:</p><p><br /></p><p>This test is...   \n",
       "2076  <p>SVOS Kernel (BUSTER) :&nbsp;4.19.60<br /></...   \n",
       "2077  <p>Seeing an issue where the controller enters...   \n",
       "\n",
       "                                 description_cont_lower  \\\n",
       "0     when executing the get port bandwidth command ...   \n",
       "1     <pre style=\"word-wrap: break-word; white-space...   \n",
       "2     <pre style=\"word-wrap: break-word; white-space...   \n",
       "3     <p style=\"font-size: 12.18px;\"><span style=\"fo...   \n",
       "4     <p>trying recipe sent by tamir</p><p>pci_confi...   \n",
       "...                                                 ...   \n",
       "2073  <div style=\"direction:ltr\"> <table border=\"1\" ...   \n",
       "2074  <p>fpga image:&nbsp; 4_fpga_usbxcoe_top_tgph1p...   \n",
       "2075  <p>fpga image:</p><p><br /></p><p>this test is...   \n",
       "2076  <p>svos kernel (buster) :&nbsp;4.19.60<br /></...   \n",
       "2077  <p>seeing an issue where the controller enters...   \n",
       "\n",
       "                          description_cont_lower_tagrem  \\\n",
       "0     when executing the get port bandwidth command ...   \n",
       "1     currently we are seeing 50dpm on this issue, w...   \n",
       "2     basin falls uses a server derived cpu with a k...   \n",
       "3     xhci_debug_device test failed on cnl b0 platfo...   \n",
       "4     trying recipe sent by tamir pci_config_registe...   \n",
       "...                                                 ...   \n",
       "2073  hardware asus prime z390-a haps80 – 2 snps dau...   \n",
       "2074  fpga image:  4_fpga_usbxcoe_top_tgph1p0_visa_1...   \n",
       "2075  fpga image: this test is intermittently passin...   \n",
       "2076  svos kernel (buster) : 4.19.60 svfs version: s...   \n",
       "2077  seeing an issue where the controller enters a ...   \n",
       "\n",
       "                  description_cont_lower_tagrem_puncrem  \\\n",
       "0     when executing the get port bandwidth command ...   \n",
       "1     currently we are seeing 50dpm on this issue  w...   \n",
       "2     basin falls uses a server derived cpu with a k...   \n",
       "3     xhci debug device test failed on cnl b0 platfo...   \n",
       "4     trying recipe sent by tamir pci config registe...   \n",
       "...                                                 ...   \n",
       "2073  hardware asus prime z390 a haps80   2 snps dau...   \n",
       "2074  fpga image   4 fpga usbxcoe top tgph1p0 visa 1...   \n",
       "2075  fpga image  this test is intermittently passin...   \n",
       "2076  svos kernel  buster    4 19 60 svfs version  s...   \n",
       "2077  seeing an issue where the controller enters a ...   \n",
       "\n",
       "           description_cont_lower_tagrem_puncrem_numrem  \\\n",
       "0     when executing the get port bandwidth command ...   \n",
       "1     currently we are seeing  dpm on this issue  we...   \n",
       "2     basin falls uses a server derived cpu with a k...   \n",
       "3     xhci debug device test failed on cnl b  platfo...   \n",
       "4     trying recipe sent by tamir pci config registe...   \n",
       "...                                                 ...   \n",
       "2073  hardware asus prime z  a haps      snps daught...   \n",
       "2074  fpga image     fpga usbxcoe top tgph p  visa  ...   \n",
       "2075  fpga image  this test is intermittently passin...   \n",
       "2076  svos kernel  buster          svfs version  svf...   \n",
       "2077  seeing an issue where the controller enters a ...   \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem  \\\n",
       "0     when executing the get port bandwidth command ...   \n",
       "1     currently we are seeing dpm on this issue we h...   \n",
       "2     basin falls uses a server derived cpu with a k...   \n",
       "3     xhci debug device test failed on cnl b platfor...   \n",
       "4     trying recipe sent by tamir pci config registe...   \n",
       "...                                                 ...   \n",
       "2073  hardware asus prime z a haps snps daughter car...   \n",
       "2074  fpga image   fpga usbxcoe top tgph p visa ww b...   \n",
       "2075  fpga image this test is intermittently passing...   \n",
       "2076  svos kernel buster   svfs version  svfs module...   \n",
       "2077  seeing an issue where the controller enters a ...   \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem  \\\n",
       "0       executing   get port bandwidth command   msl...           \n",
       "1     currently     seeing dpm     issue     unit   ...           \n",
       "2     basin falls uses   server derived cpu     kbp ...           \n",
       "3     xhci debug device test failed   cnl b platform...           \n",
       "4     trying recipe sent   tamir pci config register...           \n",
       "...                                                 ...           \n",
       "2073  hardware asus prime z   haps snps daughter car...           \n",
       "2074  fpga image   fpga usbxcoe top tgph p visa ww b...           \n",
       "2075  fpga image   test   intermittently passing   f...           \n",
       "2076  svos kernel buster   svfs version  svfs module...           \n",
       "2077  seeing   issue     controller enters   partial...           \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem  \\\n",
       "0     executing get port bandwidth command msle ssp ...                   \n",
       "1     currently seeing dpm issue unit usb port usb p...                   \n",
       "2     basin falls uses server derived cpu kbp pch de...                   \n",
       "3     xhci debug device test failed cnl platform fai...                   \n",
       "4     trying recipe sent tamir pci config registers ...                   \n",
       "...                                                 ...                   \n",
       "2073  hardware asus prime z haps snps daughter card ...                   \n",
       "2074  fpga image fpga usbxcoe top tgph visa ww e e u...                   \n",
       "2075  fpga image test intermittently passing failing...                   \n",
       "2076  svos kernel buster svfs version svfs modules s...                   \n",
       "2077  seeing issue controller enters partially fully...                   \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem  \\\n",
       "0     executing get port bandwidth command msle ssp ...                           \n",
       "1     currently seeing dpm issue unit usb port usb p...                           \n",
       "2     basin falls uses server derived cpu kbp pch de...                           \n",
       "3     xhci debug device test failed cnl platform fai...                           \n",
       "4     trying recipe sent tamir pci config registers ...                           \n",
       "...                                                 ...                           \n",
       "2073  hardware asus prime z haps snps daughter card ...                           \n",
       "2074  fpga image fpga usbxcoe top tgph visa ww e e u...                           \n",
       "2075  fpga image test intermittently passing failing...                           \n",
       "2076  svos kernel buster svfs version svfs modules s...                           \n",
       "2077  seeing issue controller enters partially fully...                           \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo  \\\n",
       "0     executing get port bandwidth command msle ssp ...                                \n",
       "1     currently seeing dpm issue unit usb port usb p...                                \n",
       "2     basin falls uses server derived cpu kbp pch de...                                \n",
       "3     xhci debug device   failed cnl platform failin...                                \n",
       "4     trying recipe sent tamir pci config registers ...                                \n",
       "...                                                 ...                                \n",
       "2073  hardware asus prime z haps snps daughter card ...                                \n",
       "2074  fpga image fpga usbxcoe top tgph visa ww e e u...                                \n",
       "2075  fpga image   intermittently passing failing su...                                \n",
       "2076  svos kernel buster svfs version svfs modules s...                                \n",
       "2077  seeing issue controller enters partially fully...                                \n",
       "\n",
       "     description_cont_lower_tagrem_puncrem_numrem_wsrem_stoprem_freqrem_rarerem_taxo_stem  \n",
       "0     execut get port bandwidth command msle ssp tra...                                    \n",
       "1     current see dpm issu unit usb port usb port mi...                                    \n",
       "2     basin fall use server deriv cpu kbp pch design...                                    \n",
       "3     xhci debug devic fail cnl platform fail seed c...                                    \n",
       "4     tri recip sent tamir pci config regist access ...                                    \n",
       "...                                                 ...                                    \n",
       "2073  hardwar asu prime z hap snp daughter card sier...                                    \n",
       "2074  fpga imag fpga usbxco top tgph visa ww e e upd...                                    \n",
       "2075  fpga imag intermitt pass fail suspect issu des...                                    \n",
       "2076  svo kernel buster svf version svf modul svo de...                                    \n",
       "2077  see issu control enter partial fulli hung non ...                                    \n",
       "\n",
       "[2078 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target distribution: noise.validation_tools       249\n",
      "noise.duplicated_sighting    178\n",
      "bug.logic.xhci               164\n",
      "noise.test_content           126\n",
      "noise.non-issue              112\n",
      "                            ... \n",
      "bug.logic.modphy               1\n",
      "environment.board.crb          1\n",
      "bug.logic.exi                  1\n",
      "bug.logic.usb3                 1\n",
      "bug.logic.pmc                  1\n",
      "Name: problem_area, Length: 70, dtype: int64\n",
      "Train-test split completed with 70.0 - 30.0 split in train-test\n",
      "Shape of X_train is: (1454, 1)\n",
      "Shape of X_test is: (624, 1)\n",
      "Shape of y_train is: (1454,)\n",
      "Shape of y_test is: (624,)\n",
      "Shape of X_train after feature extraction: (1454, 246096)\n",
      "Shape of X_test after feature extraction: (624, 246096)\n",
      "Model saved!\n",
      "Overall accuracy achieved is 19.23%\n",
      "Classification report:\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                                                0.19      0.12      0.15        33\n",
      "                  bug.circuit.modphy-usb3       0.00      0.00      0.00         2\n",
      "                         bug.circuit.usb2       0.00      0.00      0.00         1\n",
      "                           bug.duplicated       0.00      0.00      0.00        27\n",
      "                    bug.duplicated_unique       0.00      0.00      0.00         4\n",
      "                            bug.logic.exi       0.00      0.00      0.00         1\n",
      "                    bug.logic.integration       0.00      0.00      0.00         7\n",
      "                    bug.logic.modphy-usb3       0.00      0.00      0.00         6\n",
      "                           bug.logic.usb3       0.00      0.00      0.00         1\n",
      "                           bug.logic.xhci       0.19      0.25      0.21        57\n",
      "     collateral.documenation.mphy_setting       0.00      0.00      0.00         9\n",
      "       collateral.documentation.bios_spec       0.00      0.00      0.00        13\n",
      "          collateral.documentation.c-spec       0.00      0.00      0.00         9\n",
      "         collateral.documentation.erratum       0.00      0.00      0.00         1\n",
      "             collateral.documentation.rdl       0.00      0.00      0.00         5\n",
      "    collateral.documentation.visa_sigfile       0.00      0.00      0.00         1\n",
      "             collateral.intel.driver.xhci       0.00      0.00      0.00         4\n",
      "                 collateral.intel.fw.bios       0.14      0.14      0.14        14\n",
      "             collateral.intel.fw.cse_csme       0.00      0.00      0.00         1\n",
      "                collateral.intel.fw.dekel       0.62      0.24      0.34        21\n",
      "                  collateral.intel.fw.ish       0.00      0.00      0.00         2\n",
      "                  collateral.intel.fw.pmc       0.00      0.00      0.00         2\n",
      "             collateral.intel.fw.tcss_iom       0.00      0.00      0.00         7\n",
      "                      collateral.intel.hw       0.00      0.00      0.00         1\n",
      "        environment.3rd_party.application       0.00      0.00      0.00         8\n",
      "             environment.3rd_party.device       0.06      0.08      0.07        13\n",
      "                 environment.3rd_party.os       0.00      0.00      0.00         2\n",
      "                    environment.board.rvp       0.00      0.00      0.00         6\n",
      "                     environment.board.sv       0.00      0.00      0.00         2\n",
      "                         environment.fpga       0.27      0.20      0.23        15\n",
      "                 environment.fpga.backend       0.50      0.20      0.29         5\n",
      "                   environment.fpga.infra       0.00      0.00      0.00         3\n",
      "             environment.fpga.integration       0.00      0.00      0.00        15\n",
      "                environment.fpga.platform       0.00      0.00      0.00         3\n",
      "                    environment.fpga.tool       0.00      0.00      0.00         2\n",
      "                     environment.rtl.fpga       0.00      0.00      0.00        10\n",
      "                      environment.rtl.sle       0.00      0.00      0.00         7\n",
      "                environment.rtl.test_chip       0.00      0.00      0.00         2\n",
      "                          environment.sle       0.00      0.00      0.00        14\n",
      "                    environment.sle.infra       0.00      0.00      0.00         6\n",
      "              environment.sle.integration       0.00      0.00      0.00         0\n",
      "                     environment.sle.xtor       0.60      0.22      0.32        27\n",
      "environment.validation_requirement_change       0.00      0.00      0.00         1\n",
      "                   noise.cannot_reproduce       0.05      0.05      0.05        22\n",
      "                noise.duplicated_sighting       0.13      0.15      0.14        52\n",
      "                        noise.new_request       0.00      0.00      0.00         1\n",
      "                          noise.non-issue       0.11      0.22      0.15        23\n",
      "                       noise.test_content       0.12      0.12      0.12        40\n",
      "           noise.test_content.integration       0.50      0.12      0.20         8\n",
      "                         noise.user_error       0.00      0.00      0.00         3\n",
      "                   noise.validation_tools       0.21      0.72      0.32        74\n",
      "           noise.validation_tools.maestro       0.61      0.58      0.59        19\n",
      "             noise.validation_tools.sv_fw       0.00      0.00      0.00         1\n",
      "                                    other       0.00      0.00      0.00        11\n",
      "\n",
      "                                 accuracy                           0.19       624\n",
      "                                macro avg       0.08      0.06      0.06       624\n",
      "                             weighted avg       0.16      0.19      0.15       624\n",
      "\n",
      "Target distribution: noise.validation_tools       249\n",
      "noise.duplicated_sighting    178\n",
      "bug.logic.xhci               164\n",
      "noise.test_content           126\n",
      "noise.non-issue              112\n",
      "                            ... \n",
      "bug.logic.modphy               1\n",
      "environment.board.crb          1\n",
      "bug.logic.exi                  1\n",
      "bug.logic.usb3                 1\n",
      "bug.logic.pmc                  1\n",
      "Name: problem_area, Length: 70, dtype: int64\n",
      "Train-test split completed with 70.0 - 30.0 split in train-test\n",
      "Shape of X_train is: (1454, 1)\n",
      "Shape of X_test is: (624, 1)\n",
      "Shape of y_train is: (1454,)\n",
      "Shape of y_test is: (624,)\n",
      "Shape of X_train after feature extraction: (1454, 246096)\n",
      "Shape of X_test after feature extraction: (624, 246096)\n",
      "Hidden layer sizes:  5 , Activation:  relu , Solver:  adam , Learning rate:  constant , Max iteration:  200\n",
      "Model saved!\n",
      "Overall accuracy achieved is 16.03%\n",
      "Classification report:\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                                                0.16      0.09      0.12        33\n",
      "                  bug.circuit.modphy-usb3       0.00      0.00      0.00         2\n",
      "                         bug.circuit.usb2       0.00      0.00      0.00         1\n",
      "                           bug.duplicated       0.00      0.00      0.00        27\n",
      "                    bug.duplicated_unique       0.00      0.00      0.00         4\n",
      "                            bug.logic.exi       0.00      0.00      0.00         1\n",
      "                    bug.logic.integration       0.00      0.00      0.00         7\n",
      "                    bug.logic.modphy-usb3       0.00      0.00      0.00         6\n",
      "                           bug.logic.usb3       0.00      0.00      0.00         1\n",
      "                           bug.logic.xhci       0.14      0.23      0.17        57\n",
      "     collateral.documenation.mphy_setting       0.14      0.11      0.12         9\n",
      "       collateral.documentation.bios_spec       0.00      0.00      0.00        13\n",
      "          collateral.documentation.c-spec       0.00      0.00      0.00         9\n",
      "         collateral.documentation.erratum       0.00      0.00      0.00         1\n",
      "             collateral.documentation.rdl       0.00      0.00      0.00         5\n",
      "    collateral.documentation.visa_sigfile       0.00      0.00      0.00         1\n",
      "             collateral.intel.driver.xhci       0.00      0.00      0.00         4\n",
      "                 collateral.intel.fw.bios       0.08      0.07      0.07        14\n",
      "             collateral.intel.fw.cse_csme       0.00      0.00      0.00         1\n",
      "                collateral.intel.fw.dekel       0.25      0.10      0.14        21\n",
      "                  collateral.intel.fw.ish       0.00      0.00      0.00         2\n",
      "                  collateral.intel.fw.pmc       0.00      0.00      0.00         2\n",
      "             collateral.intel.fw.tcss_iom       0.00      0.00      0.00         7\n",
      "                      collateral.intel.hw       0.00      0.00      0.00         1\n",
      "        environment.3rd_party.application       0.00      0.00      0.00         8\n",
      "             environment.3rd_party.device       0.00      0.00      0.00        13\n",
      "                 environment.3rd_party.os       0.00      0.00      0.00         2\n",
      "                    environment.board.rvp       0.00      0.00      0.00         6\n",
      "                     environment.board.sv       0.00      0.00      0.00         2\n",
      "                         environment.fpga       0.50      0.13      0.21        15\n",
      "                 environment.fpga.backend       0.00      0.00      0.00         5\n",
      "                   environment.fpga.infra       0.00      0.00      0.00         3\n",
      "             environment.fpga.integration       0.00      0.00      0.00        15\n",
      "                environment.fpga.platform       0.00      0.00      0.00         3\n",
      "                    environment.fpga.tool       0.00      0.00      0.00         2\n",
      "                     environment.rtl.fpga       0.00      0.00      0.00        10\n",
      "                      environment.rtl.sle       0.00      0.00      0.00         7\n",
      "                environment.rtl.test_chip       0.00      0.00      0.00         2\n",
      "                          environment.sle       0.00      0.00      0.00        14\n",
      "                    environment.sle.infra       0.00      0.00      0.00         6\n",
      "                     environment.sle.xtor       0.55      0.22      0.32        27\n",
      "environment.validation_requirement_change       0.00      0.00      0.00         1\n",
      "                   noise.cannot_reproduce       0.00      0.00      0.00        22\n",
      "                noise.duplicated_sighting       0.11      0.33      0.17        52\n",
      "                        noise.new_request       0.00      0.00      0.00         1\n",
      "                          noise.non-issue       0.07      0.09      0.08        23\n",
      "                       noise.test_content       0.11      0.10      0.10        40\n",
      "           noise.test_content.integration       0.00      0.00      0.00         8\n",
      "                         noise.user_error       0.00      0.00      0.00         3\n",
      "                   noise.validation_tools       0.20      0.59      0.30        74\n",
      "           noise.validation_tools.maestro       0.36      0.26      0.30        19\n",
      "             noise.validation_tools.sv_fw       0.00      0.00      0.00         1\n",
      "                                    other       0.00      0.00      0.00        11\n",
      "\n",
      "                                 accuracy                           0.16       624\n",
      "                                macro avg       0.05      0.04      0.04       624\n",
      "                             weighted avg       0.12      0.16      0.12       624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nchong\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "json_path ='C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/'\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f1e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7512d5d9",
   "metadata": {},
   "source": [
    "### Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74fa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file and call the other functions\n",
    "def main(config_file):\n",
    "    \n",
    "    import json     \n",
    "    import pandas as pd  \n",
    "    \n",
    "    \n",
    "    #---------DATA LOADING----------#           \n",
    "    with open(config_file) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    user_outpath = data[\"user_outpath\"]    \n",
    "    log_path = data[\"log_path\"]\n",
    "        \n",
    "    dl = data[\"DataLoading\"]\n",
    "    path,start_date,stop_date = dl['path'],dl['start_date'],dl['stop_date']  \n",
    "    df = data_loading(path=path,start_date=start_date,stop_date=stop_date)\n",
    "    \n",
    "    #---------DATA PREPROCESSING----------# \n",
    "    #df_manipulation\n",
    "    dm = data[\"DataPreprocessing\"][\"df_manipulation\"]\n",
    "    how,col_selection,keep,subset = dm['how'],dm['col_selection'],dm['keep'],dm['subset']  \n",
    "    df = df_manipulation(df,how=how,col_selection=col_selection,keep=keep,subset=subset)\n",
    "    display(df)\n",
    "    df_all = df.copy() #id, raw text -> data preprocessing final file\n",
    "    df_out = df.copy() #id, raw text -> ml output\n",
    "    df = df.drop(\"id\",axis=1) #raw text for data preprocessing\n",
    "    \n",
    "    #remove target from df for supervised    \n",
    "    if data[\"DataPreprocessing\"][\"target\"][\"enable\"]:\n",
    "        target = data[\"DataPreprocessing\"][\"target\"][\"column\"]\n",
    "        target = df[[target]] \n",
    "        df = df.drop(target,axis=1)\n",
    "        df_all = df_all.drop(target,axis=1)\n",
    "        display(df)\n",
    "        display(target)\n",
    "        \n",
    "#    word_contractions\n",
    "    wordcont = data[\"DataPreprocessing\"][\"word_contractions\"][\"enable\"]    \n",
    "    if wordcont:\n",
    "        df = word_contractions(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1) #raw text, raw text after contractions\n",
    "            \n",
    "    #lowercase\n",
    "    lower = data[\"DataPreprocessing\"][\"lowercase\"][\"enable\"]      \n",
    "    if lower:\n",
    "        df = lowercase(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "            \n",
    "   #Remove html tag and url\n",
    "    tagrem = data[\"DataPreprocessing\"][\"remove_htmltag_url\"][\"enable\"] \n",
    "    if tagrem:\n",
    "        df = remove_htmltag_url(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    #Remove irrelevant characters and punctuation\n",
    "    pc = data[\"DataPreprocessing\"][\"remove_irrchar_punc\"]\n",
    "    puncrem,char = pc[\"enable\"],pc[\"char\"] \n",
    "    if puncrem:\n",
    "        df = remove_irrchar_punc(df,char=char)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    #Remove numbers \n",
    "    numrem = data[\"DataPreprocessing\"][\"remove_num\"][\"enable\"]\n",
    "    if numrem:\n",
    "        df = remove_num(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove multiple whitespace\n",
    "    wsrem = data[\"DataPreprocessing\"][\"remove_multwhitespace\"][\"enable\"]\n",
    "    if wsrem:\n",
    "        df = remove_multwhitespace(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove stopwords\n",
    "    sw = data[\"DataPreprocessing\"][\"remove_stopwords\"]\n",
    "    stoprem,extra_sw,remove_sw = sw[\"enable\"],sw[\"extra_sw\"],sw[\"remove_sw\"]\n",
    "    if stoprem:\n",
    "        df = remove_stopwords(df,extra_sw=extra_sw,remove_sw=remove_sw)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Remove frequent words\n",
    "    fw = data[\"DataPreprocessing\"][\"remove_freqwords\"]\n",
    "    freqrem,n = fw[\"enable\"],fw[\"n\"]\n",
    "    if freqrem:\n",
    "        df = remove_freqwords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    # Remove rare words\n",
    "    rw = data[\"DataPreprocessing\"][\"remove_rarewords\"]\n",
    "    rarerem,n = rw[\"enable\"],rw[\"n\"]    \n",
    "    if rarerem:\n",
    "        df = remove_rarewords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Custom taxonomy\n",
    "    ct = data[\"DataPreprocessing\"][\"custom_taxo\"]\n",
    "    taxo,remove_taxo,include_taxo = ct[\"enable\"], ct[\"remove_taxo\"], ct[\"include_taxo\"]\n",
    "    if taxo:\n",
    "        df = custom_taxo(df,remove_taxo,include_taxo)\n",
    "        df_all = pd.concat([df_all,df],axis=1)     \n",
    "        \n",
    "    # Stemming\n",
    "    st = data[\"DataPreprocessing\"][\"stem_words\"]\n",
    "    stem,stemmer_type = st[\"enable\"],st[\"stemmer_type\"]     \n",
    "    if stem:\n",
    "        df = stem_words(df,stemmer_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)            \n",
    "    \n",
    "    #Lemmatization\n",
    "    lem = data[\"DataPreprocessing\"][\"lemmatize_words\"]\n",
    "    lemma,lemma_type = lem[\"enable\"], lem[\"lemma_type\"]      \n",
    "    if lemma:\n",
    "        df = lemmatize_words(df,lemma_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    #column bind target to df\n",
    "    if data[\"DataPreprocessing\"][\"target\"][\"enable\"]:\n",
    "        df = pd.concat([target,df],axis=1)\n",
    "        df_all = pd.concat([target,df_all],axis=1)\n",
    "    \n",
    "    display(df)\n",
    "    display(df_all)\n",
    "    \n",
    "    \n",
    "    df_all.to_csv(user_outpath+\"preprocessed_text.csv\",index=False) #save after data preprocessing   \n",
    "    \n",
    "    #---------------ML module---------------# \n",
    "    elk_outpath = data[\"UnsupervisedLearning\"][\"elk_outpath\"]\n",
    "   \n",
    "    ####---Unsupervised Learning---###\n",
    "    #check output save in user or ELK folder\n",
    "    if data[\"UnsupervisedLearning\"][\"Output\"][\"User\"]:#store output in user folder \n",
    "        outpath = user_outpath\n",
    "    if data[\"UnsupervisedLearning\"][\"Output\"][\"ELK\"]: #store output in ELK folder\n",
    "        outpath = elk_outpath  \n",
    "\n",
    "    #k-means clustering \n",
    "    km= data[\"UnsupervisedLearning\"][\"kmeans_clustering\"]\n",
    "    kmeans = km[\"enable\"]\n",
    "    if kmeans:\n",
    "        top_n_terms,ngram_range,fe_type,n_clusters,max_n_clusters= km[\"top_n_terms\"],km[\"ngram_range\"],km[\"fe_type\"],km[\"n_clusters\"],km[\"max_n_clusters\"]        \n",
    "        df_out[\"cluster\"]=kmeans_clustering(column=df,outpath=outpath,top_n_terms=top_n_terms,ngram_range=ngram_range,fe_type=fe_type,n_clusters=n_clusters,max_n_clusters=max_n_clusters)\n",
    "        display(df_out)\n",
    "        df_out.to_csv(outpath+\"KMeansClustering_output.csv\",index=False)          \n",
    "\n",
    "    #LDA\n",
    "    lda_m= data[\"UnsupervisedLearning\"][\"lda\"]\n",
    "    LatentDirichletAllocation = lda_m[\"enable\"]\n",
    "    if LatentDirichletAllocation:\n",
    "        n_components,top_n_terms,ngram_range= lda_m[\"n_components\"],lda_m[\"top_n_terms\"],lda_m[\"ngram_range\"]       \n",
    "        df_out[\"cluster\"]=lda(column=df,outpath=outpath,n_components=n_components,top_n_terms=top_n_terms,ngram_range=ngram_range)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(outpath+\"LatentDirichletAllocation_output.csv\",index=False)       \n",
    "\n",
    "    #NMF Factorization\n",
    "    nmf_m= data[\"UnsupervisedLearning\"][\"nmf\"]\n",
    "    NonNegativeMatrixFactorization = nmf_m[\"enable\"]\n",
    "    if NonNegativeMatrixFactorization:\n",
    "        n_components,top_n_terms,fe_type,ngram_range= nmf_m[\"n_components\"],nmf_m[\"top_n_terms\"],nmf_m[\"fe_type\"],nmf_m[\"ngram_range\"]\n",
    "        df_out[\"cluster\"]=nmf(column=df,outpath=outpath,n_components=n_components,top_n_terms=top_n_terms,fe_type=fe_type,ngram_range=ngram_range)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(outpath+\"NonNegativeMatrixFactorization_output.csv\",index=False) \n",
    "\n",
    "      \n",
    "    #####---Similarity Metrics-----####\n",
    "    #Cosine Similarity\n",
    "    cs= data[\"SimilarityMetrics\"][\"cosinesimilarity\"]\n",
    "    cosinesim = cs[\"enable\"]\n",
    "    if cosinesim:\n",
    "        threshold,total_rows,base_row,ngram_range,fe_type,ascending= cs[\"threshold\"],cs[\"total_rows\"],cs[\"base_row\"],cs[\"ngram_range\"],cs[\"fe_type\"],cs[\"ascending\"]        \n",
    "        cosinesimilarity(column=df,user_outpath=user_outpath,threshold=threshold,total_rows=total_rows,base_row=base_row,ngram_range=ngram_range,fe_type=fe_type,ascending=ascending)\n",
    "\n",
    "    #Jaccard Similarity\n",
    "    js= data[\"SimilarityMetrics\"][\"jaccardsimilarity\"]\n",
    "    jaccardsim = js[\"enable\"]\n",
    "    if jaccardsim:\n",
    "        threshold,total_rows,base_row,ascending= js[\"threshold\"],js[\"total_rows\"],js[\"base_row\"],js[\"ascending\"]\n",
    "        jaccardsimilarity(column=df,user_outpath=user_outpath,threshold=threshold,total_rows=total_rows,base_row=base_row,ascending=ascending)\n",
    "\n",
    "    #####-----Supervised Learning----####\n",
    "      \n",
    "         \n",
    "    #Machine Learning\n",
    "    sl= data[\"SupervisedLearning\"][\"supervised_lng\"]\n",
    "    SupervisedLearning = sl[\"enable\"]\n",
    "    if SupervisedLearning:\n",
    "        target,test_size,ngram_range,fe_type,model_type,ascend= sl[\"target\"],sl[\"test_size\"],sl[\"ngram_range\"],sl[\"fe_type\"],sl[\"model_type\"],sl[\"ascend\"]\n",
    "        supervised_lng(df=df,user_outpath=user_outpath,target=target,test_size=test_size,ngram_range=ngram_range,fe_type=fe_type,model_type=model_type,ascend=ascend)\n",
    "\n",
    "    #Deep Learning\n",
    "    dl= data[\"SupervisedLearning\"][\"deep_lng\"]\n",
    "    DeepLearning = dl[\"enable\"]\n",
    "    if DeepLearning:\n",
    "        target,test_size,ngram_range,fe_type,hidden_layer_sizes,activation,solver,learning_rate,max_iter,ascend= dl[\"target\"],dl[\"test_size\"],dl[\"ngram_range\"],dl[\"fe_type\"],dl[\"hidden_layer_sizes\"],dl[\"activation\"],dl[\"solver\"],dl[\"learning_rate\"],dl[\"max_iter\"],dl[\"ascend\"]\n",
    "        deep_lng(df=df,user_outpath=user_outpath,target=target,test_size=test_size,ngram_range=ngram_range,fe_type=fe_type,hidden_layer_sizes=hidden_layer_sizes,activation=activation,solver=solver,learning_rate=learning_rate,max_iter=max_iter,ascend=ascend)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f2344",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f1ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "def data_loading(path,start_date=None,stop_date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    User can choose to \n",
    "    a) Load all json files following the agreed filename format\n",
    "    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and\n",
    "    stop_date must be used together)\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    \n",
    "    start_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files starting from start_date\n",
    "    - None: no start_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files starting from start_date will be loaded\n",
    "    \n",
    "    stop_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files until stop_date\n",
    "    - None: no stop_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files until stop_date will be loaded\n",
    "    '''\n",
    "    from datetime import datetime,timedelta\n",
    "    import pandas as pd\n",
    "    import glob, os, json\n",
    "    import re\n",
    "    \n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    date_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_date == None and stop_date == None :\n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "\n",
    "            pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "            match = re.search(pattern,file)\n",
    "                \n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "        print(\"Files read:\",file_list)                   \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)              \n",
    "                \n",
    "    else:\n",
    "        #convert start and stop string to datetime\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        stop = datetime.strptime(stop_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "        #iterate from start to stop dates by day and store dates in list\n",
    "        while start <= stop:\n",
    "            date_list.append(start)\n",
    "            start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "        #convert datetime objects to string\n",
    "        string_list =[d.strftime(\"%Y-%m-%d\") for d in date_list]\n",
    "#         print(string_list)\n",
    "        \n",
    "        for file in filenames: \n",
    "            \n",
    "            # search agreed file format pattern in the filename\n",
    "            for date in string_list: \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "                match = re.search(pattern,file)\n",
    "                \n",
    "                #if match is found\n",
    "                if match:\n",
    "                    pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "        print(\"Files read:\",file_list)     \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "def df_manipulation(df,how=None,col_selection=None,keep=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep columns in dataframe\n",
    "    2) Data impute: Impute NA rows with empty string\n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe  \n",
    "    how[None/string]: Drop rows when we have at least one NA or all NA. Choose\n",
    "                      # - None : NA imputed with empty string\n",
    "                      # - \"all\": Drop row with all NA\n",
    "                      # - \"any\": Drop row with at least one NA\n",
    "    col_selection [None/list]: - None [Default]: Keep all columns in dataframe \n",
    "                               - List: List of columns to keep in dataframe                      \n",
    "                                 \n",
    "    keep[None/string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. \n",
    "                      # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                      # - False : Drop all duplicates.                 \n",
    "    subset[list/None]: Subset of columns for dropping NA and identifying duplicates, use None if no column to select\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select column(s) \n",
    "    if col_selection != None:\n",
    "        df = df[col_selection]\n",
    "    \n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())         \n",
    "    \n",
    "    if how == None: # impute NA values with empty string\n",
    "        print(\"NA is imputed with empty string\")\n",
    "        impute_value = \"\"\n",
    "        df = df.fillna(impute_value)\n",
    "        print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())        \n",
    "    else: # drop rows with NA values\n",
    "        print(\"NA is dropped\")\n",
    "        df= df.dropna(axis=0, how=how,subset=subset)\n",
    "        print(\"Number of null values in df after dropping NA rows:\\n\",df.isnull().sum())\n",
    "        print(\"Shape of df after dropping NA rows:\",df.shape)\n",
    "\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    if keep==None:\n",
    "        keep=\"first\"\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def word_contractions(df):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \"\"\"\n",
    "    import contractions\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda text: \" \".join([contractions.fix(word) for word in text.split()]))\n",
    "    df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
    "    \n",
    "    df = df.add_suffix('_cont')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lowercase(df):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    df[dataframe]: input dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    df = df.add_suffix('_lower')\n",
    "        \n",
    "    return df \n",
    "\n",
    "def remove_htmltag_url(df):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    #remove html tag\n",
    "    df = df.applymap(lambda text:BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True))\n",
    "    #remove url\n",
    "    df = df.replace('https?[://%]*\\S+',' ', regex=True) \n",
    "    \n",
    "    df = df.add_suffix('_tagrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_irrchar_punc(df,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex\n",
    "    format.    \n",
    "    params:    \n",
    "    df [dataframe]: input dataframe \n",
    "    characters[string]: input regex of characters to be removed  \n",
    "    \n",
    "    \"\"\"\n",
    "    import re \n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        df = df.replace(char,' ',regex = True)\n",
    "            \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    df = df.replace(r'\\\\+x[\\d\\D][\\d\\D]',' ',regex = True)\n",
    "        \n",
    "    #Remove special characters and punctuation\n",
    "    df = df.replace('[^\\w\\s]',' ',regex = True)\n",
    "    df = df.replace(r'_',' ',regex = True)\n",
    "    \n",
    "    df = df.add_suffix('_puncrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_num(df):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df=df.replace('\\d+',' ', regex=True) \n",
    "    df = df.add_suffix('_numrem')\n",
    "    \n",
    "    return df \n",
    "\n",
    "def remove_multwhitespace(df):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    df = df.add_suffix('_wsrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(df,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "           \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        df = df.replace(pattern,' ', regex=True)\n",
    "    \n",
    "    df = df.add_suffix('_stoprem')\n",
    "                   \n",
    "    return df \n",
    "\n",
    "def remove_freqwords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "    df = df.add_suffix('_freqrem')\n",
    "    return df\n",
    "\n",
    "def remove_rarewords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n",
    "    df = df.add_suffix('_rarerem')\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_taxo(df,remove_taxo,include_taxo):\n",
    "    \"\"\"\n",
    "    User provides taxonomy to be removed or remained in the text. \n",
    "    a) user wants to remove taxonomies only -> input a list of taxonomies/regex to be removed in remove_taxo \n",
    "    b) user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases \n",
    "    (i.e remove taxo \"test\" but  \"test\" remains in \"test cycle\") -> input a list of taxonomies to be removed in remove_taxo and list of\n",
    "    phrases for the taxonomy to remain in include_taxo\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe\n",
    "    remove_taxo[list/regex]: list of taxonomy to be removed from text\n",
    "    include_taxo[list/None]: list of taxonomy to be maintained in text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import pandas as pd \n",
    "    \n",
    "    def convert(text,remove_taxo):  \n",
    "        \"\"\"\n",
    "        Uses regex given in remove_taxo to find and return all matches \n",
    "        \"\"\"\n",
    "        match = re.findall(remove_taxo,text)\n",
    "        if match:                 \n",
    "            new_row = {'Match':match}\n",
    "            return(new_row)\n",
    "        \n",
    "    #if remove_taxo is regex call convert function to get all matches as a list\n",
    "    if type(remove_taxo) == str: \n",
    "        cv_list = []\n",
    "        for i in range(len(df.columns)):\n",
    "            for text in df.iloc[:,i]:\n",
    "                cv = convert(text,remove_taxo)\n",
    "                if cv:\n",
    "                    cv_list.append(cv)\n",
    "        #             print(cv_list)\n",
    "\n",
    "        cv_df = pd.DataFrame(cv_list)\n",
    "        remove_taxo = list(cv_df[\"Match\"].apply(pd.Series).stack().unique())\n",
    "        print(\"Remove_taxo_list:\", remove_taxo)\n",
    "        \n",
    "    def taxo(text,remove_taxo,include_taxo): \n",
    "        if remove_taxo != None and include_taxo != None: #user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases (i.e remove \"test\" but remain \"test\" in \"test cyccle\")\n",
    "\n",
    "            for w in remove_taxo:\n",
    "            #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "                if all(phrase not in text for phrase in include_taxo): \n",
    "                    pattern = r'\\b'+w+r'\\b'\n",
    "                    text = re.sub(pattern,' ', text) \n",
    "                #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "                else: \n",
    "                    if all(w not in phrase for phrase in include_taxo):\n",
    "                        pattern = r'\\b'+w+r'\\b'\n",
    "                        text = re.sub(pattern,' ', text) \n",
    "                        \n",
    "        if remove_taxo != None and include_taxo == None: #user wants to remove taxonomies only:\n",
    "            for w in remove_taxo: #remove_taxo in list of words\n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text)\n",
    "                 \n",
    "        return text \n",
    "    \n",
    "    \n",
    "    df = df.applymap(lambda text: taxo(text,remove_taxo,include_taxo))     \n",
    "    df = df.add_suffix('_taxo')\n",
    "                \n",
    "    return df    \n",
    "\n",
    "def stem_words(df,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    stemmer_type[None/string]: input stemming method \n",
    "                                - None for Porter Stemmer\n",
    "                                - \"Lancaster\" for Lancaster Stemmer \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    \n",
    "    if stemmer_type == None:\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    df = df.applymap(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_stem')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def lemmatize_words(df,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    lemma_type[None/string]: input lemmatization method\n",
    "                            - None for WordNetLemmatizer\n",
    "                            - \"Spacy\" for Spacy    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    if lemma_type == None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df = df.applymap(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "        \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        df = df.applymap(lambda text: \" \".join([word.lemma_ for word in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        df = df.applymap(lambda s:s.lower() if type(s) == str else s) \n",
    "        \n",
    "    df = df.add_suffix('_lemma')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def feature_extraction(column,ngram_range=None,ascending=None,fe_type=None):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - TF-IDF(default choice) or Bag of words\n",
    "     \n",
    "    params:\n",
    "    column [series/DataFrame]: column selected for feature extraction \n",
    "                        - series: only one column is selected for feature extraction (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for feature extraction (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "                    \n",
    "    if ngram_range == None: #set ngram range as unigram by default\n",
    "        ngram_range=(1,1)\n",
    "        \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == None: #tfidf\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4e7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsupervised learning\n",
    "import pandas as pd\n",
    "def kmeans_clustering(column,outpath,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for clustering \n",
    "                        - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"   \n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn import metrics\n",
    "\n",
    "    silhouette_avg_list = []\n",
    "    n_clusters_list = []\n",
    "    dicts = {}\n",
    "    \n",
    "    #call feature extraction function    \n",
    "    ascending = None \n",
    "    X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "    X = X.drop(index='sum')\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "    #user provides the number of clusters        \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "        with open(outpath+'TopWords_SilhouetteScore_KMeans.txt','w') as f:\n",
    "            print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3),file=f)\n",
    "        \n",
    "            \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "#             print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        with open(outpath+'TopWords_SilhouetteScore_KMeans.txt','w') as f:\n",
    "            print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\",file=f) \n",
    "    \n",
    "    with open(outpath+'TopWords_SilhouetteScore_KMeans.txt','a') as f:\n",
    "        print(\"Top\",top_n_terms,\"terms per cluster:\",file=f)\n",
    "        order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "        terms = vec_type.get_feature_names()\n",
    "        for i in range(n_clusters):\n",
    "            print(\"Cluster\",i,file=f)\n",
    "            print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]],file=f) #top n terms in each cluster\n",
    "            print(\"\\n\",file=f)\n",
    "   \n",
    "               \n",
    "    return labels\n",
    "\n",
    "\n",
    "def lda(column,outpath,n_components,top_n_terms,ngram_range=None):\n",
    "    \"\"\"\n",
    "    LDA for unsupervised learning. Bag of words is selected for feature extraction\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for lda\n",
    "                        - series: only one column is selected for lda (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for lda (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in the lda_model\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "    #feature extraction\n",
    "    ascending = None\n",
    "    fe_type = \"bagofwords\"\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the LDA class \n",
    "    lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "    lda_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = lda_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "    with open(outpath+'TopWords_LDA.txt','w') as f:\n",
    "        for i,topic in enumerate(topic_words):\n",
    "            print(f\"The top {top_n_terms} words for cluster #{i}\",file=f)\n",
    "            print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]],file=f)\n",
    "            print(\"\\n\",file=f)\n",
    "            \n",
    "            \n",
    "    topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "    \n",
    "    return topic_results.argmax(axis=1)\n",
    "\n",
    "\n",
    "def nmf(column,outpath,n_components,top_n_terms,fe_type,ngram_range=None):\n",
    "    \"\"\"\n",
    "    Non-negative matrix factorization for unsupervised learning.\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for NMF \n",
    "                        - series: only one column is selected for NMF (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for NMF (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in NMF\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import NMF\n",
    "    \n",
    "    #feature extraction\n",
    "    ngram_range = None\n",
    "    ascending = None\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the NMF class \n",
    "    nmf_model = NMF(n_components,random_state=42)\n",
    "    nmf_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = nmf_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "    with open(outpath+'TopWords_NMF.txt','w') as f:\n",
    "        for i,topic in enumerate(topic_words):\n",
    "            print(f\"The top {top_n_terms} words for cluster #{i}\",file=f)\n",
    "            print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]],file=f)\n",
    "            print(\"\\n\",file=f)\n",
    "        \n",
    "    topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2697044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#supervised learning\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import numpy as np \n",
    "\n",
    "def supervised_lng(df,user_outpath,target,test_size,ngram_range=None,fe_type=None,model_type=None,ascend=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Consists of 3 supervised machine learning methods: RandomForest (Default), Naive Bayes(optional, SVM (optional)\n",
    "    \n",
    "    X[series/DataFrame]: column(s) of text for supervised learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    y[series]: target \n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       -[DEFAULT] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for default tfidf method\n",
    "    model_type[None/string]: Choose ML algorithm \n",
    "                            - None (Default algorithm is Random Forest)\n",
    "                            - 'NB'(To choose Naive Bayes as ML algorithm), \n",
    "                            - 'SVM'(To choose Support Vector Machine as ML algorithm)\n",
    "    ascend[True/False/None]:  - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                              - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                              - False(Confusion matrix arranged in descending order of accuracy % per label)  \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Target distribution:\",df[target].value_counts())\n",
    "    X= df.drop([target],axis=1)\n",
    "    y= df[target]   \n",
    "    \n",
    "    #TRAIN-TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "\n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train       \n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    if model_type == None:\n",
    "        #random forest is chosen by default\n",
    "        model = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "    if model_type == \"NB\":\n",
    "        model = MultinomialNB()\n",
    "                   \n",
    "    if model_type == \"SVM\":\n",
    "        model = svm.SVC(random_state = 42)\n",
    "    \n",
    "    model.fit(X_train, y_train) \n",
    "    \n",
    "    #MODEL SAVING\n",
    "    \n",
    "    joblib.dump(model, user_outpath + \"model.joblib\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(user_outpath+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(user_outpath+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(user_outpath+\"Confusion_Matrix.csv\",index=False)  \n",
    "\n",
    "#Deep Learning \n",
    "def deep_lng(df,user_outpath,target,test_size,ngram_range,fe_type,hidden_layer_sizes=None,activation=None,solver=None,learning_rate=None,max_iter=None,ascend=None):\n",
    "    \"\"\"\n",
    "     Deep learning method: MultiLayer Perceptron\n",
    "\n",
    "    X[series/DataFrame]: column(s) of text for deep learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])   \n",
    "    y[series]: target\n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    hidden_layer_sizes[tuple],default = (100): To set the number of layers and the number of nodes.\n",
    "                                               Each element in the tuple represents the number of nodes,\n",
    "                                               length of tuple denotes the total number of hidden layers in the network\n",
    "    activation[\"identity\", \"logistic\", \"tanh\",\"relu\"], default=\"relu\": Activation function for the hidden layer.\n",
    "    solver[\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\": The solver for weight optimization.\n",
    "    learning_rate[\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\": Learning rate schedule for weight updates\n",
    "    max_iter[int], default=200: Maximum number of iterations. The solver iterates until convergence or this number of iterations.\n",
    "    ascend [True/False/None]: - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                                 - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                                 - False(Confusion matrix arranged in descending order of accuracy % per label)                            \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)    \n",
    "    \"\"\"    \n",
    "    print(\"Target distribution:\",df[target].value_counts())\n",
    "    X= df.drop([target],axis=1)\n",
    "    y= df[target]   \n",
    "    \n",
    "    #train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "        \n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train\n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    #default hypermarameters\n",
    "    if hidden_layer_sizes == None:\n",
    "        hidden_layer_sizes = (100)\n",
    "    if activation == None:\n",
    "        activation = \"relu\"\n",
    "    if solver == None:\n",
    "        solver = \"adam\"\n",
    "    if learning_rate == None:\n",
    "        learning_rate = \"constant\"\n",
    "    if max_iter == None:\n",
    "        max_iter = 200\n",
    "    \n",
    "    print(\"Hidden layer sizes: \", hidden_layer_sizes,\", Activation: \",activation,\", Solver: \",solver,\", Learning rate: \",learning_rate,\", Max iteration: \",max_iter)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter,verbose = False,random_state=42)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    #MODEL SAVING    \n",
    "    joblib.dump(model, user_outpath + \"mlpmodel.joblib\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(user_outpath+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(user_outpath+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(user_outpath+\"Confusion_Matrix.csv\",index=False)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0eab700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity metrics\n",
    "#Cosine Similarity \n",
    "\n",
    "def cosinesimilarity(column,user_outpath,threshold=None,total_rows = None,base_row=None,ngram_range=None,fe_type=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between rows of texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:    \n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])  \n",
    "    threshold[None/float]: cut off value for the cosine similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for tfidf\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "                \n",
    "    #feature extraction              \n",
    "    X = feature_extraction(column=column,ngram_range=ngram_range,ascending=None,fe_type=fe_type)[0]\n",
    "    X = X.drop([\"sum\"],axis = 0)\n",
    "    \n",
    "    #Get cosine similarity matrix\n",
    "    similarity_matrix = pd.DataFrame(cosine_similarity(X))\n",
    "    \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "        \n",
    "       \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        print(\"Fix number of rows for comparison, each row will be taken as base and compared with the rest\")\n",
    "        results_append = []\n",
    "        for base in range(total_rows):            \n",
    "            #Create empty df\n",
    "            column_names = [\"Base Index\",\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)            \n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                \n",
    "                if similarity_matrix.iloc[base,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {'Base Index':base,'Index':i,'Similarity Score':round(similarity_matrix.iloc[base,i],4), 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                \n",
    "                    if ascending != None:            \n",
    "                        results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)\n",
    "                        \n",
    "#             display(results)\n",
    "            results_append.append(results)\n",
    "        results_append = pd.concat(results_append)\n",
    "        display(results_append)         \n",
    "        results_append.to_csv(user_outpath+\"Cosine_Similarity.csv\",index=False)\n",
    "            \n",
    "    if base_row !=None: #fix base_row index for comparison with all indexes\n",
    "        print (\"Fix one row as base, comparison will be done with all the other rows\")          \n",
    "        #Create empty df\n",
    "        column_names = [\"Base Index\",\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)\n",
    "        \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            if similarity_matrix.iloc[base_row,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {'Base Index':base_row,'Index':i, 'Similarity Score':round(similarity_matrix.iloc[base_row,i],4), 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "#         display(results)\n",
    "                \n",
    "        results.to_csv(user_outpath+\"Cosine_Similarity.csv\",index=False)  \n",
    "\n",
    "#Jaccard Similarity \n",
    "\n",
    "def jaccardsimilarity(column,user_outpath,threshold=None,total_rows = None,base_row=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the jaccard similarity between texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:\n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]]) \n",
    "    threshold[None/float]: cut off value for the jaccard similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"     \n",
    "            \n",
    "    #jaccard score computation\n",
    "    def get_jaccard_sim(str1, str2):        \n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "       \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "        \n",
    "    \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        print(\"Fix number of rows for comparison, each row will be taken as base and compared with the rest\")\n",
    "        results_append = []\n",
    "        for base in range(total_rows):\n",
    "            \n",
    "            #Create empty df\n",
    "            column_names = [\"Base Index\",\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                jac_score =  round(get_jaccard_sim(column.iloc[base],column.iloc[i]),4)\n",
    "                if jac_score >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {\"Base Index\":base,'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                \n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "                    \n",
    "#             display(results)\n",
    "            results_append.append(results)\n",
    "        results_append = pd.concat(results_append)\n",
    "        display(results_append)         \n",
    "        results_append.to_csv(user_outpath+\"Jaccard_Similarity.csv\",index=False)\n",
    "        \n",
    "    if base_row != None: #fix base_row index for comparison with all indexes\n",
    "       \n",
    "        print (\"Fix one row as base, comparison will be done with all the other rows\") \n",
    "        #Create empty df\n",
    "        column_names = [\"Base Index\",\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            jac_score = round(get_jaccard_sim(column.iloc[base_row],column.iloc[i]),4)\n",
    "            if jac_score >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {\"Base Index\":base_row,'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "            if ascending != None:            \n",
    "                results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "#         display(results)\n",
    "        results.to_csv(user_outpath+\"Jaccard_Similarity.csv\",index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac7513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8cfa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #convert csv to json\n",
    "# import csv \n",
    "# import json\n",
    "\n",
    "\n",
    "# def csv_to_json(csvFilePath, jsonFilePath):\n",
    "#     jsonArray = []\n",
    "      \n",
    "#     #read csv file\n",
    "#     with open(csvFilePath) as csvf: \n",
    "#         #load csv file data using csv library's dictionary reader\n",
    "#         csvReader = csv.DictReader(csvf) \n",
    "\n",
    "#         #convert each csv row into python dict\n",
    "#         for row in csvReader: \n",
    "#             #add this python dict to json array\n",
    "#             jsonArray.append(row)\n",
    "  \n",
    "#     #convert python jsonArray to JSON String and write to file\n",
    "#     with open(jsonFilePath, 'w', encoding='utf-8') as jsonf: \n",
    "#         jsonString = json.dumps(jsonArray, indent=4)\n",
    "#         jsonf.write(jsonString)\n",
    "          \n",
    "# csvFilePath = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/sip_sighting_usb_duplicate_ai.csv\"\n",
    "# jsonFilePath = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/VICE_JSON/(2021-08-25)1_VICE_1.json\"\n",
    "\n",
    "# csv_to_json(csvFilePath, jsonFilePath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd825076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "# df= pd.read_csv(path+\"sip_sighting_usb_duplicate_ai.csv\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #unsupervised learning\n",
    "# import pandas as pd\n",
    "# def kmeans_clustering(column,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "#     \"\"\"\n",
    "#     K- means clustering for unsupervised learning. User can choose either options:\n",
    "#     (1) provide the number of clusters or\n",
    "#     (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "#     silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "#     params:\n",
    "#     column [series/DataFrame]: column(s) selected for clustering \n",
    "#                         - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "#                         - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "#     top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "#     ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "#                                    - [default] ngram_range of (1, 1) means only unigrams, \n",
    "#                                    - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "#                                    - ngram_range of (2, 2) means only bigram\n",
    "#     fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "#     n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "#     max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "#     \"\"\"   \n",
    "#     from sklearn.cluster import KMeans\n",
    "#     from sklearn import metrics\n",
    "\n",
    "#     silhouette_avg_list = []\n",
    "#     n_clusters_list = []\n",
    "#     dicts = {}\n",
    "    \n",
    "#     #call feature extraction function    \n",
    "#     ascending = None \n",
    "#     X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "#     X = X.drop(index='sum')\n",
    "#     vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "#     #user provides the number of clusters        \n",
    "#     if n_clusters != None:\n",
    "#         model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "#         model.fit_predict(X)\n",
    "#         labels = model.labels_\n",
    "\n",
    "#         silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "#         print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "        \n",
    "            \n",
    "#     #user provides the maximum number of clusters \n",
    "#     if max_n_clusters != None:\n",
    "#         for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "#             model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "#             model.fit_predict(X)\n",
    "#             labels = model.labels_\n",
    "\n",
    "#             silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "#             print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "#             silhouette_avg_list.append(silhouette_avg)\n",
    "#             n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "#         for i in range(len(n_clusters_list)):\n",
    "#             dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "#         n_clusters_max = max(dicts,key=dicts.get)\n",
    "#         silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "#         model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "#         model.fit_predict(X)\n",
    "#         labels = model.labels_\n",
    "#         n_clusters = n_clusters_max\n",
    "#         print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "#     print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "#     order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "#     terms = vec_type.get_feature_names()\n",
    "#     for i in range(n_clusters):\n",
    "#         print(\"Cluster %d:\" % i)\n",
    "#         print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "#         print(\"\\n\")\n",
    "   \n",
    "               \n",
    "#     return labels\n",
    "\n",
    "\n",
    "# def lda(column,n_components,top_n_terms,ngram_range=None):\n",
    "#     \"\"\"\n",
    "#     LDA for unsupervised learning. Bag of words is selected for feature extraction\n",
    "#     params:\n",
    "#     column [series/DataFrame]: column(s) selected for lda\n",
    "#                         - series: only one column is selected for lda (e.g. df[\"title_clean\"])\n",
    "#                         - DataFrame: more than one column is selected for lda (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "#     n_components[int]: the number of topics/clusters used in the lda_model\n",
    "#     top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "#     ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "#                                    - [default] ngram_range of (1, 1) means only unigrams, \n",
    "#                                    - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "#                                    - ngram_range of (2, 2) means only bigram\n",
    "    \n",
    "#     \"\"\"\n",
    "#     from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "#     #feature extraction\n",
    "#     ascending = None\n",
    "#     fe_type = \"bagofwords\"\n",
    "#     vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "#     vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "#     # Create object for the LDA class \n",
    "#     lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "#     lda_model.fit(vectorized)\n",
    "    \n",
    "#     # Components_ gives us our topic distribution \n",
    "#     topic_words = lda_model.components_\n",
    "\n",
    "#     # Top n words for a topic\n",
    "\n",
    "#     for i,topic in enumerate(topic_words):\n",
    "#         print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "#         print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#     topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "    \n",
    "#     return topic_results.argmax(axis=1)\n",
    "\n",
    "\n",
    "# def nmf(column,n_components,top_n_terms,fe_type,ngram_range=None):\n",
    "#     \"\"\"\n",
    "#     Non-negative matrix factorization for unsupervised learning.\n",
    "#     params:\n",
    "#     column [series/DataFrame]: column(s) selected for NMF \n",
    "#                         - series: only one column is selected for NMF (e.g. df[\"title_clean\"])\n",
    "#                         - DataFrame: more than one column is selected for NMF (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "#     n_components[int]: the number of topics/clusters used in NMF\n",
    "#     top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "#     fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "#     ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "#                                    - [default] ngram_range of (1, 1) means only unigrams, \n",
    "#                                    - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "#                                    - ngram_range of (2, 2) means only bigram\n",
    "#     \"\"\"\n",
    "#     from sklearn.decomposition import NMF\n",
    "    \n",
    "#     #feature extraction\n",
    "#     ngram_range = None\n",
    "#     ascending = None\n",
    "#     vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "#     vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "#     # Create object for the NMF class \n",
    "#     nmf_model = NMF(n_components,random_state=42)\n",
    "#     nmf_model.fit(vectorized)\n",
    "    \n",
    "#     # Components_ gives us our topic distribution \n",
    "#     topic_words = nmf_model.components_\n",
    "\n",
    "#     # Top n words for a topic\n",
    "\n",
    "#     for i,topic in enumerate(topic_words):\n",
    "#         print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "#         print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#     topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "#     return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cbfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write config file for supervised\n",
    "# import json\n",
    "# json_path ='C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/'\n",
    "# path = json_path + \"VICE_JSON/\"\n",
    "# # path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/\"\n",
    "# config = {   \n",
    "#     \"Email\":\"abc@intel.com\",\n",
    "#     \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "#     \"DataPreprocessing\":{    \n",
    "#     \"df_manipulation\": {\"how\":\"any\",\"col_selection\":[\"id\",\"title\",\"description\",\"problem_area\"],\"keep\":\"first\",\"subset\":None},   \n",
    "#     \"target\":{\"column\":\"problem_area\"}, #for supervised learning only\n",
    "#     \"word_contractions\": {\"enable\": True},\n",
    "#     \"lowercase\": {\"enable\": True},\n",
    "#     \"remove_htmltag_url\": {\"enable\":True},\n",
    "#     \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "#     \"remove_num\":{\"enable\":True},\n",
    "#     \"remove_multwhitespace\":{\"enable\":True},\n",
    "#     \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "#     \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "#     \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "#     \"custom_taxo\":{\"enable\":True,\"remove_taxo\":[\"gio\",\"fields\",\"test\"],\"include_taxo\":[\"test suite execution\",\"kpi metric\"]},\n",
    "#     \"stem_words\":{\"enable\":True,\"stemmer_type\":None},\n",
    "#     \"lemmatize_words\":{\"enable\":False,\"lemma_type\":None}\n",
    "#     },   \n",
    "#     \"SupervisedLearning\":{\n",
    "#         \"supervised_lng\":{\"enable\":False,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"model_type\":None,\"ascend\":None},\n",
    "#         \"deep_lng\":{\"enable\":True,\"target\":\"problem_area\",\"test_size\":0.3,\"ngram_range\":None,\"fe_type\":None,\"hidden_layer_sizes\":(5),\"activation\":None,\"solver\":None,\"learning_rate\":None,\"max_iter\":None,\"ascend\":None}\n",
    "#     }\n",
    "\n",
    "# }\n",
    "\n",
    "# with open(json_path+'config_supervised.json', 'w') as f:\n",
    "#     json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b61f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_taxo(df,remove_taxo,include_taxo):\n",
    "#     \"\"\"\n",
    "#     User provides taxonomy to be removed or remained in the text. \n",
    "#     a) user wants to remove taxonomies only -> input a list of taxonomies to be removed in remove_taxo \n",
    "#     b) user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases \n",
    "#     (i.e remove taxo \"test\" but  \"test\" remains in \"test cycle\") -> input a list of taxonomies to be removed in remove_taxo and list of\n",
    "#     phrases for the taxonomy to remain in include_taxo\n",
    "    \n",
    "#     params:\n",
    "#     df [dataframe]: input dataframe\n",
    "#     remove_taxo[list]: list of taxonomy to be removed from text\n",
    "#     include_taxo[list/None]: list of taxonomy to be maintained in text\n",
    "#     \"\"\"\n",
    "#     import re\n",
    "    \n",
    "#     def taxo(text,remove_taxo,include_taxo):\n",
    "#         if remove_taxo != None and include_taxo != None: #user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases (i.e remove \"test\" but remain \"test\" in \"test cyccle\")\n",
    "#             for w in remove_taxo:\n",
    "#             #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "#                 if all(phrase not in text for phrase in include_taxo): \n",
    "#                     pattern = r'\\b'+w+r'\\b'\n",
    "#                     text = re.sub(pattern,' ', text) \n",
    "#                 #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "#                 else: \n",
    "#                     if all(w not in phrase for phrase in include_taxo):\n",
    "#                         pattern = r'\\b'+w+r'\\b'\n",
    "#                         text = re.sub(pattern,' ', text) \n",
    "                        \n",
    "#         if remove_taxo != None and include_taxo == None: #user wants to remove taxonomies only:\n",
    "#              for w in remove_taxo:\n",
    "#                 pattern = r'\\b'+w+r'\\b'\n",
    "#                 text = re.sub(pattern,' ', text)\n",
    "                 \n",
    "#         return text \n",
    "    \n",
    "    \n",
    "#     df = df.applymap(lambda text: taxo(text,remove_taxo,include_taxo))    \n",
    "#     df = df.add_suffix('_taxo')\n",
    "                \n",
    "#     return df    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
