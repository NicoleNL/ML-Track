{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd583e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write config file\n",
    "import json\n",
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/\"\n",
    "\n",
    "config = {\n",
    "    \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "    \"DataPreprocessing\":{\n",
    "    \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "    \"word_contractions\": {\"enable\": True},\n",
    "    \"lowercase\": {\"enable\": True},\n",
    "    \"remove_htmltag_url\": {\"enable\":True},\n",
    "    \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "    \"remove_num\":{\"enable\":True},\n",
    "    \"remove_multwhitespace\":{\"enable\":True},\n",
    "    \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "    \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "    \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "    \"custom_taxo\":{\"enable\":True,\"remove_taxo\":[\"gio\",\"fields\",\"test\"],\"include_taxo\":[\"test suite execution\",\"kpi metric\"]},\n",
    "    \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "    \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "    },\n",
    "    \"UnsupervisedLearning\":{\n",
    "        \"kmeans_clustering\":{\"enable\":True,\"top_n_terms\":10,\"ngram_range\":None,\"fe_type\":None,\"n_clusters\":None,\"max_n_clusters\":10}\n",
    "        \"lda\"\n",
    "        \"nmf\"\n",
    "    },\n",
    "    \"SupervisedLearning\":{\n",
    "        \n",
    "    },\n",
    "    \"SimilarityMetrics\":{\n",
    "        \n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "with open(json_path+'config.json', 'w') as f:\n",
    "    json.dump(config,f,indent=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63866130",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = json_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "74fa07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file and call the other functions\n",
    "def main(json_path,out_path):\n",
    "    \n",
    "    import json     \n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(json_path+'config.json') as config_file:\n",
    "        data = json.load(config_file)\n",
    "    \n",
    "    #---------DATA LOADING----------#\n",
    "    dl = data[\"DataLoading\"]\n",
    "    path,start_date,stop_date = dl['path'],dl['start_date'],dl['stop_date']  \n",
    "    df = data_loading(path=path,start_date=start_date,stop_date=stop_date)\n",
    "\n",
    "    #---------DATA PREPROCESSING----------# \n",
    "    #df_manipulation\n",
    "    dm = data[\"DataPreprocessing\"][\"df_manipulation\"]\n",
    "    col_selection,keep,subset = dm['col_selection'],dm['keep'],dm['subset']  \n",
    "    df = df_manipulation(df,col_selection=col_selection,keep=keep,subset=subset)\n",
    "    df_all = df.copy() #id, raw text -> data preprocessing final file\n",
    "    df_out = df.copy() #id, raw text -> ml output\n",
    "    df = df.drop(\"id\",axis=1) #raw text\n",
    "    \n",
    "    #word_contractions\n",
    "    wordcont = data[\"DataPreprocessing\"][\"word_contractions\"][\"enable\"]    \n",
    "    if wordcont:\n",
    "        df = word_contractions(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1) #raw text, raw text after contractions\n",
    "            \n",
    "    #lowercase\n",
    "    lower = data[\"DataPreprocessing\"][\"lowercase\"][\"enable\"]      \n",
    "    if lower:\n",
    "        df = lowercase(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "            \n",
    "   #Remove html tag and url\n",
    "    tagrem = data[\"DataPreprocessing\"][\"remove_htmltag_url\"][\"enable\"] \n",
    "    if tagrem:\n",
    "        df = remove_htmltag_url(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    #Remove irrelevant characters and punctuation\n",
    "    pc = data[\"DataPreprocessing\"][\"remove_irrchar_punc\"]\n",
    "    puncrem,char = pc[\"enable\"],pc[\"char\"] \n",
    "    if puncrem:\n",
    "        df = remove_irrchar_punc(df,char=char)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    #Remove numbers \n",
    "    numrem = data[\"DataPreprocessing\"][\"remove_num\"][\"enable\"]\n",
    "    if numrem:\n",
    "        df = remove_num(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove multiple whitespace\n",
    "    wsrem = data[\"DataPreprocessing\"][\"remove_multwhitespace\"][\"enable\"]\n",
    "    if wsrem:\n",
    "        df = remove_multwhitespace(df)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stoprem = data[\"DataPreprocessing\"][\"remove_stopwords\"][\"enable\"]\n",
    "    if stoprem:\n",
    "        df = remove_stopwords(df,extra_sw=None,remove_sw=None)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Remove frequent words\n",
    "    fw = data[\"DataPreprocessing\"][\"remove_freqwords\"]\n",
    "    freqrem,n = fw[\"enable\"],fw[\"n\"]\n",
    "    if freqrem:\n",
    "        df = remove_freqwords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    # Remove rare words\n",
    "    rw = data[\"DataPreprocessing\"][\"remove_rarewords\"]\n",
    "    rarerem,n = rw[\"enable\"],rw[\"n\"]    \n",
    "    if rarerem:\n",
    "        df = remove_rarewords(df,n)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "        \n",
    "    # Custom taxonomy\n",
    "    ct = data[\"DataPreprocessing\"][\"custom_taxo\"]\n",
    "    taxo,remove_taxo,include_taxo = ct[\"enable\"], ct[\"remove_taxo\"], ct[\"include_taxo\"]\n",
    "    if taxo:\n",
    "        df = custom_taxo(df,remove_taxo,include_taxo)\n",
    "        df_all = pd.concat([df_all,df],axis=1)     \n",
    "        \n",
    "    # Stemming\n",
    "    st = data[\"DataPreprocessing\"][\"stem_words\"]\n",
    "    stem,stemmer_type = st[\"enable\"],st[\"stemmer_type\"]     \n",
    "    if stem:\n",
    "        df = stem_words(df,stemmer_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)            \n",
    "    \n",
    "    #Lemmatization\n",
    "    lem = data[\"DataPreprocessing\"][\"lemmatize_words\"]\n",
    "    lemma,lemma_type = lem[\"enable\"], lem[\"lemma_type\"]      \n",
    "    if lemma:\n",
    "        df = lemmatize_words(df,lemma_type)\n",
    "        df_all = pd.concat([df_all,df],axis=1)\n",
    "    \n",
    "    display(df_all)    \n",
    "    df_all.to_csv(out_path+\"preprocessed_text.csv\",index=False) #save after data preprocessing   \n",
    "    \n",
    "    #---------------ML module---------------# \n",
    "    #Unsupervised Learning\n",
    "    #k-means clustering \n",
    "    km= data[\"UnsupervisedLearning\"][\"kmeans_clustering\"]\n",
    "    kmeans = km[\"enable\"]\n",
    "    if kmeans:\n",
    "        top_n_terms,ngram_range,fe_type,n_clusters,max_n_clusters= km[\"top_n_terms\"],km[\"ngram_range\"],km[\"fe_type\"],km[\"n_clusters\"],km[\"max_n_clusters\"]        \n",
    "        df_out[\"cluster\"]=kmeans_clustering(column=df,top_n_terms=top_n_terms,ngram_range=ngram_range,fe_type=fe_type,n_clusters=n_clusters,max_n_clusters=max_n_clusters)\n",
    "        display(df_out)        \n",
    "        df_out.to_csv(out_path+\"kmeans_output.csv\",index=False)          \n",
    "    \n",
    "    #Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6ee69fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read: ['C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)1_firstSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)3_secondSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-10-11)3_secondSet_1.json']\n",
      "Shape of df before manipulation: (2712, 15)\n",
      "Shape of df after selecting columns: (2712, 2)\n",
      "Number of null values in df:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of null values in df after NA imputation:\n",
      " id             0\n",
      "description    0\n",
      "dtype: int64\n",
      "Number of duplicates in the df: 1808\n",
      "Shape of df after manipulation: (904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>description_contract</th>\n",
       "      <th>description_contract_lower</th>\n",
       "      <th>description_contract_lower_tagrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem</th>\n",
       "      <th>description_contract_lower_tagrem_puncrem_numrem_wsrem_taxo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "      <td>please provide a way to update     from git re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>&lt;p&gt;test suite execution finished before execut...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>&lt;p&gt;i am trying to clone defects from another t...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "      <td>i am trying to clone defects from another   cy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again.</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "      <td>retest some function again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;hello,&amp;...</td>\n",
       "      <td>hello, please import time global domain: time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello  please import time global domain  time ...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi_metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team  thank you for providing kpi metri...</td>\n",
       "      <td>hi gio team thank you for providing kpi metric...</td>\n",
       "      <td>hi   team thank you for providing kpi metric f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>&lt;div&gt;the schedule test suite allow for the use...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule test suite allow for the user to ...</td>\n",
       "      <td>the schedule   suite allow for the user to clo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>&lt;p&gt;hi gio team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;thank you f...</td>\n",
       "      <td>hi gio team, thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team  thank you for providing kpi featu...</td>\n",
       "      <td>hi gio team thank you for providing kpi featur...</td>\n",
       "      <td>hi   team thank you for providing kpi feature ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>&lt;p&gt;converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>converting to enhancement... would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement    would like the ab...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        description  \\\n",
       "0     1308651592  Please provide a way to update GIO fields from...   \n",
       "1     1308671310  <p>Test suite execution finished before execut...   \n",
       "2     1308673361  <p>I am trying to clone defects from another t...   \n",
       "3     1507656633                        Retest some function again.   \n",
       "4     1507656638               enter the support needed at here ...   \n",
       "..           ...                                                ...   \n",
       "899  22012641037  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  22012645565  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  22012704243  <div>The schedule test suite allow for the use...   \n",
       "902  22012765885  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  22013190829  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                                  description_contract  \\\n",
       "0    Please provide a way to update GIO fields from...   \n",
       "1    <p>Test suite execution finished before execut...   \n",
       "2    <p>I am trying to clone defects from another t...   \n",
       "3                          Retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  <div>The schedule test suite allow for the use...   \n",
       "902  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                            description_contract_lower  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    <p>test suite execution finished before execut...   \n",
       "2    <p>i am trying to clone defects from another t...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">hello,&...   \n",
       "900  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "901  <div>the schedule test suite allow for the use...   \n",
       "902  <p>hi gio team,</p><p><br /></p><p>thank you f...   \n",
       "903  <p>converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                     description_contract_lower_tagrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  hello, please import time global domain: time ...   \n",
       "900  hi gio team, thank you for providing kpi_metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team, thank you for providing kpi featu...   \n",
       "903  converting to enhancement... would like the ab...   \n",
       "\n",
       "             description_contract_lower_tagrem_puncrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "      description_contract_lower_tagrem_puncrem_numrem  \\\n",
       "0    please provide a way to update gio fields from...   \n",
       "1    test suite execution finished before executing...   \n",
       "2    i am trying to clone defects from another test...   \n",
       "3                          retest some function again    \n",
       "4                 enter the support needed at here       \n",
       "..                                                 ...   \n",
       "899  hello  please import time global domain  time ...   \n",
       "900  hi gio team  thank you for providing kpi metri...   \n",
       "901  the schedule test suite allow for the user to ...   \n",
       "902  hi gio team  thank you for providing kpi featu...   \n",
       "903  converting to enhancement    would like the ab...   \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem  \\\n",
       "0    please provide a way to update gio fields from...       \n",
       "1    test suite execution finished before executing...       \n",
       "2    i am trying to clone defects from another test...       \n",
       "3                          retest some function again        \n",
       "4                    enter the support needed at here        \n",
       "..                                                 ...       \n",
       "899  hello please import time global domain time kp...       \n",
       "900  hi gio team thank you for providing kpi metric...       \n",
       "901  the schedule test suite allow for the user to ...       \n",
       "902  hi gio team thank you for providing kpi featur...       \n",
       "903  converting to enhancement would like the abili...       \n",
       "\n",
       "    description_contract_lower_tagrem_puncrem_numrem_wsrem_taxo  \n",
       "0    please provide a way to update     from git re...           \n",
       "1    test suite execution finished before executing...           \n",
       "2    i am trying to clone defects from another   cy...           \n",
       "3                          retest some function again            \n",
       "4                    enter the support needed at here            \n",
       "..                                                 ...           \n",
       "899  hello please import time global domain time kp...           \n",
       "900  hi   team thank you for providing kpi metric f...           \n",
       "901  the schedule   suite allow for the user to clo...           \n",
       "902  hi   team thank you for providing kpi feature ...           \n",
       "903  converting to enhancement would like the abili...           \n",
       "\n",
       "[904 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The silhouette_score is : 0.017\n",
      "For n_clusters = 3 The silhouette_score is : 0.013\n",
      "For n_clusters = 4 The silhouette_score is : 0.013\n",
      "For n_clusters = 5 The silhouette_score is : 0.011\n",
      "For n_clusters = 6 The silhouette_score is : 0.011\n",
      "For n_clusters = 7 The silhouette_score is : 0.014\n",
      "For n_clusters = 8 The silhouette_score is : 0.015\n",
      "For n_clusters = 9 The silhouette_score is : 0.015\n",
      "For n_clusters = 10 The silhouette_score is : 0.016\n",
      "\n",
      "The optimal number of clusters selected is 2 with silhouette_score of 0.017 \n",
      "\n",
      "Top 10 terms per cluster:\n",
      "Cluster 0:\n",
      "['project', 'dng', 'to', 'program', 'sve', 'requirement', 'create', 'yocto', 'link', 'request']\n",
      "\n",
      "\n",
      "Cluster 1:\n",
      "['the', 'to', 'in', 'is', 'and', 'for', 'cycle', 'not', 'of', 'this']\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "      <th>cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                        description  cluster\n",
       "0     1308651592  Please provide a way to update GIO fields from...        1\n",
       "1     1308671310  <p>Test suite execution finished before execut...        1\n",
       "2     1308673361  <p>I am trying to clone defects from another t...        1\n",
       "3     1507656633                        Retest some function again.        1\n",
       "4     1507656638               enter the support needed at here ...        1\n",
       "..           ...                                                ...      ...\n",
       "899  22012641037  <div><span style=\"font-size: 12.18px;\">Hello,&...        0\n",
       "900  22012645565  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...        1\n",
       "901  22012704243  <div>The schedule test suite allow for the use...        1\n",
       "902  22012765885  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...        1\n",
       "903  22013190829  <p>Converting to enhancement...</p><p><br /></...        1\n",
       "\n",
       "[904 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "main(json_path,out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f2344",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f1ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading\n",
    "def data_loading(path,start_date=None,stop_date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    User can choose to \n",
    "    a) Load all json files following the agreed filename format\n",
    "    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and\n",
    "    stop_date must be used together)\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    \n",
    "    start_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files starting from start_date\n",
    "    - None: no start_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files starting from start_date will be loaded\n",
    "    \n",
    "    stop_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files until stop_date\n",
    "    - None: no stop_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files until stop_date will be loaded\n",
    "    '''\n",
    "    from datetime import datetime,timedelta\n",
    "    import pandas as pd\n",
    "    import glob, os, json\n",
    "    import re\n",
    "    \n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    date_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_date == None and stop_date == None :\n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "\n",
    "            pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "            match = re.search(pattern,file)\n",
    "                \n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "        print(\"Files read:\",file_list)                   \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)              \n",
    "                \n",
    "    else:\n",
    "        #convert start and stop string to datetime\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        stop = datetime.strptime(stop_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "        #iterate from start to stop dates by day and store dates in list\n",
    "        while start <= stop:\n",
    "            date_list.append(start)\n",
    "            start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "        #convert datetime objects to string\n",
    "        string_list =[d.strftime(\"%Y-%m-%d\") for d in date_list]\n",
    "#         print(string_list)\n",
    "        \n",
    "        for file in filenames: \n",
    "            \n",
    "            # search agreed file format pattern in the filename\n",
    "            for date in string_list: \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "                match = re.search(pattern,file)\n",
    "                \n",
    "                #if match is found\n",
    "                if match:\n",
    "                    pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "        print(\"Files read:\",file_list)     \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f9fc7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "def df_manipulation(df,col_selection=None,keep=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep columns in dataframe\n",
    "    2) Data impute: Impute NA rows with empty string\n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe     \n",
    "    col_selection [None/list]: - None [Default]: Keep all columns in dataframe \n",
    "                               - List: List of columns to keep in dataframe                      \n",
    "                                 \n",
    "    keep[None/string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. \n",
    "                      # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                      # - False : Drop all duplicates.                 \n",
    "    subset[list/None]: Subset of columns for identifying duplicates, use None if no column to select\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select column(s) \n",
    "    if col_selection != None:\n",
    "        df = df[col_selection]\n",
    "    \n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with empty string\n",
    "    impute_value = \"\"\n",
    "    df = df.fillna(impute_value)\n",
    "    print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    if keep==None:\n",
    "        keep=\"first\"\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def word_contractions(df):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \"\"\"\n",
    "    import contractions\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda text: \" \".join([contractions.fix(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_contract')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def lowercase(df):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    df[dataframe]: input dataframe\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    df = df.add_suffix('_lower')\n",
    "        \n",
    "    return df \n",
    "\n",
    "def remove_htmltag_url(df):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    #remove html tag\n",
    "    df = df.applymap(lambda text:BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True))\n",
    "    #remove url\n",
    "    df = df.replace('https?[://%]*\\S+',' ', regex=True) \n",
    "    \n",
    "    df = df.add_suffix('_tagrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_irrchar_punc(df,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex\n",
    "    format.    \n",
    "    params:    \n",
    "    df [dataframe]: input dataframe \n",
    "    characters[string]: input regex of characters to be removed  \n",
    "    \n",
    "    \"\"\"\n",
    "    import re \n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        df = df.replace(char,' ',regex = True)\n",
    "            \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    df = df.replace(r'\\\\+x[\\d\\D][\\d\\D]',' ',regex = True)\n",
    "        \n",
    "    #Remove special characters and punctuation\n",
    "    df = df.replace('[^\\w\\s]',' ',regex = True)\n",
    "    df = df.replace(r'_',' ',regex = True)\n",
    "    \n",
    "    df = df.add_suffix('_puncrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_num(df):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df=df.replace('\\d+',' ', regex=True) \n",
    "    df = df.add_suffix('_numrem')\n",
    "    \n",
    "    return df \n",
    "\n",
    "def remove_multwhitespace(df):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    df = df.add_suffix('_wsrem')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_stopwords(df,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        df = df.replace(pattern,' ', regex=True)\n",
    "    \n",
    "    df = df.add_suffix('_stoprem')\n",
    "                   \n",
    "    return df \n",
    "\n",
    "def remove_freqwords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "    df = df.add_suffix('_freqrem')\n",
    "    return df\n",
    "\n",
    "def remove_rarewords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n",
    "    df = df.add_suffix('_rarerem')\n",
    "    return df\n",
    "\n",
    "\n",
    "def custom_taxo(df,remove_taxo,include_taxo):\n",
    "    \"\"\"\n",
    "    User provides taxonomy to be removed or remained in the text\n",
    "    params:\n",
    "    df [dataframe]: input dataframe\n",
    "    remove_taxo[list]: list of taxonomy to be removed from text\n",
    "    include_taxo[list]: list of taxonomy to be maintained in text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    def taxo(text,remove_taxo,include_taxo):\n",
    "        for w in remove_taxo:\n",
    "        #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "            if all(phrase not in text for phrase in include_taxo): \n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text) \n",
    "            #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "            else: \n",
    "                if all(w not in phrase for phrase in include_taxo):\n",
    "                    pattern = r'\\b'+w+r'\\b'\n",
    "                    text = re.sub(pattern,' ', text) \n",
    "        return text \n",
    "    \n",
    "    df = df.applymap(lambda text: taxo(text,remove_taxo,include_taxo))    \n",
    "    df = df.add_suffix('_taxo')\n",
    "                \n",
    "    return df    \n",
    "\n",
    "def stem_words(df,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    stemmer_type[None/string]: input stemming method \n",
    "                                - None for Porter Stemmer\n",
    "                                - \"Lancaster\" for Lancaster Stemmer \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.stem import LancasterStemmer\n",
    "    \n",
    "    if stemmer_type == None:\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    df = df.applymap(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
    "    df = df.add_suffix('_stem')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def lemmatize_words(df,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy \n",
    "    params:\n",
    "    df[dataframe]: input dataframe\n",
    "    lemma_type[None/string]: input lemmatization method\n",
    "                            - None for WordNetLemmatizer\n",
    "                            - \"Spacy\" for Spacy    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import spacy\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    if lemma_type == None:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df = df.applymap(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "        \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        df = df.applymap(lambda text: \" \".join([word.lemma_ for word in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        df = df.applymap(lambda s:s.lower() if type(s) == str else s) \n",
    "        \n",
    "    df = df.add_suffix('_lemma')\n",
    "        \n",
    "    return df\n",
    "\n",
    "def feature_extraction(column,ngram_range=None,ascending=None,fe_type=None):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - TF-IDF(default choice) or Bag of words\n",
    "     \n",
    "    params:\n",
    "    column [series/DataFrame]: column selected for feature extraction \n",
    "                        - series: only one column is selected for feature extraction (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for feature extraction (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "                    \n",
    "    if ngram_range == None: #set ngram range as unigram by default\n",
    "        ngram_range=(1,1)\n",
    "        \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == None: #tfidf\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dce514a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsupervised learning\n",
    "import pandas as pd\n",
    "def kmeans_clustering(column,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for clustering \n",
    "                        - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"   \n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn import metrics\n",
    "\n",
    "    silhouette_avg_list = []\n",
    "    n_clusters_list = []\n",
    "    dicts = {}\n",
    "    \n",
    "    #call feature extraction function    \n",
    "    ascending = None \n",
    "    X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "    X = X.drop(index='sum')\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "    #user provides the number of clusters        \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "        print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "        \n",
    "            \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "    print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "    terms = vec_type.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "        print(\"\\n\")\n",
    "   \n",
    "               \n",
    "    return labels\n",
    "\n",
    "\n",
    "def lda(column,n_components,top_n_terms,ngram_range=None):\n",
    "    \"\"\"\n",
    "    LDA for unsupervised learning. Bag of words is selected for feature extraction\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for lda\n",
    "                        - series: only one column is selected for lda (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for lda (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in the lda_model\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import LatentDirichletAllocation\n",
    "    \n",
    "    #feature extraction\n",
    "    ascending = None\n",
    "    fe_type = \"bagofwords\"\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the LDA class \n",
    "    lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "    lda_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = lda_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "    \n",
    "    return topic_results.argmax(axis=1)\n",
    "\n",
    "\n",
    "def nmf(column,n_components,top_n_terms,fe_type,ngram_range=None):\n",
    "    \"\"\"\n",
    "    Non-negative matrix factorization for unsupervised learning.\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for NMF \n",
    "                        - series: only one column is selected for NMF (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for NMF (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in NMF\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import NMF\n",
    "    \n",
    "    #feature extraction\n",
    "    ngram_range = None\n",
    "    ascending = None\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the NMF class \n",
    "    nmf_model = NMF(n_components,random_state=42)\n",
    "    nmf_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = nmf_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbbf493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b2104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eac7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 1: DL -> DP -> Supervised learning\n",
    "# config 2: DL -> DP -> Unsupervised learning\n",
    "# config 3: DL -> DP -> Similarity metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8335bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file\n",
    "# import json\n",
    "# json_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "\n",
    "# with open(json_path+'config.json') as config_file:\n",
    "#     data = json.load(config_file)\n",
    "\n",
    "# path = data[\"DataLoading\"]['path']\n",
    "# start_date = data[\"DataLoading\"]['start_date']\n",
    "# stop_date = data[\"DataLoading\"]['stop_date']\n",
    "# print(path)\n",
    "# print(start_date)\n",
    "# print(stop_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11973277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     method #1\n",
    "#     [\"title\",\"Desc\",\"comment\"]\n",
    "    \n",
    "#     wordcont = data[\"df_manipulation\"][\"word_contractions\"]\n",
    "#     if wordcont:\n",
    "#         df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "#         df[\"desc_cont\"] = [word_contractions(text) for text in df[\"desc\"]]\n",
    "#         df[\"comment_cont\"] = [word_contractions(text) for text in df[\"comment\"]\n",
    "    \n",
    "#     method #2\n",
    "#     df (title,desc,comments) -> df(title,desc,comments) + df (title_cont,desc_cont,comments_cont)\n",
    "#     df (title,desc,comments) -> concat df(title,desc,comments) [df1]\n",
    "#                               columnbind(df,df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3ebece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = {\n",
    "#     \"DataLoading\": {\"path\": path, \"start_date\": None,\"stop_date\":None},\n",
    "#     \"df_manipulation\": {\"col_selection\":[\"id\",\"description\"],\"keep\": None,\"subset\":None},    \n",
    "#     \"word_contractions\": {\"enable\": True},\n",
    "#     \"lowercase\": {\"enable\": True},\n",
    "#     \"remove_htmltag_url\": {\"enable\":True},\n",
    "#     \"remove_irrchar_punc\":{\"enable\":True,\"char\":None},\n",
    "#     \"remove_num\":{\"enable\":True},\n",
    "#     \"remove_multwhitespace\":{\"enable\":True},\n",
    "#     \"remove_stopwords\":{\"enable\":True,\"extra_sw\":None,\"remove_sw\": None},\n",
    "#     \"remove_freqwords\":{\"enable\":True,\"n\":10},\n",
    "#     \"remove_rarewords\":{\"enable\":True,\"n\":10},\n",
    "#     \"stem_words\":{\"enable\":False,\"stemmer_type\":None},\n",
    "#     \"lemmatize_words\":{\"enable\":True,\"lemma_type\":None}\n",
    "\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
