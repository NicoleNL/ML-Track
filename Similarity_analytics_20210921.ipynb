{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889562e",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0951373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user input file path\n",
    "path = 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae60212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(path,start_date=None,stop_date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    User can choose to \n",
    "    a) Load all json files following the agreed filename format\n",
    "    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and\n",
    "    stop_date must be used together)\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    \n",
    "    start_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files starting from start_date\n",
    "    - None: no start_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files starting from start_date will be loaded\n",
    "    \n",
    "    stop_date[None/string in YYYY-MM-DD format](optional,default is None): \n",
    "    User can choose to load files until stop_date\n",
    "    - None: no stop_date is provided, all files are loaded\n",
    "    - string in YYYY-MM-DD format: files until stop_date will be loaded\n",
    "    '''\n",
    "    from datetime import datetime,timedelta\n",
    "    import pandas as pd\n",
    "    import glob, os, json\n",
    "    import re\n",
    "\n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    date_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    if start_date == None and stop_date == None :\n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "\n",
    "            pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "            match = re.search(pattern,file)\n",
    "                \n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "        print(\"Files read:\",file_list)                   \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)              \n",
    "                \n",
    "    else:\n",
    "        #convert start and stop string to datetime\n",
    "        start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "        stop = datetime.strptime(stop_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "        #iterate from start to stop dates by day and store dates in list\n",
    "        while start <= stop:\n",
    "            date_list.append(start)\n",
    "            start = start + timedelta(days=1)  # increase day one by one\n",
    "\n",
    "        #convert datetime objects to string\n",
    "        string_list =[d.strftime(\"%Y-%m-%d\") for d in date_list]\n",
    "#         print(string_list)\n",
    "        \n",
    "        for file in filenames: \n",
    "            \n",
    "            # search agreed file format pattern in the filename\n",
    "            for date in string_list: \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "                match = re.search(pattern,file)\n",
    "                \n",
    "                #if match is found\n",
    "                if match:\n",
    "                    pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "        print(\"Files read:\",file_list)     \n",
    "        for file in file_list:\n",
    "            with open(file) as f:\n",
    "                #flatten json into pd dataframe\n",
    "                json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                json_data = pd.DataFrame(json_data)\n",
    "                #label which file each row is from \n",
    "                json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "            df = df.append(json_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b008a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = data_loading(path,start_date = \"2021-08-25\",stop_date = \"2021-08-25\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a04a4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read: ['C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)1_firstSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-08-25)3_secondSet_1.json', 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/(2021-10-11)3_secondSet_1.json']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>hierarchy_id</th>\n",
       "      <th>rev</th>\n",
       "      <th>tenant</th>\n",
       "      <th>subject</th>\n",
       "      <th>is_current</th>\n",
       "      <th>hierarchy_path</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>record_type</th>\n",
       "      <th>row_num</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>provide method to update GIO fields from git r...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>++++1562123662 fbakhda\\nHi @Panceac, Cornel Eu...</td>\n",
       "      <td>2021-07-21 12:30:31.387</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308651592/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>1</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>Test suite execution terminates before executi...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>++++1361513318 cmoala\\nsys_tsdval@GL-IAF1-V-S0...</td>\n",
       "      <td>2021-05-04 09:30:00.320</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308671310/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>2</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>Cloning defects from another test cycle is not...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>++++1361514315 cmoala\\nObserved that only impl...</td>\n",
       "      <td>2021-05-20 11:47:18.927</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308673361/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>3</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>[Testing Only] this is enhancement only</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td></td>\n",
       "      <td>2020-03-13 10:16:18.703</td>\n",
       "      <td></td>\n",
       "      <td>31</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1507656633/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>4</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>[Testing Only] this is consultation only</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>++++1661488832 prajput\\nHSDES testing. Please ...</td>\n",
       "      <td>2020-06-01 09:49:55.913</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1507656638/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>5</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-26 13:19:20.430</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012641037/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>900</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-05-20 13:03:09.327</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012645565/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>901</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-04-26 10:04:12.410</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012704243/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>902</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30 00:35:58.927</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012765885/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>903</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-13 15:41:16.507</td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22013190829/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>904</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2712 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0     1308651592  provide method to update GIO fields from git r...   \n",
       "1     1308671310  Test suite execution terminates before executi...   \n",
       "2     1308673361  Cloning defects from another test cycle is not...   \n",
       "3     1507656633            [Testing Only] this is enhancement only   \n",
       "4     1507656638           [Testing Only] this is consultation only   \n",
       "..           ...                                                ...   \n",
       "899  22012641037                                                NaN   \n",
       "900  22012645565                                                NaN   \n",
       "901  22012704243                                                NaN   \n",
       "902  22012765885                                                NaN   \n",
       "903  22013190829                                                NaN   \n",
       "\n",
       "                                           description  \\\n",
       "0    Please provide a way to update GIO fields from...   \n",
       "1    <p>Test suite execution finished before execut...   \n",
       "2    <p>I am trying to clone defects from another t...   \n",
       "3                          Retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  <div>The schedule test suite allow for the use...   \n",
       "902  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                                              comments  \\\n",
       "0    ++++1562123662 fbakhda\\nHi @Panceac, Cornel Eu...   \n",
       "1    ++++1361513318 cmoala\\nsys_tsdval@GL-IAF1-V-S0...   \n",
       "2    ++++1361514315 cmoala\\nObserved that only impl...   \n",
       "3                                                        \n",
       "4    ++++1661488832 prajput\\nHSDES testing. Please ...   \n",
       "..                                                 ...   \n",
       "899                                                NaN   \n",
       "900                                                NaN   \n",
       "901                                                NaN   \n",
       "902                                                NaN   \n",
       "903                                                NaN   \n",
       "\n",
       "                updated_date hierarchy_id rev     tenant  subject is_current  \\\n",
       "0    2021-07-21 12:30:31.387                8  iot_platf  support          1   \n",
       "1    2021-05-04 09:30:00.320               11  iot_platf  support          1   \n",
       "2    2021-05-20 11:47:18.927                9  iot_platf  support          1   \n",
       "3    2020-03-13 10:16:18.703               31  iot_platf  support          1   \n",
       "4    2020-06-01 09:49:55.913               19  iot_platf  support          1   \n",
       "..                       ...          ...  ..        ...      ...        ...   \n",
       "899  2021-03-26 13:19:20.430               11  iot_platf  support          1   \n",
       "900  2021-05-20 13:03:09.327               11  iot_platf  support          1   \n",
       "901  2021-04-26 10:04:12.410                9  iot_platf  support          1   \n",
       "902  2021-06-30 00:35:58.927               14  iot_platf  support          1   \n",
       "903  2021-07-13 15:41:16.507               16  iot_platf  support          1   \n",
       "\n",
       "                          hierarchy_path   parent_id record_type row_num  \\\n",
       "0     /1201559436/1208431055/1308651592/  1208431055      parent       1   \n",
       "1     /1201559436/1208431055/1308671310/  1208431055      parent       2   \n",
       "2     /1201559436/1208431055/1308673361/  1208431055      parent       3   \n",
       "3     /1201559436/1208431055/1507656633/  1208431055      parent       4   \n",
       "4     /1201559436/1208431055/1507656638/  1208431055      parent       5   \n",
       "..                                   ...         ...         ...     ...   \n",
       "899  /1201559436/1208431055/22012641037/  1208431055      parent     900   \n",
       "900  /1201559436/1208431055/22012645565/  1208431055      parent     901   \n",
       "901  /1201559436/1208431055/22012704243/  1208431055      parent     902   \n",
       "902  /1201559436/1208431055/22012765885/  1208431055      parent     903   \n",
       "903  /1201559436/1208431055/22013190829/  1208431055      parent     904   \n",
       "\n",
       "                               file  \n",
       "0     (2021-08-25)1_firstSet_1.json  \n",
       "1     (2021-08-25)1_firstSet_1.json  \n",
       "2     (2021-08-25)1_firstSet_1.json  \n",
       "3     (2021-08-25)1_firstSet_1.json  \n",
       "4     (2021-08-25)1_firstSet_1.json  \n",
       "..                              ...  \n",
       "899  (2021-10-11)3_secondSet_1.json  \n",
       "900  (2021-10-11)3_secondSet_1.json  \n",
       "901  (2021-10-11)3_secondSet_1.json  \n",
       "902  (2021-10-11)3_secondSet_1.json  \n",
       "903  (2021-10-11)3_secondSet_1.json  \n",
       "\n",
       "[2712 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_loading(path,start_date = None,stop_date = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c03d8",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "### a) Dataframe manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baadc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(df,how,keep=\"first\",cols_tokeep=None,cols_todrop=None,impute_value=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep or drop columns in dataframe\n",
    "    2) Data impute: Impute or drop NA rows \n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    how[string]: Drop rows when we have at least one NA or all NA. Choose\n",
    "                      # - \"all\": Drop row with all NA\n",
    "                      # - \"any\": Drop row with at least one NA\n",
    "    keep[string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. \n",
    "                      # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                      # - False : Drop all duplicates.\n",
    "    cols_tokeep [list/None][DEFAULT]: list of columns to keep, if there is no list use None \n",
    "    cols_todrop [list/None]: list of columns to drop, if there is no list use None \n",
    "    impute_value [string/None]: value to be imputed (i.e \"\" for empty string). If no value to be imputed but there are \n",
    "                        rows to be dropped use None\n",
    "                  \n",
    "    subset[list/None]: Subset of columns for dropping NA and identifying duplicates, use None if no column to select\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select columns or drop unwanted columns\n",
    "    if cols_tokeep != None:\n",
    "        df = df[cols_tokeep]\n",
    "    if cols_todrop != None:\n",
    "        df = df.drop(cols_todrop,axis=1)\n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with user's choice of imputation value\n",
    "    if impute_value != None:\n",
    "        df = df.fillna(impute_value)\n",
    "        print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "        \n",
    "    else: # drop rows with NA values\n",
    "        df= df.dropna(axis=0, how=how,subset=subset)\n",
    "        print(\"Number of null values in df after dropping NA rows:\\n\",df.isnull().sum())\n",
    "        print(\"Shape of df after dropping NA rows:\",df.shape)\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    if keep == None:\n",
    "        keep = \"first\"\n",
    "        \n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8931eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df before manipulation: (2712, 15)\n",
      "Shape of df after selecting columns: (2712, 2)\n",
      "Number of null values in df:\n",
      " title          1808\n",
      "description       0\n",
      "dtype: int64\n",
      "Number of null values in df after dropping NA rows:\n",
      " title          0\n",
      "description    0\n",
      "dtype: int64\n",
      "Shape of df after dropping NA rows: (904, 2)\n",
      "Number of duplicates in the df: 0\n",
      "Shape of df after manipulation: (904, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update GIO fields from git r...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test suite execution terminates before executi...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloning defects from another test cycle is not...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Testing Only] this is enhancement only</td>\n",
       "      <td>Retest some function again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Testing Only] this is consultation only</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  provide method to update GIO fields from git r...   \n",
       "1  Test suite execution terminates before executi...   \n",
       "2  Cloning defects from another test cycle is not...   \n",
       "3            [Testing Only] this is enhancement only   \n",
       "4           [Testing Only] this is consultation only   \n",
       "\n",
       "                                         description  \n",
       "0  Please provide a way to update GIO fields from...  \n",
       "1  <p>Test suite execution finished before execut...  \n",
       "2  <p>I am trying to clone defects from another t...  \n",
       "3                        Retest some function again.  \n",
       "4               enter the support needed at here ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\"],cols_todrop=None,impute_value=None,subset=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2089e",
   "metadata": {},
   "source": [
    "\n",
    "### b) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1540a0a",
   "metadata": {},
   "source": [
    "### 2) Expand contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "453da296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def word_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return \" \".join([contractions.fix(word) for word in text.split()])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff07071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"desc_cont\"]=  [word_contractions(text) for text in df[\"description\"]]\n",
    "df[\"comments_cont\"]=  [word_contractions(text) for text in df[\"comments\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47492958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e49df",
   "metadata": {},
   "source": [
    "### 3) Convert all characters into lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01121fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return text.lower() if type(text) == str else text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"desc_lower\"]= [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "df[\"comments_lower\"]= [lowercase(text) for text in df[\"comments_cont\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_lower\",\"desc_lower\",\"comments_lower\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a61d4",
   "metadata": {},
   "source": [
    "### 4) Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3ba0c",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[[\"title_rare\",\"desc_rare\",\"comments_rare\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a97f0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def stem_words(text,stemmer_type=None):\n",
    "    \"\"\"\n",
    "    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer \n",
    "    params:\n",
    "    text[string]: input string \n",
    "    stemmer_type[None/string]: input stemming method \n",
    "                                - None for Porter Stemmer\n",
    "                                - \"Lancaster\" for Lancaster Stemmer \n",
    "    \"\"\"\n",
    "    if stemmer_type == None:\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_por\"] = [stem_words(text,stemmer_type=None) for text in df1[\"title_rare\"]]\n",
    "df1[\"desc_stem_por\"] = [stem_words(text,stemmer_type=None) for text in df1[\"desc_rare\"]]\n",
    "df1[\"comments_stem_por\"]= [stem_words(text,stemmer_type=None) for text in df1[\"comments_rare\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"title_rare\"]]\n",
    "df1[\"desc_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"desc_rare\"]]\n",
    "df1[\"comments_stem_lan\"]= [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"comments_rare\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48f7de",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11146a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(column,lemma_type=None):\n",
    "    \"\"\"\n",
    "    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy \n",
    "    params:\n",
    "    column[series]: input series/column to be lemmatized\n",
    "    lemma_type[None/string]: input lemmatization method\n",
    "                            - None for WordNetLemmatizer\n",
    "                            - \"Spacy\" for Spacy    \n",
    "    \"\"\"\n",
    "    if lemma_type == None:\n",
    "     \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return column.apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "   \n",
    "    \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        column = column.apply(lambda text: \" \".join([w.lemma_ for w in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        column = column.apply(lambda text: text.lower() if type(text) == str else text )\n",
    "        \n",
    "        return column\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7092bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spacy\n",
    "df2[\"title_lemma_spacy\"] = lemmatize_words(column= df2[\"title_rare\"],lemma_type=\"Spacy\")\n",
    "df2[\"desc_lemma_spacy\"] = lemmatize_words(column= df2[\"desc_rare\"],lemma_type=\"Spacy\")\n",
    "df2[\"comments_lemma_spacy\"] = lemmatize_words(column= df2[\"comments_rare\"],lemma_type=\"Spacy\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18243072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordNetLemmatizer\n",
    "df2[\"title_lemma_word\"] = lemmatize_words(column= df2[\"title_rare\"],lemma_type=None)\n",
    "df2[\"desc_lemma_word\"] = lemmatize_words(column= df2[\"desc_rare\"],lemma_type=None)\n",
    "df2[\"comments_lemma_word\"] = lemmatize_words(column= df2[\"comments_rare\"],lemma_type=None)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d27c8",
   "metadata": {},
   "source": [
    "### b) Noise filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661e07",
   "metadata": {},
   "source": [
    "### 1) Remove html tag and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def remove_htmltag_url(text):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    text [string]: input string\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    #remove html tag\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True) \n",
    "    #remove url\n",
    "    text_clean = re.sub('https?[://%]*\\S+', ' ',text) \n",
    "    return text_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f23e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"desc_tag\"]= [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "df[\"comments_tag\"]= [remove_htmltag_url(text) for text in df[\"comments_lower\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2f91a",
   "metadata": {},
   "source": [
    "### 3) Remove irrelevant characters, punctuation, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02440a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_tag\",\"desc_tag\",\"comments_tag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a84fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_irrchar_punc(text,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex\n",
    "    format.    \n",
    "    params:    \n",
    "    text[string]: input string \n",
    "    characters[string]: input regex of characters to be removed\n",
    "    \"\"\"\n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        text = re.sub(char, ' ',text) \n",
    "    \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    text = re.sub(r'\\\\+x[\\d\\D][\\d\\D]', ' ',text) \n",
    "    \n",
    "    #Remove special characters and punctuation\n",
    "    text = re.sub('[^\\w\\s]', ' ',text) \n",
    "    text = re.sub(r'_', ' ',text) \n",
    "   \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a76f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"desc_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "df[\"comments_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d676006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1] #desc before rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27350f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4] #desc rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2235b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#special character removal added by user\n",
    "char = '\\++\\d+'\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=char) for text in df[\"title_tag\"]]\n",
    "df[\"desc_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"desc_tag\"]]\n",
    "df[\"comments_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec85f0",
   "metadata": {},
   "source": [
    "### 3) Remove numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_rem\",\"desc_rem\",\"comments_rem\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c131608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub('\\d+', ' ',text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56627dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"desc_num\"]= [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "df[\"comments_num\"]= [remove_num(text) for text in df[\"comments_rem\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f0d60",
   "metadata": {},
   "source": [
    "### 4) Remove multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_num\",\"desc_num\",\"comments_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "660093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multwhitespace(text):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub(' +', ' ',text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"desc_white\"]= [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "df[\"comments_white\"]= [remove_multwhitespace(text) for text in df[\"comments_num\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb117fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e5d5",
   "metadata": {},
   "source": [
    "### 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_white\",\"desc_white\",\"comments_white\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "609fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    text[string]: input string\n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        text = re.sub(pattern,' ', text)\n",
    "                   \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words/phrase to be added to the stop words \n",
    "# extra_sw = ['hsdes',\"testing\"]\n",
    "#list of words/phrase to be removed from stop words\n",
    "# remove_sw = [\"i\",\"am\"]\n",
    "\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"desc_white\"]]\n",
    "df[\"comments_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"comments_white\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905229f9",
   "metadata": {},
   "source": [
    "### 5) Remove frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37464615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a389eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freqwords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    column[series]: input column to remove frequent words\n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed from column:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "df[\"comments_freq\"] = remove_freqwords(df[\"comments_stop\"],n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada84766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952af778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ef266",
   "metadata": {},
   "source": [
    "### 6) Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50d0e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rarewords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    column[series]: input column to remove rare words\n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the rare words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed from column:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "        \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "df[\"desc_rare\"] = remove_rarewords(df[\"desc_stop\"],n)\n",
    "df[\"comments_rare\"] = remove_rarewords(df[\"comments_stop\"],n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,1] #converting is rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4224f",
   "metadata": {},
   "source": [
    "### c) Custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20da5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_tokenization_split(column,delim =None):\n",
    "    \"\"\"\n",
    "    Custom tokenization using split() \n",
    "    params:\n",
    "    column[series]: input column           \n",
    "    delim[None/string],default delimiter (delim=None) is whitespace: specify delimiter to separate strings\n",
    "                        - None: delimiter is white space\n",
    "                        - string: delimiter is the string specified       \n",
    "    \"\"\"\n",
    "    \n",
    "    if delim==None:\n",
    "        print(\"Text is split by whitespace\") #default delimiter is space if not specified \n",
    "\n",
    "    else:\n",
    "        print(\"Text is split by:\", delim) #can accept one or more delimiter\n",
    "\n",
    "    return column.apply(lambda text: text.split() if delim==None else text.split(delim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe926d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split\n",
    "df[\"title_token\"]= cust_tokenization_split(column = df[\"title_rare\"],delim= None) \n",
    "df[\"desc_token\"]= cust_tokenization_split(column = df[\"desc_rare\"],delim= None)\n",
    "df[\"comments_token\"]= cust_tokenization_split(column = df[\"comments_rare\"],delim= None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271ae1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "def cust_tokenization_nltk(column,token_type):\n",
    "    \"\"\"\n",
    "    Custom tokenization using NLTK \n",
    "    params:\n",
    "    column[series]: input column \n",
    "    token_type[\"string\"]: type of nltk tokenization\n",
    "    a) token_type = \"WordToken\" tokenizes a string into a list of words\n",
    "    b) token_type = \"SentToken\" tokenizes a string containing sentences into a list of sentences\n",
    "    c) token_type = \"WhiteSpaceToken\" tokenizes a string on whitespace (space, tab, newline)\n",
    "    d) token_type = \"WordPunctTokenizer\" tokenizes a string on punctuations\n",
    "    \"\"\"\n",
    "    if token_type == \"WordToken\":\n",
    "        tokenizer = word_tokenize\n",
    "    if token_type == \"SentToken\":\n",
    "        tokenizer = sent_tokenize\n",
    "    if token_type == \"WhiteSpaceToken\":\n",
    "        tokenizer = WhitespaceTokenizer().tokenize\n",
    "    if token_type == \"WordPunctTokenizer\":\n",
    "        tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "    return column.apply(lambda text: tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1827f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk\n",
    "df[\"title_token\"]= cust_tokenization_nltk(column = df[\"title_rare\"],token_type= \"WordToken\") \n",
    "df[\"desc_token\"]= cust_tokenization_nltk(column = df[\"desc_rare\"],token_type=\"WordToken\")\n",
    "df[\"comments_token\"]= cust_tokenization_nltk(column = df[\"comments_rare\"],token_type= \"WordToken\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013f586",
   "metadata": {},
   "source": [
    "## d) Custom taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4535b2",
   "metadata": {},
   "source": [
    "### i) Configurability for user to provide taxonomy mapping (to remove/remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b03012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_rare\",\"desc_rare\",\"comments_rare\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de60e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_taxo(df,remove_taxo,include_taxo):\n",
    "    \"\"\"\n",
    "    User provides taxonomy to be removed or remained in the text. \n",
    "    a) user wants to remove taxonomies only -> input a list of taxonomies to be removed in remove_taxo \n",
    "    b) user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases \n",
    "    (i.e remove taxo \"test\" but  \"test\" remains in \"test cycle\") -> input a list of taxonomies to be removed in remove_taxo and list of\n",
    "    phrases for the taxonomy to remain in include_taxo\n",
    "    \n",
    "    params:\n",
    "    df [dataframe]: input dataframe\n",
    "    remove_taxo[list/regex]: list of taxonomy to be removed from text\n",
    "    include_taxo[list/None]: list of taxonomy to be maintained in text\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import pandas as pd \n",
    "    \n",
    "    def convert(text,remove_taxo):  \n",
    "        \"\"\"\n",
    "        Uses regex given in remove_taxo to find and return all matches \n",
    "        \"\"\"\n",
    "        match = re.findall(remove_taxo,text)\n",
    "        if match:                 \n",
    "            new_row = {'Match':match}\n",
    "            return(new_row)\n",
    "        \n",
    "    #if remove_taxo is regex call convert function to get all matches as a list\n",
    "    if type(remove_taxo) == str: \n",
    "        cv_list = []\n",
    "        for i in range(len(df.columns)):\n",
    "            for text in df.iloc[:,i]:\n",
    "                cv = convert(text,remove_taxo)\n",
    "                if cv:\n",
    "                    cv_list.append(cv)\n",
    "        #             print(cv_list)\n",
    "\n",
    "        cv_df = pd.DataFrame(cv_list)\n",
    "        remove_taxo = list(cv_df[\"Match\"].apply(pd.Series).stack().unique())\n",
    "        print(\"Remove_taxo_list:\", remove_taxo)\n",
    "        \n",
    "    def taxo(text,remove_taxo,include_taxo): \n",
    "        if remove_taxo != None and include_taxo != None: #user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases (i.e remove \"test\" but remain \"test\" in \"test cyccle\")\n",
    "\n",
    "            for w in remove_taxo:\n",
    "            #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "                if all(phrase not in text for phrase in include_taxo): \n",
    "                    pattern = r'\\b'+w+r'\\b'\n",
    "                    text = re.sub(pattern,' ', text) \n",
    "                #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "                else: \n",
    "                    if all(w not in phrase for phrase in include_taxo):\n",
    "                        pattern = r'\\b'+w+r'\\b'\n",
    "                        text = re.sub(pattern,' ', text) \n",
    "                        \n",
    "        if remove_taxo != None and include_taxo == None: #user wants to remove taxonomies only:\n",
    "            for w in remove_taxo: #remove_taxo in list of words\n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text)\n",
    "                 \n",
    "        return text \n",
    "    \n",
    "    \n",
    "    df = df.applymap(lambda text: taxo(text,remove_taxo,include_taxo))     \n",
    "    df = df.add_suffix('_taxo')\n",
    "                \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0cd95f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove_taxo_list: ['test suite', 'test cycle', 'test execution', 'test case', 'test entry', 'test result', 'test cases', 'test schedule', 'test build', 'test recipe', 'test to', 'test schedular', 'test stuck', 'test tools', 'test components', 'test sut', 'test cycles', 'test procedure', 'test occasionally', 'test report', 'test please', 'test group', 'test page', 'test script', 'test component', 'test id', 'test excution', 'test never', 'test sst', 'test automation', 'test results', 'test type', 'test did', 'test center', 'test run', 'test in', 'test reporting', 'test list', 'test completed', 'test ended', 'test as', 'test show', 'test is', 'test python', 'test request', 'test which', 'test customization', 'test and', 'test planning', 'test after', 'test configuration', 'test through', 'test gio', 'test modification', 'test client', 'test test', 'test passing', 'test executed', 'test host', 'test command', 'test only', 'test project', 'test hang', 'test config', 'test groups', 'test skipping', 'test stable', 'test cylce', 'test executor', 'test setup', 'test limit', 'test not', 'test campaign', 'test require', 'test completion', 'test dynamic', 'test api', 'test from', 'test suites', 'test automatically', 'test priority', 'test some', 'test had', 'test overall', 'test failed', 'test scripts', 'test tool', 'test tried', 'test always', 'test scheduler', 'test pen', 'test cyles', 'test observed', 'test if', 'test tagging', 'test set', 'test plan', 'test status', 'test first', 'test integration', 'test validation', 'test environment', 'test coverage', 'test that', 'test data', 'test link', 'test testing', 'test except', 'test update', 'test value', 'test changes', 'test pass', 'test category', 'test ma', 'test description', 'test should', 'test for', 'test again', 'test exectuion', 'test recipes', 'test was', 'test jsd', 'test the', 'test listing', 'test wlan', 'test with', 'test up', 'test were', 'test but', 'test has', 'test verdict', 'test procedures', 'test detail', 'test details', 'test level', 'test will', 'test version', 'test properly', 'test however', 'test artifact', 'test progress', 'test you', 'test i', 'test a', 'test consolidation', 'test on', 'test metric', 'test manually', 'test contents', 'test one', 'test summary', 'test testcyclegroup', 'test we', 'test apps', 'test runs', 'test jenkins', 'test image', 'test engine', 'test kmb', 'test crash', 'test args', 'test steps', 'test trial', 'test observation', 'test centre', 'test statistics', 'test thanks', 'test usually', 'test requirement', 'test step', 'test c', 'test excel', 'test does', 'test other', 'test interface', 'test yes', 'test field', 'test true', 'test tc', 'test contest', 'test sub', 'test content', 'test syncing', 'test complexity', 'test names', 'test times', 'test log', 'test pipeline', 'test modemtool', 'test usf', 'test framework', 'test mar', 'test job', 'test where', 'test bsp', 'test started', 'test exiting', 'test do', 'test info', 'test yocto', 'test owner', 'test bed', 'test could', 'test intf', 'test can', 'test this', 'test optimization', 'test remaiing', 'test are', 'test otherwise', 'test bronze', 'test ordered', 'test start', 'test parameter', 'test smartx', 'test release', 'test name', 'test cannot', 'test jsonfile', 'test plugin', 'test arguments', 'test at']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean_taxo</th>\n",
       "      <th>desc_clean_taxo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update gio fields from git r...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test suite execution terminates before executi...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloning defects from another   is not working</td>\n",
       "      <td>i am trying to clone defects from another   i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing only this is enhancement only</td>\n",
       "      <td>retest some function again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing only this is consultation only</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>import gc time global domain artifact in gio f...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>kpi metric phase extract kpi metric trend acr...</td>\n",
       "      <td>hi gio team thank you for providing kpi metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>ability to clone schedule   from other programs</td>\n",
       "      <td>the schedule   allow for the user to clone   w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>kpi metric enhance kpi feature to plot graphs...</td>\n",
       "      <td>hi gio team thank you for providing kpi featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>is there a way to pull all   for a program in ...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_clean_taxo  \\\n",
       "0    provide method to update gio fields from git r...   \n",
       "1    test suite execution terminates before executi...   \n",
       "2        cloning defects from another   is not working   \n",
       "3                testing only this is enhancement only   \n",
       "4               testing only this is consultation only   \n",
       "..                                                 ...   \n",
       "899  import gc time global domain artifact in gio f...   \n",
       "900   kpi metric phase extract kpi metric trend acr...   \n",
       "901    ability to clone schedule   from other programs   \n",
       "902   kpi metric enhance kpi feature to plot graphs...   \n",
       "903  is there a way to pull all   for a program in ...   \n",
       "\n",
       "                                       desc_clean_taxo  \n",
       "0    please provide a way to update gio fields from...  \n",
       "1    test suite execution finished before executing...  \n",
       "2    i am trying to clone defects from another   i ...  \n",
       "3                          retest some function again   \n",
       "4                    enter the support needed at here   \n",
       "..                                                 ...  \n",
       "899  hello please import time global domain time kp...  \n",
       "900  hi gio team thank you for providing kpi metric...  \n",
       "901  the schedule   allow for the user to clone   w...  \n",
       "902  hi gio team thank you for providing kpi featur...  \n",
       "903  converting to enhancement would like the abili...  \n",
       "\n",
       "[904 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_taxo(df1,remove_taxo = r'test \\w+',include_taxo=[\"test suite execution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f41a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove_taxo_list: ['test suite', 'test cycle', 'test execution', 'test case', 'test entry', 'test result', 'test cases', 'test schedule', 'test build', 'test recipe', 'test to', 'test schedular', 'test stuck', 'test tools', 'test components', 'test sut', 'test cycles', 'test procedure', 'test occasionally', 'test report', 'test please', 'test group', 'test page', 'test script', 'test component', 'test id', 'test excution', 'test never', 'test sst', 'test automation', 'test results', 'test type', 'test did', 'test center', 'test run', 'test in', 'test reporting', 'test list', 'test completed', 'test ended', 'test as', 'test show', 'test is', 'test python', 'test request', 'test which', 'test customization', 'test and', 'test planning', 'test after', 'test configuration', 'test through', 'test gio', 'test modification', 'test client', 'test test', 'test passing', 'test executed', 'test host', 'test command', 'test only', 'test project', 'test hang', 'test config', 'test groups', 'test skipping', 'test stable', 'test cylce', 'test executor', 'test setup', 'test limit', 'test not', 'test campaign', 'test require', 'test completion', 'test dynamic', 'test api', 'test from', 'test suites', 'test automatically', 'test priority', 'test some', 'test had', 'test overall', 'test failed', 'test scripts', 'test tool', 'test tried', 'test always', 'test scheduler', 'test pen', 'test cyles', 'test observed', 'test if', 'test tagging', 'test set', 'test plan', 'test status', 'test first', 'test integration', 'test validation', 'test environment', 'test coverage', 'test that', 'test data', 'test link', 'test testing', 'test except', 'test update', 'test value', 'test changes', 'test pass', 'test category', 'test ma', 'test description', 'test should', 'test for', 'test again', 'test exectuion', 'test recipes', 'test was', 'test jsd', 'test the', 'test listing', 'test wlan', 'test with', 'test up', 'test were', 'test but', 'test has', 'test verdict', 'test procedures', 'test detail', 'test details', 'test level', 'test will', 'test version', 'test properly', 'test however', 'test artifact', 'test progress', 'test you', 'test i', 'test a', 'test consolidation', 'test on', 'test metric', 'test manually', 'test contents', 'test one', 'test summary', 'test testcyclegroup', 'test we', 'test apps', 'test runs', 'test jenkins', 'test image', 'test engine', 'test kmb', 'test crash', 'test args', 'test steps', 'test trial', 'test observation', 'test centre', 'test statistics', 'test thanks', 'test usually', 'test requirement', 'test step', 'test c', 'test excel', 'test does', 'test other', 'test interface', 'test yes', 'test field', 'test true', 'test tc', 'test contest', 'test sub', 'test content', 'test syncing', 'test complexity', 'test names', 'test times', 'test log', 'test pipeline', 'test modemtool', 'test usf', 'test framework', 'test mar', 'test job', 'test where', 'test bsp', 'test started', 'test exiting', 'test do', 'test info', 'test yocto', 'test owner', 'test bed', 'test could', 'test intf', 'test can', 'test this', 'test optimization', 'test remaiing', 'test are', 'test otherwise', 'test bronze', 'test ordered', 'test start', 'test parameter', 'test smartx', 'test release', 'test name', 'test cannot', 'test jsonfile', 'test plugin', 'test arguments', 'test at']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean_taxo</th>\n",
       "      <th>desc_clean_taxo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update gio fields from git r...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>execution terminates before executing all tests</td>\n",
       "      <td>execution finished before executing all test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloning defects from another   is not working</td>\n",
       "      <td>i am trying to clone defects from another   i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing only this is enhancement only</td>\n",
       "      <td>retest some function again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing only this is consultation only</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>import gc time global domain artifact in gio f...</td>\n",
       "      <td>hello please import time global domain time kp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>kpi metric phase extract kpi metric trend acr...</td>\n",
       "      <td>hi gio team thank you for providing kpi metric...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>ability to clone schedule   from other programs</td>\n",
       "      <td>the schedule   allow for the user to clone   w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>kpi metric enhance kpi feature to plot graphs...</td>\n",
       "      <td>hi gio team thank you for providing kpi featur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>is there a way to pull all   for a program in ...</td>\n",
       "      <td>converting to enhancement would like the abili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>904 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_clean_taxo  \\\n",
       "0    provide method to update gio fields from git r...   \n",
       "1      execution terminates before executing all tests   \n",
       "2        cloning defects from another   is not working   \n",
       "3                testing only this is enhancement only   \n",
       "4               testing only this is consultation only   \n",
       "..                                                 ...   \n",
       "899  import gc time global domain artifact in gio f...   \n",
       "900   kpi metric phase extract kpi metric trend acr...   \n",
       "901    ability to clone schedule   from other programs   \n",
       "902   kpi metric enhance kpi feature to plot graphs...   \n",
       "903  is there a way to pull all   for a program in ...   \n",
       "\n",
       "                                       desc_clean_taxo  \n",
       "0    please provide a way to update gio fields from...  \n",
       "1      execution finished before executing all test...  \n",
       "2    i am trying to clone defects from another   i ...  \n",
       "3                          retest some function again   \n",
       "4                    enter the support needed at here   \n",
       "..                                                 ...  \n",
       "899  hello please import time global domain time kp...  \n",
       "900  hi gio team thank you for providing kpi metric...  \n",
       "901  the schedule   allow for the user to clone   w...  \n",
       "902  hi gio team thank you for providing kpi featur...  \n",
       "903  converting to enhancement would like the abili...  \n",
       "\n",
       "[904 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_taxo(df1,remove_taxo = r'test \\w+',include_taxo=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da280dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update GIO fields from git r...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test suite execution terminates before executi...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloning defects from another test cycle is not...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Testing Only] this is enhancement only</td>\n",
       "      <td>Retest some function again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Testing Only] this is consultation only</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  provide method to update GIO fields from git r...   \n",
       "1  Test suite execution terminates before executi...   \n",
       "2  Cloning defects from another test cycle is not...   \n",
       "3            [Testing Only] this is enhancement only   \n",
       "4           [Testing Only] this is consultation only   \n",
       "\n",
       "                                         description  \n",
       "0  Please provide a way to update GIO fields from...  \n",
       "1  <p>Test suite execution finished before execut...  \n",
       "2  <p>I am trying to clone defects from another t...  \n",
       "3                        Retest some function again.  \n",
       "4               enter the support needed at here ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "# df1 = df1[[\"title\",\"description\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67b55277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"desc_cont\"] = [word_contractions(text) for text in df1[\"description\"]]\n",
    "df1[\"desc_lower\"] = [lowercase(text) for text in df1[\"desc_cont\"]]\n",
    "df1[\"desc_tag\"] = [remove_htmltag_url(text) for text in df1[\"desc_lower\"]]\n",
    "df1[\"desc_rem\"] = [remove_irrchar_punc(text,char=None) for text in df1[\"desc_tag\"]]\n",
    "df1[\"desc_num\"] = [remove_num(text) for text in df1[\"desc_rem\"]]\n",
    "df1[\"desc_clean\"] = [remove_multwhitespace(text) for text in df1[\"desc_num\"]]\n",
    "\n",
    "df1[\"title_cont\"] = [word_contractions(text) for text in df1[\"title\"]]\n",
    "df1[\"title_lower\"] = [lowercase(text) for text in df1[\"title_cont\"]]\n",
    "df1[\"title_tag\"] = [remove_htmltag_url(text) for text in df1[\"title_lower\"]]\n",
    "df1[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df1[\"title_tag\"]]\n",
    "df1[\"title_num\"] = [remove_num(text) for text in df1[\"title_rem\"]]\n",
    "df1[\"title_clean\"] = [remove_multwhitespace(text) for text in df1[\"title_num\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9d45576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>desc_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update gio fields from git r...</td>\n",
       "      <td>please provide a way to update gio fields from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test suite execution terminates before executi...</td>\n",
       "      <td>test suite execution finished before executing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cloning defects from another test cycle is not...</td>\n",
       "      <td>i am trying to clone defects from another test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>testing only this is enhancement only</td>\n",
       "      <td>retest some function again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>testing only this is consultation only</td>\n",
       "      <td>enter the support needed at here</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         title_clean  \\\n",
       "0  provide method to update gio fields from git r...   \n",
       "1  test suite execution terminates before executi...   \n",
       "2  cloning defects from another test cycle is not...   \n",
       "3              testing only this is enhancement only   \n",
       "4             testing only this is consultation only   \n",
       "\n",
       "                                          desc_clean  \n",
       "0  please provide a way to update gio fields from...  \n",
       "1  test suite execution finished before executing...  \n",
       "2  i am trying to clone defects from another test...  \n",
       "3                        retest some function again   \n",
       "4                  enter the support needed at here   "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df1[[\"title_clean\",\"desc_clean\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words to remove\n",
    "# remove_taxo = [\"gio\",\"fields\",\"test\"]\n",
    "# #list of words to maintain\n",
    "# include_taxo = [\"test suite execution\",\"kpi metric\"]\n",
    "# df1[\"desc_taxo\"]=  [custom_taxo(text,remove_taxo,include_taxo) for text in df1[\"desc_white\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd346a",
   "metadata": {},
   "source": [
    "### ii) Custom Named Entity Recognition (Methodology to recommend potential taxonomy)\n",
    "1) User to split text data into train, validation, test\n",
    "\n",
    "2) User to create custom entity data for the train and validation\n",
    "\n",
    "3) User to get base_config.cfg file from Spacy website and save in same path as jupyter notebook\n",
    "\n",
    "4) Function will \n",
    "\n",
    "    i) convert data into .spacy format \n",
    "    \n",
    "    ii) build/save NER model in given path or load previously built NER model\n",
    "    \n",
    "    iii) Label entities in test data to recommend potential taxonomy to user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d34b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "\n",
    "def convert_spacy(DATA):\n",
    "    \"\"\"\n",
    "    Convert  data into .spacy format\n",
    "    DATA[]: Train/validation data to be converted to .spacy format\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        \n",
    "    return db\n",
    "\n",
    "    \n",
    "def custom_ner(TRAIN_DATA,VAL_DATA,path):\n",
    "    \"\"\"\n",
    "    Build and save custom NER model in given path. \n",
    "    \n",
    "    \"\"\"\n",
    "    #convert train and validation data into .spacy format\n",
    "    db_train = convert_spacy(TRAIN_DATA) \n",
    "    db_val = convert_spacy(VAL_DATA) \n",
    "    \n",
    "    #save train and validation data in .spacy format in path\n",
    "    db_train.to_disk(path +'train.spacy')\n",
    "    db_val.to_disk(path +'val.spacy')\n",
    "    \n",
    "    print(\"Train and validation converted to .spacy format and saved\")\n",
    "    \n",
    "    #autofill base_config file saved by user from spacy website\n",
    "    !python -m spacy init fill-config base_config.cfg config.cfg\n",
    "    \n",
    "    #Model building and saving in path\n",
    "    !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy\n",
    "    \n",
    "    print(\"Custom NER model built and saved!\")\n",
    "    \n",
    "def check_ents(path,column):\n",
    "    \"\"\"\n",
    "    Check entities after loading best model\n",
    "    \n",
    "    \"\"\"\n",
    "    #Load best model\n",
    "    nlp = spacy.load(path + \"/output/model-best/\")     \n",
    "    print(\"Best model loaded!\")\n",
    "    \n",
    "    entities = []\n",
    "    for text in column.tolist():\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.text+' - '+ent.label_)\n",
    "    print(np.unique(np.array(entities)))        \n",
    "\n",
    "def ner_wrapper(TRAIN_DATA,VAL_DATA,path,column,train_model):  \n",
    "    \"\"\"\n",
    "    User can choose to train the spacy model or load spacy model\n",
    "    params:\n",
    "    TRAIN_DATA[NER format]: train data for model building\n",
    "    VAL_DATA[NER format]: validation data for model building\n",
    "    path[string]: input path to store model. Path has to be the same as base_config.cfg file downloaded from spacy\n",
    "                  website and jupyter notebook.\n",
    "    column[series]: column for entities to be checked\n",
    "    train_model[True/False]: True if want to train model. False to load model (no training)\n",
    "    \"\"\"\n",
    "    if train_model == True:\n",
    "        custom_ner(TRAIN_DATA,VAL_DATA,path)\n",
    "        check_ents(path,column)\n",
    "        \n",
    "    if train_model == False:\n",
    "        check_ents(path,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom entity data for the train and validation\n",
    "TRAIN_DATA = [\n",
    "[\"jchun wai kit is working on this to enable in new tcp\", {\"entities\": [[0, 13, \"NAME\"]]}], \n",
    "[\"siewlita pending release\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi lim chih quanx per our communication i still have one more question\", {\"entities\": [[3, 17, \"NAME\"]]}],\n",
    "[\"yeetheng the auto test trigger after build complete is working fine today\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi jon here is the recipe link weichuan hi can you try to reproduce the issue once more\", {\"entities\": [[3, 6, \"NAME\"],[31, 39, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "VAL_DATA = [\n",
    "[\"wei chuan has updated me with the sample of test execution by automation manual chart\", {\"entities\": [[0, 9, \"NAME\"]]}],\n",
    "[\"subject gio logs and gio installation hi ajay jonathan i just noticed that star is directing all the logs to gio folder\", {\"entities\": [[41, 45, \"NAME\"],[46, 55, \"NAME\"]]}],\n",
    "[\"hi firesh final verdict in jenkins coming as fail even after all the triggered tests are passed\", {\"entities\": [[3, 9, \"NAME\"],[27, 35, \"NAME\"]]}],\n",
    "[\"wai kit below is the requirement needed from gio product defect detection\", {\"entities\": [[0, 7, \"NAME\"]]}],\n",
    "[\"just string field regards robert nowicki\", {\"entities\": [[26, 40, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "#jupyter notebook and base_config.cfg path have to be the same\n",
    "path = \"C:/Users/nchong/\"\n",
    "\n",
    "#load and clean test data\n",
    "df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54740791",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "#title\n",
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "df[\"title_lemma\"] = lemmatize_words(column= df[\"title_rare\"],lemma_type=None)\n",
    "df[\"title_clean\"] = df[\"title_lemma\"]\n",
    "\n",
    "#description\n",
    "df[\"desc_cont\"] = [word_contractions(text) for text in df[\"description\"]]\n",
    "df[\"desc_lower\"] = [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "df[\"desc_tag\"] = [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "df[\"desc_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "df[\"desc_num\"] = [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "df[\"desc_white\"] = [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"desc_white\"]]\n",
    "n=10\n",
    "df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "df[\"desc_rare\"] = remove_rarewords(df[\"desc_freq\"],n)\n",
    "df[\"desc_lemma\"] = lemmatize_words(column= df[\"desc_rare\"],lemma_type=None)\n",
    "df[\"desc_clean\"] = df[\"desc_lemma\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95cc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_clean\",\"desc_clean\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec289e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def feature_extraction(column,ngram_range=None,ascending=None,fe_type=None):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - TF-IDF(default choice) or Bag of words\n",
    "     \n",
    "    params:\n",
    "    column [series/DataFrame]: column selected for feature extraction \n",
    "                        - series: only one column is selected for feature extraction (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for feature extraction (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    \n",
    "    \"\"\"\n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "                    \n",
    "    if ngram_range == None: #set ngram range as unigram by default\n",
    "        ngram_range=(1,1)\n",
    "        \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == None: #tfidf\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254ef7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction(column=df[[\"title_clean\",\"desc_clean\"]],ngram_range=None,ascending=False,fe_type=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extraction(column=df[\"title_clean\"],ngram_range=None,ascending=False,fe_type=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12edfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop([\"id\"],axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description\n",
    "df1[\"desc_cont\"] = [word_contractions(text) for text in df1[\"description\"]]\n",
    "df1[\"desc_lower\"] = [lowercase(text) for text in df1[\"desc_cont\"]]\n",
    "df1[\"desc_tag\"] = [remove_htmltag_url(text) for text in df1[\"desc_lower\"]]\n",
    "df1[\"desc_rem\"] = [remove_irrchar_punc(text,char=None) for text in df1[\"desc_tag\"]]\n",
    "df1[\"desc_num\"] = [remove_num(text) for text in df1[\"desc_rem\"]]\n",
    "df1[\"desc_white\"] = [remove_multwhitespace(text) for text in df1[\"desc_num\"]]\n",
    "#list of words to remove\n",
    "remove_taxo = [\"gio\",\"fields\",\"test\"]\n",
    "#list of words to maintain\n",
    "include_taxo = [\"test suite execution\",\"kpi metric\"]\n",
    "df1[\"desc_taxo\"]=  [custom_taxo(text,remove_taxo,include_taxo) for text in df1[\"desc_white\"]]\n",
    "\n",
    "# df1[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df1[\"desc_white\"]]\n",
    "# n=10\n",
    "# df1[\"desc_freq\"] = remove_freqwords(df1[\"desc_stop\"],n)\n",
    "# df1[\"desc_rare\"] = remove_rarewords(df1[\"desc_freq\"],n)\n",
    "# df1[\"desc_lemma\"] = lemmatize_words(column= df1[\"desc_rare\"],lemma_type=None)\n",
    "# df1[\"desc_clean\"] = df1[\"desc_lemma\"]\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a1c45",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "### i ) K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3efbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_manipulation(df1,how=\"any\",keep=\"first\",cols_tokeep=[\"title\"],cols_todrop=None,impute_value=None,subset=[\"title\"])\n",
    "df1[\"title_cont\"] = [word_contractions(text) for text in df1[\"title\"]]\n",
    "df1[\"title_lower\"] = [lowercase(text) for text in df1[\"title_cont\"]]\n",
    "df1[\"title_tag\"] = [remove_htmltag_url(text) for text in df1[\"title_lower\"]]\n",
    "df1[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df1[\"title_tag\"]]\n",
    "df1[\"title_num\"] = [remove_num(text) for text in df1[\"title_rem\"]]\n",
    "df1[\"title_white\"] = [remove_multwhitespace(text) for text in df1[\"title_num\"]]\n",
    "df1[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df1[\"title_white\"]]\n",
    "n=10\n",
    "\n",
    "df1[\"title_freq\"] = remove_freqwords(df1[\"title_stop\"],n)\n",
    "df1[\"title_rare\"] = remove_rarewords(df1[\"title_freq\"],n)\n",
    "df1[\"title_lemma_word\"] = lemmatize_words(column= df1[\"title_rare\"],lemma_type=None)\n",
    "df1[\"title_clean\"] = df1[\"title_lemma_word\"]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1eba4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1[[\"title_clean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "silhouette_avg_list = []\n",
    "n_clusters_list = []\n",
    "dicts = {}\n",
    "\n",
    "def kmeans_clustering(column,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for clustering \n",
    "                        - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"   \n",
    "    #call feature extraction function    \n",
    "    ascending = None \n",
    "    X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "    X = X.drop(index='sum')\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "    #user provides the number of clusters        \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "        print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "        \n",
    "            \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "    print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "    terms = vec_type.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "        print(\"\\n\")\n",
    "   \n",
    "               \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eaa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "silhouette_avg_list = []\n",
    "n_clusters_list = []\n",
    "dicts = {}\n",
    "\n",
    "def kmeans_clustering(column,top_n_terms,ngram_range=None,fe_type=None,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for clustering \n",
    "                        - series: only one column is selected for clustering (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for clustering (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"   \n",
    "    #call feature extraction function    \n",
    "    ascending = None \n",
    "    X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "    X = X.drop(index='sum')\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "    #user provides the number of clusters        \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = round(metrics.silhouette_score(X, labels,random_state=42),3)\n",
    "         print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "#         score = {'No. of clusters',n_clusters,'Silhouette score':silhouette_score}\n",
    "#         overall_acc = pd.DataFrame([score])\n",
    "            \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "            \n",
    "            with open(path+'my_file.txt','w') as f:\n",
    "                print(\"For n_clusters =\", n_clusters,\"The silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "    print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "    terms = vec_type.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "        print(\"\\n\")\n",
    "   \n",
    "               \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'my_file.txt','a') as f:\n",
    "    silhouette_avg=1\n",
    "    n_clusters=2\n",
    "    print(\"For n_clusters =\", n_clusters,\"the silhouette_score is :\", round(silhouette_avg,3),file=f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = \"First line\"\n",
    "line2 = \"Second line\"\n",
    "line3 = \"Third line\"\n",
    "with open(path+'my_file.txt','w') as out:\n",
    "    out.writelines([line1,\"\\n\",line2,\"\\n\",line3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e269a",
   "metadata": {},
   "source": [
    "#### Case 1: user provides the number of clusters ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df1[\"title_clean\"]\n",
    "\n",
    "#k means clustering\n",
    "df1[\"cluster\"] = kmeans_clustering(column,top_n_terms=10,ngram_range=None,fe_type=\"bagofwords\",n_clusters=5,max_n_clusters=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n",
    "column = df1[[\"title_clean\",\"desc_clean\"]]\n",
    "\n",
    "#k means clustering\n",
    "df1[\"cluster\"] = kmeans_clustering(column,top_n_terms=10,ngram_range=None,fe_type = \"bagofwords\",n_clusters=5,max_n_clusters=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e7cf8",
   "metadata": {},
   "source": [
    "#### Case 2: user provides max number of clusters ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b623ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df1[[\"title_clean\",\"desc_clean\"]]\n",
    "\n",
    "#k means clustering\n",
    "df1[\"cluster\"] = kmeans_clustering(column,top_n_terms=10,ngram_range=None,fe_type =\"bagofwords\",n_clusters=None,max_n_clusters=20)\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Case 2: user provides max number of clusters ### \n",
    "\n",
    "column = df1[\"title_clean\"]\n",
    "\n",
    "#k means clustering\n",
    "df1[\"cluster\"] = kmeans_clustering(column,top_n_terms=10,ngram_range=None,fe_type =\"bagofwords\",n_clusters=None,max_n_clusters=20)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885667b",
   "metadata": {},
   "source": [
    "### ii) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62727db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(\"cluster\",axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of LDA:\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def lda(column,n_components,top_n_terms,ngram_range=None):\n",
    "    \"\"\"\n",
    "    LDA for unsupervised learning. Bag of words is selected for feature extraction\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for lda\n",
    "                        - series: only one column is selected for lda (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for lda (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in the lda_model\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #feature extraction\n",
    "    ascending = None\n",
    "    fe_type = \"bagofwords\"\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the LDA class \n",
    "    lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "    lda_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = lda_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user provides number of component and top n terms in each cluster/topic\n",
    "column = df1[\"title_clean\"]\n",
    "\n",
    "#LDA\n",
    "df1[\"topic\"] = lda(column,n_components=5,top_n_terms=10,ngram_range=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28ef35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df1[[\"title_clean\",\"desc_clean\"]]\n",
    "\n",
    "#LDA\n",
    "df1[\"topic\"] = lda(column,n_components=5,top_n_terms=10,ngram_range=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e492a93",
   "metadata": {},
   "source": [
    "### iii)  NMF factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9e2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"topic\"],axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42360c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "def nmf(column,n_components,top_n_terms,fe_type,ngram_range=None):\n",
    "    \"\"\"\n",
    "    Non-negative matrix factorization for unsupervised learning.\n",
    "    params:\n",
    "    column [series/DataFrame]: column(s) selected for NMF \n",
    "                        - series: only one column is selected for NMF (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected for NMF (e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    n_components[int]: the number of topics/clusters used in NMF\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    fe_type[string/None]: Feature extraction type: Choose \"bagofwords\" for bow or None for default tfidf method\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                   - [default] ngram_range of (1, 1) means only unigrams, \n",
    "                                   - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                   - ngram_range of (2, 2) means only bigram\n",
    "    \"\"\"\n",
    "    #feature extraction\n",
    "    ngram_range = None\n",
    "    ascending = None\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "    # Create object for the NMF class \n",
    "    nmf_model = NMF(n_components,random_state=42)\n",
    "    nmf_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = nmf_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5ca4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user provides number of component and top n terms in each cluster/topic\n",
    "column = df1[\"title_clean\"]\n",
    "\n",
    "#NMF\n",
    "df1[\"topic\"] = nmf(column,n_components=5,top_n_terms=10,fe_type=\"bagofwords\",ngram_range=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user provides number of component and top n terms in each cluster/topic\n",
    "# #feature extraction\n",
    "column = df1[[\"title_clean\",\"desc_clean\"]]\n",
    "\n",
    "#NMF\n",
    "df1[\"topic\"] = nmf(column,n_components=5,top_n_terms=10,fe_type=\"bagofwords\",ngram_range=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d21c2",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VICE dataset\n",
    "import pandas as pd\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "df= pd.read_csv(path+\"sip_sighting_usb_duplicate_ai.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cb6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"problem_area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66af9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"problem_area\"],cols_todrop=None,impute_value=None,subset=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "df[\"title_lemma\"] = lemmatize_words(column= df[\"title_rare\"],lemma_type=None)\n",
    "df[\"title_clean\"] = df[\"title_lemma\"]\n",
    "\n",
    "df[\"desc_cont\"] = [word_contractions(text) for text in df[\"description\"]]\n",
    "df[\"desc_lower\"] = [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "df[\"desc_tag\"] = [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "df[\"desc_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "df[\"desc_num\"] = [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "df[\"desc_white\"] = [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"desc_white\"]]\n",
    "n=10\n",
    "df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "df[\"desc_rare\"] = remove_rarewords(df[\"desc_freq\"],n)\n",
    "df[\"desc_lemma\"] = lemmatize_words(column= df[\"desc_rare\"],lemma_type=None)\n",
    "df[\"desc_clean\"] = df[\"desc_lemma\"]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec99ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_clean\",\"desc_clean\",\"problem_area\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path ='C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/ML_Testing/'\n",
    "user_outpath = json_path + 'user_outpath/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b280852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def supervised_lng(df,user_outpath,target,test_size,ngram_range=None,fe_type=None,model_type=None,ascend=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Consists of 3 supervised machine learning methods: RandomForest (Default), Naive Bayes(optional, SVM (optional)\n",
    "    \n",
    "    X[series/DataFrame]: column(s) of text for supervised learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])\n",
    "    y[series]: target \n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       -[DEFAULT] ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for default tfidf method\n",
    "    model_type[None/string]: Choose ML algorithm \n",
    "                            - None (Default algorithm is Random Forest)\n",
    "                            - 'NB'(To choose Naive Bayes as ML algorithm), \n",
    "                            - 'SVM'(To choose Support Vector Machine as ML algorithm)\n",
    "    ascend[True/False/None]:  - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                              - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                              - False(Confusion matrix arranged in descending order of accuracy % per label)  \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)\n",
    "        \n",
    "    \"\"\"\n",
    "    X= df.drop([target],axis=1)\n",
    "    y= df[target]   \n",
    "    \n",
    "    #TRAIN-TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "\n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train       \n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    \n",
    "    \n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    if model_type == None:\n",
    "        #random forest is chosen by default\n",
    "        model = RandomForestClassifier(random_state = 42)\n",
    "    \n",
    "    if model_type == \"NB\":\n",
    "        model = MultinomialNB()\n",
    "                   \n",
    "    if model_type == \"SVM\":\n",
    "        model = svm.SVC(random_state = 42)\n",
    "    \n",
    "    model.fit(X_train, y_train) \n",
    "    \n",
    "    #MODEL SAVING\n",
    "    \n",
    "    joblib.dump(model, path + \"model.joblib\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    # print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    # print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(user_outpath+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(user_outpath+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(user_outpath+\"Confusion_Matrix.csv\",index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eaa861",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df.copy()\n",
    "df1= df1.drop([\"desc_clean\"],axis=1)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"problem_area\"\n",
    "test_size = 0.3\n",
    "ngram_range = None\n",
    "fe_type = \"bagofwords\"\n",
    "model_type = None\n",
    "supervised_lng(df1,user_outpath,target,test_size,ngram_range,fe_type,model_type,ascend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6198a105",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "ngram_range = None\n",
    "fe_type = \"bagofwords\"\n",
    "model_type = None\n",
    "save_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "ascend= None\n",
    "supervised_lng(df,user_outpath,target,test_size,ngram_range,fe_type,model_type,ascend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9460527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import numpy as np \n",
    "\n",
    "def deep_lng(df,user_outpath,target,test_size,ngram_range,fe_type,hidden_layer_sizes=None,activation=None,solver=None,learning_rate=None,max_iter=None,ascend=None):\n",
    "    \"\"\"\n",
    "     Deep learning method: MultiLayer Perceptron\n",
    "\n",
    "    X[series/DataFrame]: column(s) of text for deep learning\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])   \n",
    "    y[series]: target\n",
    "    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "                          If int, represents the absolute number of test samples.\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    hidden_layer_sizes[tuple],default = (100): To set the number of layers and the number of nodes.\n",
    "                                               Each element in the tuple represents the number of nodes,\n",
    "                                               length of tuple denotes the total number of hidden layers in the network\n",
    "    activation[\"identity\", \"logistic\", \"tanh\",\"relu\"], default=\"relu\": Activation function for the hidden layer.\n",
    "    solver[\"lbfgs\", \"sgd\", \"adam\"], default=\"adam\": The solver for weight optimization.\n",
    "    learning_rate[\"constant\", \"invscaling\", \"adaptive\"], default=\"constant\": Learning rate schedule for weight updates\n",
    "    max_iter[int], default=200: Maximum number of iterations. The solver iterates until convergence or this number of iterations.\n",
    "    ascend [True/False/None]: - None (Default: Confusion matrix is arranged in alphabetical order)\n",
    "                                 - True(Confusion matrix arranged in ascending order of accuracy % per label), \n",
    "                                 - False(Confusion matrix arranged in descending order of accuracy % per label)                            \n",
    "    save_path[None/string]: Path to save model\n",
    "                            - None (Default - Model is not saved)\n",
    "                            - String (Model is saved as model.joblib in the save_path specified as a string)    \n",
    "    \"\"\"    \n",
    "    \n",
    "    X= df.drop([target],axis=1)\n",
    "    y= df[target]   \n",
    "    \n",
    "    #train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "    print(\"Train-test split completed with\",(1-test_size)*100,\"-\",test_size*100,\"split in train-test\")\n",
    "    print(\"Shape of X_train is:\", X_train.shape)\n",
    "    print(\"Shape of X_test is:\",X_test.shape)\n",
    "    print(\"Shape of y_train is:\",y_train.shape)\n",
    "    print(\"Shape of y_test is:\",y_test.shape)\n",
    "    \n",
    "    if type(X_train) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_train = X_train.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)         \n",
    "        \n",
    "    if type(X_test) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        X_test = X_test.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "        \n",
    "    #FEATURE EXTRACTION\n",
    "    column = X_train\n",
    "    ascending = None\n",
    "    #fit_transform X_train\n",
    "    X_train = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "    #only transform X_test\n",
    "    vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "    X_test = vec_type.transform(X_test)\n",
    "    print(\"Shape of X_train after feature extraction:\",X_train.shape)\n",
    "    print(\"Shape of X_test after feature extraction:\",X_test.shape)\n",
    "    \n",
    "    #MODEL BUILDING\n",
    "    #default hypermarameters\n",
    "    if hidden_layer_sizes == None:\n",
    "        hidden_layer_sizes = (100)\n",
    "    if activation == None:\n",
    "        activation = \"relu\"\n",
    "    if solver == None:\n",
    "        solver = \"adam\"\n",
    "    if learning_rate == None:\n",
    "        learning_rate = \"constant\"\n",
    "    if max_iter == None:\n",
    "        max_iter = 200\n",
    "    \n",
    "    print(\"Hidden layer sizes: \", hidden_layer_sizes,\", Activation: \",activation,\", Solver: \",solver,\", Learning rate: \",learning_rate,\", Max iteration: \",max_iter)\n",
    "    \n",
    "    model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, activation=activation, solver=solver, max_iter=max_iter,verbose = False,random_state=42)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    \n",
    "    #MODEL SAVING    \n",
    "    joblib.dump(model, path + \"mlpmodel.joblib\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    # predicting test set results\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # MODEL EVALUATION  \n",
    "   \n",
    "    # print('Overall accuracy achieved is ' + str(round(metrics.accuracy_score(y_test, y_pred)*100,2)) + \"%\")\n",
    "    # print(\"Classification report:\\n\",metrics.classification_report(y_test, y_pred,zero_division=0))\n",
    "    \n",
    "    #overall accuracy\n",
    "    overall_acc = round(metrics.accuracy_score(y_test, y_pred)*100,2)\n",
    "    overall_acc = {'Overall Acc %':overall_acc}\n",
    "    overall_acc = pd.DataFrame([overall_acc])\n",
    "    overall_acc.to_csv(user_outpath+\"Overall_Accuracy.csv\")\n",
    "\n",
    "    #classification report\n",
    "    report = metrics.classification_report(y_test, y_pred,zero_division=0,output_dict=True)\n",
    "    report = pd.DataFrame(report).transpose()\n",
    "    report.to_csv(user_outpath+\"Classification_Report.csv\")\n",
    "\n",
    "    #confusion matrix with accuracies for each label\n",
    "    class_accuracies = []\n",
    "\n",
    "    for class_ in y_test.sort_values(ascending= True).unique():\n",
    "        class_acc = round(np.mean(y_pred[y_test == class_] == class_)*100,2)\n",
    "        class_accuracies.append(class_acc)\n",
    "    class_acc = pd.DataFrame(class_accuracies,index=y_test.sort_values(ascending= True).unique(),columns= [\"Accuracy %\"])\n",
    "\n",
    "    cf_matrix = pd.DataFrame(\n",
    "        metrics.confusion_matrix(y_test, y_pred, labels= y_test.sort_values(ascending= True).unique()), \n",
    "        index=y_test.sort_values(ascending= True).unique(), \n",
    "        columns=y_test.sort_values(ascending= True).unique()\n",
    "    )\n",
    "    \n",
    "    if ascend == None:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1)\n",
    "    else:\n",
    "        cf_matrix = pd.concat([cf_matrix,class_acc],axis=1).sort_values(by=['Accuracy %'], ascending=ascend)\n",
    "          \n",
    "    cf_matrix.to_csv(user_outpath+\"Confusion_Matrix.csv\",index=False)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb9200",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "ngram_range = None\n",
    "fe_type = None\n",
    "hidden_layer_sizes = (5)\n",
    "activation= None\n",
    "solver=None\n",
    "learning_rate=None\n",
    "max_iter= None\n",
    "ascend= False\n",
    "\n",
    "deep_lng(df,user_outpath,target,test_size,ngram_range,fe_type,hidden_layer_sizes,activation,solver,learning_rate,max_iter,ascend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.3\n",
    "ngram_range = None\n",
    "fe_type = None\n",
    "hidden_layer_sizes = (5)\n",
    "activation= None\n",
    "solver=None\n",
    "learning_rate=None\n",
    "max_iter= None\n",
    "ascend= False\n",
    "\n",
    "deep_lng(df1,user_outpath,target,test_size,ngram_range,fe_type,hidden_layer_sizes,activation,solver,learning_rate,max_iter,ascend)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be3dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"title_clean\",\"desc_clean\"]]\n",
    "y= df[\"problem_area\"]\n",
    "test_size = 0.3\n",
    "ngram_range = None\n",
    "fe_type = None\n",
    "hidden_layer_sizes = (5,5)\n",
    "activation= None\n",
    "solver=None\n",
    "learning_rate=None\n",
    "max_iter= None\n",
    "ascend= False\n",
    "save_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "\n",
    "deep_lng(X,y,test_size,ngram_range,fe_type,hidden_layer_sizes,activation,solver,learning_rate,max_iter,ascend,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1aeea",
   "metadata": {},
   "source": [
    "### Similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VICE dataset\n",
    "import pandas as pd\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "df= pd.read_csv(path+\"sip_sighting_usb_duplicate_ai.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34956f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\"],cols_todrop=None,impute_value=None,subset=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8649335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "n=10\n",
    "\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "\n",
    "df[\"title_lemma\"] = lemmatize_words(column= df[\"title_rare\"],lemma_type=None)\n",
    "\n",
    "df[\"title_clean\"] = df[\"title_lemma\"]\n",
    "\n",
    "df[\"desc_cont\"] = [word_contractions(text) for text in df[\"description\"]]\n",
    "df[\"desc_lower\"] = [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "df[\"desc_tag\"] = [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "df[\"desc_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "df[\"desc_num\"] = [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "df[\"desc_white\"] = [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"desc_white\"]]\n",
    "n=10\n",
    "df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "df[\"desc_rare\"] = remove_rarewords(df[\"desc_freq\"],n)\n",
    "df[\"desc_lemma\"] = lemmatize_words(column= df[\"desc_rare\"],lemma_type=None)\n",
    "df[\"desc_clean\"] = df[\"desc_lemma\"]\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_clean\",\"desc_clean\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13471df9",
   "metadata": {},
   "source": [
    "### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29209543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosinesimilarity(column,threshold=None,total_rows = None,base_row=None,ngram_range=None,fe_type=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between rows of texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:\n",
    "    \n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]])  \n",
    "    threshold[None/float]: cut off value for the cosine similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    fe_type[None/string]: Feature extraction type: Choose \"bagofwords\" or None for tfidf\n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"     \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "                \n",
    "    #feature extraction              \n",
    "    X = feature_extraction(column=column,ngram_range=ngram_range,ascending=None,fe_type=fe_type)[0]\n",
    "    X = X.drop([\"sum\"],axis = 0)\n",
    "    \n",
    "    #Get cosine similarity matrix\n",
    "    similarity_matrix = pd.DataFrame(cosine_similarity(X))\n",
    "    \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "       \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        for base in range(total_rows): \n",
    "            print (\"\")\n",
    "            print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "            \n",
    "            #Create empty df\n",
    "            column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)\n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                \n",
    "                if similarity_matrix.iloc[base,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {'Index':i, 'Similarity Score':round(similarity_matrix.iloc[base,i],4), 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                    if ascending != None:            \n",
    "                        results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)\n",
    "                        \n",
    "            display(results)\n",
    "#             print(results['Similarity Score'].mean())\n",
    "           \n",
    "\n",
    "    if base_row !=None: #fix base_row index for comparison with all indexes\n",
    "        print (\"Using index \" + str(base_row) + \" as base:\") #fix one index as base\n",
    "        \n",
    "        #Create empty df\n",
    "        column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)\n",
    "        \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            if similarity_matrix.iloc[base_row,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {'Index':i, 'Similarity Score':round(similarity_matrix.iloc[base_row,i],4), 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "                    \n",
    "        display(results) \n",
    "#         print(results['Similarity Score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b7e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosinesimilarity(column = df[\"title_clean\"],threshold=None,total_rows=10,base_row=None,ngram_range=None,fe_type=None,ascending=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37c5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosinesimilarity(column = df[\"title_clean\"],threshold= 0.5,total_rows=None,base_row=4,ngram_range=None,fe_type=None,ascending=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd74e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosinesimilarity(column = df[[\"title_clean\",\"desc_clean\"]],threshold=None,total_rows=10,base_row=None,ngram_range=None,fe_type=None,ascending=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosinesimilarity(column = df[[\"title_clean\",\"desc_clean\"]],threshold=0,total_rows=None,base_row=4,ngram_range=None,fe_type=None,ascending=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6c73c8",
   "metadata": {},
   "source": [
    "### Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(column,threshold=None,total_rows = None,base_row=None,ascending=None):\n",
    "    \"\"\"\n",
    "    Compute the jaccard similarity between texts. User can \n",
    "    a) fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "    b) fix one row as base, comparison will be done with all the other rows\n",
    "    \n",
    "    params:\n",
    "    column[series/DataFrame]: column(s) of text for row wise similarity comparison\n",
    "                        - series: only one column is selected (e.g. df[\"title_clean\"])\n",
    "                        - DataFrame: more than one column is selected(e.g. df[[\"title_clean\",\"desc_clean\"]]) \n",
    "    threshold[None/float]: cut off value for the jaccard similarity, only texts with values above or equal to threshold\n",
    "                           will be printed\n",
    "                        - None: Default threhold is 0.5\n",
    "                        - float: any value between 0 and 1 \n",
    "    total_rows[None/int]: Number of rows for comparison, choose None for option b \n",
    "    base_row[None/int]: Row fixed as base, choose None for option a \n",
    "    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)  \n",
    "    \n",
    "    \"\"\"     \n",
    "            \n",
    "    #jaccard score computation\n",
    "    def get_jaccard_sim(str1, str2):        \n",
    "        a = set(str1.split()) \n",
    "        b = set(str2.split())\n",
    "        c = a.intersection(b)\n",
    "        return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "    \n",
    "    if type(column) == pd.DataFrame: #concat the columns into one string if there is more than one column \n",
    "        column = column.apply(lambda row: ' '.join(row.values.astype(str)), axis=1) \n",
    "       \n",
    "    #threshold\n",
    "    if threshold == None:\n",
    "        threshold = 0.5\n",
    "        \n",
    "    if total_rows !=None: #fix number of rows for comparison, each row will be taken as base and compared with the rest\n",
    "        for base in range(total_rows): \n",
    "            print (\"\")\n",
    "            print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "            \n",
    "            #Create empty df\n",
    "            column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "            results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "            for i in range(total_rows): #compare base with other index\n",
    "                jac_score =  round(get_jaccard_sim(column.iloc[base],column.iloc[i]),4)\n",
    "                if jac_score > threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                    new_row = {'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                    #append row to the dataframe\n",
    "                    results = results.append(new_row, ignore_index=True)\n",
    "                if ascending != None:            \n",
    "                    results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "                    \n",
    "            display(results) \n",
    "        \n",
    "    if base_row != None: #fix base_row index for comparison with all indexes\n",
    "       \n",
    "        print (\"Using index \" + str(base_row) + \" as base row:\") #fix one index as base_row\n",
    "        #Create empty df\n",
    "        column_names = [\"Index\", \"Similarity Score\", \"Text\"]\n",
    "        results = pd.DataFrame(columns = column_names)                   \n",
    "            \n",
    "        for i in range(len(column)): #compare base_row with other index\n",
    "            jac_score = round(get_jaccard_sim(column.iloc[base_row],column.iloc[i]),4)\n",
    "            if jac_score >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "                new_row = {'Index':i, 'Similarity Score':jac_score, 'Text':column.iloc[i]}\n",
    "                #append row to the dataframe\n",
    "                results = results.append(new_row, ignore_index=True)\n",
    "            if ascending != None:            \n",
    "                results = results.sort_values(by ='Similarity Score', axis = 0,ascending=ascending)  \n",
    "\n",
    "        display(results) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb199",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(column= df[\"title_clean\"],threshold=0.5,total_rows = 10,base_row=None,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61504d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(column= df[\"title_clean\"],threshold=0.5,total_rows = None,base_row=4,ascending=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eac42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(column= df[[\"title_clean\",\"desc_clean\"]],threshold=None,total_rows = 10,base_row=None,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138118c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(column= df[[\"title_clean\",\"desc_clean\"]],threshold=None,total_rows = None,base_row=4,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #user provides number of component and top n terms in each cluster/topic\n",
    "# #feature extraction\n",
    "# column = df3[\"title_lemma_word\"]\n",
    "# ngram_range = (1,1)\n",
    "# ascending = False\n",
    "# fe_type = \"bagofwords\"\n",
    "# vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "# vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "# #NMF\n",
    "# df3[\"topic\"] = nmf(vectorized,vec_type,n_components=17,top_n_terms=10)\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eea3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert and save train/validation data as .spacy\n",
    "# out_path = \"C:/Users/nchong/\"\n",
    "# db_train = convert_spacy(TRAIN_DATA)\n",
    "# db_train.to_disk(out_path +'train.spacy') # save the docbin object\n",
    "# db_val = convert_spacy(VAL_DATA)\n",
    "# db_val.to_disk(out_path +'val.spacy') # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89862c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model\n",
    "# nlp1 = spacy.load(\"C:/Users/nchong/output/model-best/\") #load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43084cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp1(\"waikitcx hi arisha please provide us the\") # input sample text\n",
    "\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_ents(text):\n",
    "#     doc= nlp1(text)\n",
    "#     if doc.ents:\n",
    "#         for ent in doc.ents:\n",
    "#             return(ent.text+' - '+ent.label_)\n",
    "#     else:\n",
    "#         return('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c1474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_loading(path,df=None,date=None):\n",
    "#     '''\n",
    "#     Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "#     Can support incremental aggregation of dataset, by setting arg df as the existing dataframe\n",
    "#     Returns a single dataframe.\n",
    "    \n",
    "#     params:\n",
    "#     path [string]: path of the files, without filename\n",
    "#     df [dataframe] (optional,default is None): input existing dataframe to merge with new files\n",
    "#     date [\"string\"](optional,default is None): user can choose to load only files from specific date in YYYY-MM-DD format\n",
    "#     '''\n",
    "#     filenames = os.listdir(path)\n",
    "#     file_list=[]\n",
    "#     dfs = []\n",
    "\n",
    "#     if df is None: #no existing dataframe\n",
    "        \n",
    "#         for file in filenames:\n",
    "#             # search agreed file format pattern in the filename\n",
    "#             if date == None:\n",
    "#                 pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "                \n",
    "#             else:\n",
    "# #              \n",
    "#                 pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "    \n",
    "#             match = re.search(pattern,file)\n",
    "#             #if match is found\n",
    "#             if match:\n",
    "#                 pattern = os.path.join(path, file) #join path with file name\n",
    "#                 file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "#                 for file in file_list:\n",
    "#                     with open(file) as f:\n",
    "#                         #flatten json into pd dataframe\n",
    "#                         json_data = pd.json_normalize(json.loads(f.read()))\n",
    "#                         #label which file each row is from \n",
    "#                         json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "#                     dfs.append(json_data)\n",
    "#                 df = pd.concat(dfs)\n",
    "                \n",
    "#     else: #existing dataframe exists and want to append new files to existing dataframe\n",
    "             \n",
    "#         for file in filenames:\n",
    "\n",
    "#             if file not in df[\"file\"].unique(): #check if file is new - to support merging of new dataset with previously read ones\n",
    "\n",
    "#                 # search agreed file format pattern in the filename\n",
    "                \n",
    "#                 if date == None:\n",
    "#                     pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "#                 else:\n",
    "#                     pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "                     \n",
    "#                 match = re.search(pattern,file)\n",
    "\n",
    "#                 #if match is found\n",
    "#                 if match:\n",
    "#                     json_pattern = os.path.join(path, file) #join path with file name\n",
    "#                     file_list.append(json_pattern) #list of json files \n",
    "\n",
    "#                     for file in file_list:\n",
    "#                         with open(file) as f:\n",
    "#                             #flatten json into pd dataframe\n",
    "#                             json_data = pd.json_normalize(json.loads(f.read()))\n",
    "#                             #label which file each row is from \n",
    "#                             json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "#                         dfs.append(json_data)\n",
    "#                     new_df = pd.concat(dfs)           \n",
    "#                     df=pd.concat([df,new_df])\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939883e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "# from nltk.tokenize import WhitespaceTokenizer\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# import re\n",
    "# #remove token method - seperate nltk and split functions \n",
    "# def cust_tokenization(column,token_met,token_type,delim =None):\n",
    "#     \"\"\"\n",
    "#     Custom tokenization, 2 options are available: split() or nltk \n",
    "#     params:\n",
    "#     df [dataframe]: input dataframe \n",
    "#     token_met[\"string\"]: input tokenization method (\"split\" or \"nltk\")\n",
    "    \n",
    "#     token_type[\"string\"](use only if token_met= \"nltk\"): type of nltk tokenization\n",
    "#     a) token_type = \"WordToken\" tokenizes a string into a list of words\n",
    "#     b) token_type = \"SentToken\" tokenizes a string containing sentences into a list of sentences\n",
    "#     c) token_type = \"WhiteSpaceToken\" tokenizes a string on whitespace (space, tab, newline)\n",
    "#     d) token_type = \"WordPunctTokenizer\" tokenizes a string on punctuations\n",
    "         \n",
    "#     delim[\"string\"](use only if token_met = \"split\"): specify delimiter to separate strings,\n",
    "#     default delimiter (delim=None) is whitespace,  an alternate option for token_type = \"WhiteSpaceToken\"\n",
    "    \n",
    "#     \"\"\"\n",
    "#     if token_met == \"split\":\n",
    "#         if delim==None:\n",
    "#             print(\"Text is split by space\") #default delimiter is space if not specified \n",
    "\n",
    "#         else:\n",
    "#             print(\"Text is split by:\", delim) #can accept one or more delimiter\n",
    "\n",
    "#         return column.apply(lambda text: text.split() if delim==None else text.split(delim))\n",
    "    \n",
    "\n",
    "#     if token_met == \"nltk\":\n",
    "    \n",
    "#         if token_type == \"WordToken\":\n",
    "#             tokenizer = word_tokenize\n",
    "#         if token_type == \"SentToken\":\n",
    "#             tokenizer = sent_tokenize\n",
    "#         if token_type == \"WhiteSpaceToken\":\n",
    "#             tokenizer = WhitespaceTokenizer().tokenize\n",
    "#         if token_type == \"WordPunctTokenizer\":\n",
    "#             tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "#         return column.apply(lambda text: tokenizer(text))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab019e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime,timedelta\n",
    "# def data_loading(path,date_list=None):\n",
    "#     '''\n",
    "#     Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "#     User can choose to load only files from specific date\n",
    "    \n",
    "#     params:\n",
    "#     path [string]: path of the files, without filename\n",
    "#     date_list [\"list\"](optional,default is None): user can choose to load only files from specific date in YYYY-MM-DD format\n",
    "#     '''\n",
    "    \n",
    "#     filenames = os.listdir(path)\n",
    "#     file_list=[]\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     if date_list == None:\n",
    "#         for file in filenames:\n",
    "#             # search agreed file format pattern in the filename\n",
    "\n",
    "#             pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "#             match = re.search(pattern,file)\n",
    "                \n",
    "#             #if match is found\n",
    "#             if match:\n",
    "#                 pattern = os.path.join(path, file) #join path with file name\n",
    "#                 file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "            \n",
    "#         print(\"Files read:\",file_list)                   \n",
    "#         for file in file_list:\n",
    "#             with open(file) as f:\n",
    "#                 #flatten json into pd dataframe\n",
    "#                 json_data = pd.json_normalize(json.loads(f.read()))\n",
    "#                 json_data = pd.DataFrame(json_data)\n",
    "#                 #label which file each row is from \n",
    "#                 json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "#             df = df.append(json_data)              \n",
    "                \n",
    "#     else:\n",
    "#         for file in filenames: \n",
    "            \n",
    "#             # search agreed file format pattern in the filename\n",
    "#             for date in date_list: \n",
    "#                 pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "        \n",
    "#                 match = re.search(pattern,file)\n",
    "                \n",
    "#                 #if match is found\n",
    "#                 if match:\n",
    "#                     pattern = os.path.join(path, file) #join path with file name\n",
    "#                     file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "#         print(\"Files read:\",file_list)     \n",
    "#         for file in file_list:\n",
    "#             with open(file) as f:\n",
    "#                 #flatten json into pd dataframe\n",
    "#                 json_data = pd.json_normalize(json.loads(f.read()))\n",
    "#                 json_data = pd.DataFrame(json_data)\n",
    "#                 #label which file each row is from \n",
    "#                 json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "#             df = df.append(json_data)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e10c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fix number of rows for comparison\n",
    "# total_rows = 10 #total rows to consider for comparison\n",
    "# threshold = 0.1 #similarity metric threshold\n",
    "# column = df[[\"title_clean\"]]\n",
    "\n",
    "# for base in range(total_rows): \n",
    "#     print (\"\")\n",
    "#     print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "#     print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Title' : <500}\")\n",
    "\n",
    "#     for i in range(total_rows): #compare base with other index\n",
    "#         jac_score =  round(get_jaccard_sim(column.iloc[base].values[0],column.iloc[i].values[0]),4)\n",
    "#         if jac_score > threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#             print(f\"{i : <10}{jac_score : <20}{column.iloc[i].values[0] : <500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fix base_row index for comparison with all indexes\n",
    "# base_row=4\n",
    "# threshold = 0\n",
    "# column = df[[\"title_clean\"]]\n",
    "\n",
    "# print (\"Using index \" + str(base_row) + \" as base row:\") #fix one index as base_row\n",
    "# print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Title' : <500}\")\n",
    "\n",
    "# for i in range(len(column)): #compare base_row with other index\n",
    "#     jac_score = round(get_jaccard_sim(column.iloc[base_row].values[0],column.iloc[i].values[0]),4)\n",
    "#     if jac_score >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#         print(f\"{i : <10}{jac_score : <20}{column.iloc[i].values[0] : <500}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10fe930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #feature extraction\n",
    "# X = feature_extraction(column = df[\"title_clean\"],ngram_range=(1,1),ascending=None,fe_type=\"tfidf\")[0]\n",
    "# X = X.drop([\"sum\"],axis = 0)\n",
    "# X\n",
    "#Cosine similarity\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# similarity_matrix = pd.DataFrame(cosine_similarity(X))\n",
    "# similarity_matrix\n",
    "#user give total rows to compare\n",
    "# total_rows = 10 #total rows to consider for comparison\n",
    "# threshold = 0.2 #similarity metric threshold\n",
    "# column = df[[\"title_clean\"]]\n",
    "\n",
    "# for base in range(total_rows): \n",
    "#     print (\"\")\n",
    "#     print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "#     print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Title' : <500}\")\n",
    "#     for i in range(total_rows): #compare base with other index\n",
    "#         if similarity_matrix.iloc[base,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#             print(f\"{i : <10}{round(similarity_matrix.iloc[base,i],4) : <20}{column.iloc[i].values[0] : <500}\")\n",
    "#user give base to compare\n",
    "# base_row = 4 #base for comparison\n",
    "# threshold = 0.2 #similarity metric threshold\n",
    "# column = df[[\"title_clean\"]]\n",
    "\n",
    "# print (\"Using index \" + str(base_row) + \" as base:\") #fix one index as base\n",
    "# print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Title' : <500}\")\n",
    "# for i in range(len(column)): #compare base_row with other index\n",
    "#     if similarity_matrix.iloc[base_row,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#         print(f\"{i : <10}{round(similarity_matrix.iloc[base_row,i],4) : <20}{column.iloc[i].values[0] : <500}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f052764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def cosinesimilarity(column,threshold,total_rows = None,base_row=None,ngram_range=None,fe_type=None):\n",
    "    \n",
    "#     #feature extraction\n",
    "#     if ngram_range == None:\n",
    "#         ngram_range = (1,1)\n",
    "#     if fe_type == None:\n",
    "#         fe_type =\"tfidf\"\n",
    "    \n",
    "       \n",
    "#     X = feature_extraction(column=column,ngram_range=ngram_range,ascending=None,fe_type=fe_type)[0]\n",
    "#     X = X.drop([\"sum\"],axis = 0)\n",
    "    \n",
    "#     #Get cosine similarity matrix\n",
    "#     similarity_matrix = pd.DataFrame(cosine_similarity(X))\n",
    "    \n",
    "#     if total_rows !=None:\n",
    "#         for base in range(total_rows): \n",
    "#             print (\"\")\n",
    "#             print (\"Using index \" + str(base) + \" as base:\") #fix one index as base\n",
    "#             print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Text' : <500}\")\n",
    "#             for i in range(total_rows): #compare base with other index\n",
    "#                 if similarity_matrix.iloc[base,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#                     print(f\"{i : <10}{round(similarity_matrix.iloc[base,i],4) : <20}{column.iloc[i] : <500}\")\n",
    "    \n",
    "#     if base_row !=None:\n",
    "#         print (\"Using index \" + str(base_row) + \" as base:\") #fix one index as base\n",
    "#         print(f\"{'Index' : <10}{'Similarity Score' : <20}{'Text' : <500}\")\n",
    "#         for i in range(len(column)): #compare base_row with other index\n",
    "#             if similarity_matrix.iloc[base_row,i] >= threshold: #print if comparison shows that silarity metric is more than threshold\n",
    "#                 print(f\"{i : <10}{round(similarity_matrix.iloc[base_row,i],4) : <20}{column.iloc[i] : <500}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7febb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# def feature_extraction(column,ngram_range,ascending,fe_type):\n",
    "#     \"\"\"\n",
    "#     Feature extraction methods - Bag of words or TF-IDF\n",
    "    \n",
    "#     params:\n",
    "#     column [series]: column to select\n",
    "#     ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "#                                        - [default] ngram_range of (1, 1) means only unigrams, \n",
    "#                                        - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "#                                        - ngram_range of (2, 2) means only bigram\n",
    "#     ascending [True/False/None]: - None (words arranged in alphabetical order)\n",
    "#                                  - True(words arranged in ascending order of sum), \n",
    "#                                  - False(words arranged in descending order of sum)                               \n",
    "#     fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "#     \"\"\"\n",
    "#     if ngram_range == None:\n",
    "#         ngram_range=(1,1)\n",
    "    \n",
    "#     if fe_type == \"bagofwords\":\n",
    "#         vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "#         vectorized = vec_type.fit_transform(column)\n",
    "#         df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "#         df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "#     if fe_type == \"tfidf\":\n",
    "#         vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "#         vectorized = vec_type.fit_transform(column)\n",
    "#         df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "#         df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "#     if ascending != None:\n",
    "            \n",
    "#         df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "#     return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48296641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['title_clean', 'desc_clean']\n",
    "# df['combined'] = df[cols].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dab04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from configparser import ConfigParser\n",
    "\n",
    "# # instantiate\n",
    "# config = ConfigParser()\n",
    "\n",
    "# # parse ini file\n",
    "# ini_path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/\"\n",
    "# config.read(ini_path+'default.ini')\n",
    "\n",
    "# # read values \n",
    "# #from data loading section\n",
    "# path = config.get('dataloading', 'path')\n",
    "\n",
    "# #from data preprocessing section\n",
    "\n",
    "\n",
    "# # from ML module section\n",
    "# #Unsupervised\n",
    "# #Supervised\n",
    "# #Similarity metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
