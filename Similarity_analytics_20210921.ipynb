{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889562e",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import glob, os, json\n",
    "import re\n",
    "\n",
    "#user input file path\n",
    "path = 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(path,df=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    Can support incremental aggregation of dataset, by setting arg df as the existing dataframe\n",
    "    Returns a single dataframe.\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    df [dataframe] (optional,default is None): input existing dataframe to merge with new files\n",
    "    '''\n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    dfs = []\n",
    "\n",
    "    if df is None: #no existing dataframe\n",
    "        \n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "            match = re.search(r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\",file)\n",
    "\n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "                for file in file_list:\n",
    "                    with open(file) as f:\n",
    "                        #flatten json into pd dataframe\n",
    "                        json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                        #label which file each row is from \n",
    "                        json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                    dfs.append(json_data)\n",
    "                df = pd.concat(dfs)\n",
    "                \n",
    "    else: #existing dataframe exists and want to append new files to existing dataframe\n",
    "             \n",
    "        for file in filenames:\n",
    "\n",
    "            if file not in df[\"file\"].unique(): #check if file is new - to support merging of new dataset with previously read ones\n",
    "\n",
    "                # search agreed file format pattern in the filename\n",
    "                match = re.search(r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\",file)\n",
    "\n",
    "                #if match is found\n",
    "                if match:\n",
    "                    json_pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(json_pattern) #list of json files \n",
    "\n",
    "                    for file in file_list:\n",
    "                        with open(file) as f:\n",
    "                            #flatten json into pd dataframe\n",
    "                            json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                            #label which file each row is from \n",
    "                            json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                        dfs.append(json_data)\n",
    "                    new_df = pd.concat(dfs)           \n",
    "                    df=pd.concat([df,new_df])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c24924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_loading(path,df=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39870e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/'+'data_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c03d8",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "### a) Dataframe manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baadc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(df,how,keep,cols_tokeep=None,cols_todrop=None,impute_value=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep or drop columns in dataframe\n",
    "    2) Data impute: Impute or drop NA rows \n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    cols_tokeep [list/None]: list of columns to keep, if there is no list use None\n",
    "    cols_todrop [list/None]: list of columns to drop, if there is no list use None\n",
    "    impute_value [string/None]: value to be imputed (i.e \"\" for empty string). If no value to be imputed but there are \n",
    "                        rows to be dropped use None\n",
    "    how[string]: Drop rows when we have at least one NA or all NA. Choose\n",
    "                      # - \"all\": Drop row with all NA\n",
    "                      # - \"any\": Drop row with at least one NA\n",
    "                  \n",
    "    subset[list/None]: Subset of columns for dropping NA and identifying duplicates, use None if no column to select\n",
    "    keep[string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                        # - \"first\" : Drop duplicates except for the first occurrence. \n",
    "                        # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                        # - False : Drop all duplicates.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select columns or drop unwanted columns\n",
    "    if cols_tokeep != None:\n",
    "        df = df[cols_tokeep]\n",
    "    if cols_todrop != None:\n",
    "        df = df.drop(cols_todrop,axis=1)\n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with user's choice of imputation value\n",
    "    if impute_value != None:\n",
    "        df = df.fillna(impute_value)\n",
    "        print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "        \n",
    "    else: # drop rows with NA values\n",
    "        df= df.dropna(axis=0, how=how,subset=subset)\n",
    "        print(\"Number of null values in df after dropping NA rows:\\n\",df.isnull().sum())\n",
    "        print(\"Shape of df after dropping NA rows:\",df.shape)\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8931eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2089e",
   "metadata": {},
   "source": [
    "\n",
    "### b) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1540a0a",
   "metadata": {},
   "source": [
    "### 2) Expand contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453da296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def word_contractions(df):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \"\"\"\n",
    "    df = df.applymap(lambda text: \" \".join([contractions.fix(word) for word in text.split()]))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff07071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = word_contractions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47492958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e49df",
   "metadata": {},
   "source": [
    "### 3) Convert all characters into lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01121fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(df):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    df[dataframe]: input dataframe\n",
    "    \"\"\"\n",
    "    df = df.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lowercase(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2ffefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2790e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a61d4",
   "metadata": {},
   "source": [
    "### 4) Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3ba0c",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def stem_words(df,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words, 2 options available: Porter Stemmer or Lancaster Stemmer \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    stemmer_type[string]: input stemming method (\"Porter\" or \"Lancaster\")\n",
    "    \"\"\"\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    df = df.applymap(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693988d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stem_words(df,stemmer_type = \"Lancaster\")\n",
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48f7de",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac3bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(df,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words, 2 options available: WordNetLemmatizer or Spacy \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    lemma_type[string]: input lemmatization method (\"WordNet\" or \"Spacy\")\n",
    "    \"\"\"\n",
    "    if lemma_type == \"WordNet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df = df.applymap(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        df = df.applymap(lambda text: \" \".join([word.lemma_ for word in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        df = df.applymap(lambda s:s.lower() if type(s) == str else s) \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize_words(df,lemma_type = \"Spacy\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13e30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd1b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize_words(df,lemma_type = \"WordNet\")\n",
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d27c8",
   "metadata": {},
   "source": [
    "### b) Noise filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661e07",
   "metadata": {},
   "source": [
    "### 1) Remove html tag and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def remove_htmltag_url(df):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    #remove html tag\n",
    "    df = df.applymap(lambda text:BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True))\n",
    "    #remove url\n",
    "    df = df.replace('https?[://%]*\\S+',' ', regex=True) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f23e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_htmltag_url(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2f91a",
   "metadata": {},
   "source": [
    "### 3) Remove irrelevant characters, punctuation, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrchar_punc(df):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    #remove &nbsp; &quot; and &gt; - anything that starts wth $ and ends with ;\n",
    "    df = df.replace('\\&.+?\\;',' ',regex = True)\n",
    "    #Remove comment id number+name  \"++++1562123662 fbakhda\\n\"  in comment field since not relevant\n",
    "    df = df.replace('\\++.*\\\\n',' ', regex=True)\n",
    "    #Remove \"image.png\\\"\n",
    "    df = df.replace('image.png\\\\\\\\',' ', regex=True)\n",
    "    # Remove eg: cid:image004.jpg@01D66BEC.314074D0\\\n",
    "    df = df.replace('cid:image.*\\\\\\\\',' ', regex=True)\n",
    "    # Remove utf-8 literals\n",
    "    df = df.replace(r'\\\\+x[\\d\\D][\\d\\D]',' ', regex=True)\n",
    "    #Remove special characters and punctuation\n",
    "    df = df.replace('[^\\w\\s]',' ', regex=True)\n",
    "    df = df.replace(r\"_\", \" \", regex=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e073201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_irrchar_punc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb2e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9235b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/20210920/'+'data_rem_irrchars_punc.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec85f0",
   "metadata": {},
   "source": [
    "### 3) Remove numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(df):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df=df.replace('\\d+',' ', regex=True) \n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56627dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_num(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebc4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d78620",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/20210920/'+'rem_puncs_withspace.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f0d60",
   "metadata": {},
   "source": [
    "### 4) Remove multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multwhitespace(df):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    \n",
    "    \"\"\"\n",
    "    df = df.replace(' +',' ', regex=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_multwhitespace(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb117fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b71b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e5d5",
   "metadata": {},
   "source": [
    "### 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ca58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    text[string]: input string\n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        text = re.sub(pattern,' ', text)\n",
    "                   \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words/phrase to be added to the stop words \n",
    "extra_sw = [\"gio\",\"defects\",'hsdes',\"test cycle\",\"testing\"]\n",
    "#list of words/phrase to be removed from stop words\n",
    "remove_sw = [\"i\",\"am\"]\n",
    "arg1 = extra_sw\n",
    "arg2 = remove_sw\n",
    "\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"title\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"description\"]]\n",
    "df[\"comments_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"comments\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafbb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4eaccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905229f9",
   "metadata": {},
   "source": [
    "### 5) Remove frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a389eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freqwords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c760354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_freqwords(df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952af778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf14f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ef266",
   "metadata": {},
   "source": [
    "### 6) Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rarewords(df,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    for i in df:\n",
    "    \n",
    "        for text in df[i].values:\n",
    "            for word in text.split():\n",
    "                cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "    df = df.applymap(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_rarewords(df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/20210920/'+'final_withspacylemma.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4224f",
   "metadata": {},
   "source": [
    "### c) Custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "\n",
    "def cust_tokenization(df,token_met,token_type,delim =None):\n",
    "    \"\"\"\n",
    "    Custom tokenization, 2 options are available: split() or nltk \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    token_met[\"string\"]: input tokenization method (\"split\" or \"nltk\")\n",
    "    \n",
    "    token_type[\"string\"](use only if token_met= \"nltk\"): type of nltk tokenization\n",
    "    a) token_type = \"WordToken\" tokenizes a string into a list of words\n",
    "    b) token_type = \"SentToken\" tokenizes a string containing sentences into a list of sentences\n",
    "    c) token_type = \"WhiteSpaceToken\" tokenizes a string on whitespace (space, tab, newline)\n",
    "    d) token_type = \"WordPunctTokenizer\" tokenizes a string on punctuations\n",
    "         \n",
    "    delim[\"string\"](use only if token_met = \"split\"): specify delimiter to separate strings,\n",
    "    default delimiter (delim=None) is whitespace,  an alternate option for token_type = \"WhiteSpaceToken\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if token_met == \"split\":\n",
    "        if delim==None:\n",
    "            print(\"Text is split by space\") #default delimiter is space if not specified \n",
    "\n",
    "        else:\n",
    "            print(\"Text is split by:\", delim) #can accept one or more delimiter\n",
    "\n",
    "        df = df.applymap(lambda text: text.split() if delim==None else text.split(delim))\n",
    "\n",
    "    if token_met == \"nltk\":\n",
    "    \n",
    "        if token_type == \"WordToken\":\n",
    "            tokenizer = word_tokenize\n",
    "        if token_type == \"SentToken\":\n",
    "            tokenizer = sent_tokenize\n",
    "        if token_type == \"WhiteSpaceToken\":\n",
    "            tokenizer = WhitespaceTokenizer().tokenize\n",
    "        if token_type == \"WordPunctTokenizer\":\n",
    "            tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "        df = df.applymap(lambda text: tokenizer(text))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1019c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split\n",
    "df = cust_tokenization(df,token_met=\"split\",token_type=None,delim = '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1827f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk\n",
    "df = cust_tokenization(df,token_met=\"nltk\",token_type=\"WordToken\",delim = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013f586",
   "metadata": {},
   "source": [
    "## d) Custom taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4535b2",
   "metadata": {},
   "source": [
    "### i) Configurability for user to provide taxonomy mapping (to remove/remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df897ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words to remove\n",
    "remove_tax = [\"gio\",\"fields\",\"test\"]\n",
    "#list of words to maintain\n",
    "include_tax = [\"test suite execution\",\"clone defects\"]\n",
    "\n",
    "import re\n",
    "\n",
    "def custom_tax(text,remove_tax,include_tax):\n",
    "    for w in remove_tax:\n",
    "        #row without any item from include_tax -> replace all remove_tax items with empty string\n",
    "        if all(phrase not in text for phrase in include_tax): \n",
    "            pattern = r'\\b'+w+r'\\b'\n",
    "            text = re.sub(pattern,' ', text) \n",
    "        #row with any item from include_tax -> only replace remove_tax item that is not in include_tax\n",
    "        else: \n",
    "            if all(w not in phrase for phrase in include_tax):\n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text) \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"title\"]]\n",
    "df[\"description_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"description\"]]\n",
    "df[\"comments_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"comments\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd346a",
   "metadata": {},
   "source": [
    "### ii)  Named Entity Recognition (Methodology to recommend potential taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef78db3",
   "metadata": {},
   "source": [
    "### a) Run existing Spacy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f194201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#load pre existing spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdd0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pipeline components\n",
    "nlp.pipe_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ac11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to display basic entity info:\n",
    "def show_ents_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            return(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "    else:\n",
    "        return('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_ner\"]=  [show_ents_spacy(text)for text in df[\"title\"]]\n",
    "df[\"description_ner\"]=  [show_ents_spacy(text) for text in df[\"description\"]]\n",
    "df[\"comments_ner\"]=  [show_ents_spacy(text) for text in df[\"comments\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2bda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/'+'ner_spacy.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d650d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb05a",
   "metadata": {},
   "source": [
    "### 2) Train custom NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d34b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def convert_spacy(DATA):\n",
    "    \"\"\"\n",
    "    Convert data into .spacy format\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        \n",
    "    return db\n",
    "\n",
    "    \n",
    "def custom_ner(TRAIN_DATA,VAL_DATA,path):\n",
    "    \"\"\"\n",
    "    Build custom NER model\n",
    "    \"\"\"\n",
    "    #convert train and validation data into .spacy format\n",
    "    db_train = convert_spacy(TRAIN_DATA) \n",
    "    db_val = convert_spacy(VAL_DATA) \n",
    "    \n",
    "    #save train and validation data in .spacy format in path\n",
    "    db_train.to_disk(path +'train.spacy')\n",
    "    db_val.to_disk(path +'val.spacy')\n",
    "    \n",
    "    #autofill base_config file saved by user from spacy website\n",
    "    !python -m spacy init fill-config base_config.cfg config.cfg\n",
    "    \n",
    "    #Model building and saving in path\n",
    "    !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy\n",
    "    \n",
    "    print(\"Custom NER model built and saved!\")\n",
    " \n",
    "\n",
    "def entities(TRAIN_DATA,VAL_DATA,path,text):\n",
    "    print(text)\n",
    "    custom_ner(TRAIN_DATA,VAL_DATA,path)\n",
    "     #Load best model\n",
    "    nlp = spacy.load(path + \"/output/model-best/\")     \n",
    "    print(\"Best model loaded!\")\n",
    "    \n",
    "    doc= nlp(text) #create doc object\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            return(ent.text+' - '+ent.label_)\n",
    "    else:\n",
    "        return('No named entities found.')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# def ner_wrapper(TRAIN_DATA,VAL_DATA,path,text):\n",
    "#     nlp = custom_ner(TRAIN_DATA,VAL_DATA,path)\n",
    "#     text = show_ents(text)\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea345e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df before manipulation: (600, 3)\n",
      "Shape of df after selecting columns: (600, 3)\n",
      "Number of null values in df:\n",
      " title          297\n",
      "description      2\n",
      "comments       335\n",
      "dtype: int64\n",
      "Number of null values in df after NA imputation:\n",
      " title          0\n",
      "description    0\n",
      "comments       0\n",
      "dtype: int64\n",
      "Number of duplicates in the df: 0\n",
      "Shape of df after manipulation: (600, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>gio planning test skipping</td>\n",
       "      <td>i have an issue  using the test case api py sc...</td>\n",
       "      <td>fbakhda story has been planned for this sprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>gio stop schedule does not actually stop the ...</td>\n",
       "      <td>we had multiple test recipes queued up in the ...</td>\n",
       "      <td>waikitcx hi arisha please provide us the sche...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>create new repo under seg tbh dse piv pse</td>\n",
       "      <td>repo name hspe thb kpi it should preferably be...</td>\n",
       "      <td>siewlita hi project is created in gio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>de cannot delete recipes even they are not li...</td>\n",
       "      <td>trying to delete test recipes yielded the foll...</td>\n",
       "      <td>soonhenx hi ken s ng can you verify the issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>unchecked heartbeatd and gvd logs is eating sp...</td>\n",
       "      <td>after each run the sut has no more space left ...</td>\n",
       "      <td>wteh hi ken s ng thanks for bringing up this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "601                         gio planning test skipping   \n",
       "602   gio stop schedule does not actually stop the ...   \n",
       "603         create new repo under seg tbh dse piv pse    \n",
       "604   de cannot delete recipes even they are not li...   \n",
       "605  unchecked heartbeatd and gvd logs is eating sp...   \n",
       "\n",
       "                                           description  \\\n",
       "601  i have an issue  using the test case api py sc...   \n",
       "602  we had multiple test recipes queued up in the ...   \n",
       "603  repo name hspe thb kpi it should preferably be...   \n",
       "604  trying to delete test recipes yielded the foll...   \n",
       "605  after each run the sut has no more space left ...   \n",
       "\n",
       "                                              comments  \n",
       "601   fbakhda story has been planned for this sprin...  \n",
       "602   waikitcx hi arisha please provide us the sche...  \n",
       "603             siewlita hi project is created in gio   \n",
       "604   soonhenx hi ken s ng can you verify the issue...  \n",
       "605   wteh hi ken s ng thanks for bringing up this ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train data\n",
    "TRAIN_DATA = [\n",
    "[\"jchun wai kit is working on this to enable in new tcp\", {\"entities\": [[0, 13, \"NAME\"]]}], \n",
    "[\"siewlita pending release\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi lim chih quanx per our communication i still have one more question\", {\"entities\": [[3, 17, \"NAME\"]]}],\n",
    "[\"yeetheng the auto test trigger after build complete is working fine today\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi jon here is the recipe link weichuan hi can you try to reproduce the issue once more\", {\"entities\": [[3, 6, \"NAME\"],[31, 39, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "VAL_DATA = [\n",
    "[\"wei chuan has updated me with the sample of test execution by automation manual chart\", {\"entities\": [[0, 9, \"NAME\"]]}],\n",
    "[\"subject gio logs and gio installation hi ajay jonathan i just noticed that star is directing all the logs to gio folder\", {\"entities\": [[41, 45, \"NAME\"],[46, 55, \"NAME\"]]}],\n",
    "[\"hi firesh final verdict in jenkins coming as fail even after all the triggered tests are passed\", {\"entities\": [[3, 9, \"NAME\"],[27, 35, \"NAME\"]]}],\n",
    "[\"wai kit below is the requirement needed from gio product defect detection\", {\"entities\": [[0, 7, \"NAME\"]]}],\n",
    "[\"just string field regards robert nowicki\", {\"entities\": [[26, 40, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "#jupyter notebook and base_config.cfg path have to be the same\n",
    "path = \"C:/Users/nchong/\"\n",
    "\n",
    "#load and clean test data\n",
    "df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "df_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1651.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1238.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
      "[i] Saving to output directory: outputCustom NER model built and saved!\n",
      "\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     25.83   10.81    5.97   57.14    0.11\n",
      "200     200         48.06    711.39   44.44  100.00   28.57    0.44\n",
      "400     400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "600     600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "800     800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1000    1000          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1200    1200          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1400    1400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1600    1600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1800    1800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-07 14:45:36,194] [INFO] Set up nlp object from config\n",
      "[2021-10-07 14:45:36,202] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-10-07 14:45:36,206] [INFO] Created vocabulary\n",
      "[2021-10-07 14:45:36,207] [INFO] Finished initializing nlp object\n",
      "[2021-10-07 14:45:36,283] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1237.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 710.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-07 14:47:41,365] [INFO] Set up nlp object from config\n",
      "[2021-10-07 14:47:41,376] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-10-07 14:47:41,381] [INFO] Created vocabulary\n",
      "[2021-10-07 14:47:41,382] [INFO] Finished initializing nlp object\n",
      "[2021-10-07 14:47:41,475] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: outputCustom NER model built and saved!\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     25.83   10.81    5.97   57.14    0.11\n",
      "200     200         48.06    711.39   44.44  100.00   28.57    0.44\n",
      "400     400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "600     600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "800     800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1000    1000          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1200    1200          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1400    1400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1600    1600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1800    1800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 1000.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 999.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-07 14:49:41,544] [INFO] Set up nlp object from config\n",
      "[2021-10-07 14:49:41,560] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-10-07 14:49:41,568] [INFO] Created vocabulary\n",
      "[2021-10-07 14:49:41,570] [INFO] Finished initializing nlp object\n",
      "[2021-10-07 14:49:41,725] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     25.83   10.81    5.97   57.14    0.11\n",
      "200     200         48.06    711.39   44.44  100.00   28.57    0.44\n",
      "400     400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "600     600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "800     800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1000    1000          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1200    1200          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1400    1400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1600    1600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1800    1800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n",
      "Custom NER model built and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 714.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 833.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-10-07 14:51:53,912] [INFO] Set up nlp object from config\n",
      "[2021-10-07 14:51:53,923] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2021-10-07 14:51:53,927] [INFO] Created vocabulary\n",
      "[2021-10-07 14:51:53,928] [INFO] Finished initializing nlp object\n",
      "[2021-10-07 14:51:54,058] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: output\n",
      "Custom NER model built and saved![i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     25.83   10.81    5.97   57.14    0.11\n",
      "200     200         48.06    711.39   44.44  100.00   28.57    0.44\n",
      "400     400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "600     600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "800     800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1000    1000          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1200    1200          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1400    1400          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1600    1600          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "1800    1800          0.00      0.00   44.44  100.00   28.57    0.44\n",
      "[+] Saved pipeline to output directory\n",
      "output\\model-last\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 999.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 713.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "df_test[\"title_ner\"]=  [entities(TRAIN_DATA,VAL_DATA,path,text) for text in df_test[\"title\"]]\n",
    "df_test[\"description_ner\"]= [entities(TRAIN_DATA,VAL_DATA,path,text)for text in df_test[\"description\"]]\n",
    "df_test[\"comments_ner\"]=  [entities(TRAIN_DATA,VAL_DATA,path,text) for text in df_test[\"comments\"]]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0122305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert and save train/validation data as .spacy\n",
    "# out_path = \"C:/Users/nchong/\"\n",
    "# db_train = convert_spacy(TRAIN_DATA)\n",
    "# db_train.to_disk(out_path +'train.spacy') # save the docbin object\n",
    "# db_val = convert_spacy(VAL_DATA)\n",
    "# db_val.to_disk(out_path +'val.spacy') # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d90af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8142e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model\n",
    "# nlp1 = spacy.load(\"C:/Users/nchong/output/model-best/\") #load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ae32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp1(\"waikitcx hi arisha please provide us the\") # input sample text\n",
    "\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb05834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp1(\"weichuan hi hashim the feature has been released please verified if the feature work correctly thank you very much\") # input sample text\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ffb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_ents(text):\n",
    "#     doc= nlp1(text)\n",
    "#     if doc.ents:\n",
    "#         for ent in doc.ents:\n",
    "#             return(ent.text+' - '+ent.label_)\n",
    "#     else:\n",
    "#         return('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "# df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac2caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[\"title_ner\"]=  [show_ents(text)for text in df_test[\"title\"]]\n",
    "# df_test[\"description_ner\"]=  [show_ents(text) for text in df_test[\"description\"]]\n",
    "# df_test[\"comments_ner\"]=  [show_ents(text) for text in df_test[\"comments\"]]\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54740791",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec289e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def feature_extraction(column,ngram_range,ascending,fe_type):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - Bag of words or TF-IDF\n",
    "    \n",
    "    params:\n",
    "    column [series]: column to select\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    \"\"\"\n",
    "    \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == \"tfidf\":\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df.iloc[:3,0]\n",
    "ngram_range = (1,1)\n",
    "ascending = None\n",
    "fe_type = \"bagofwords\"\n",
    "feature_extraction(column,ngram_range,ascending,fe_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df.iloc[:3,0]\n",
    "ngram_range = (1,1)\n",
    "ascending = True\n",
    "fe_type = \"tfidf\"\n",
    "feature_extraction(column,ngram_range,ascending,fe_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08c9f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fill NAN with empty string\n",
    "# def columns_req(text_list,df):\n",
    "#     \"\"\"\n",
    "#     Filters df to only include the string columns provided by user\n",
    "#     Fills NAN with empty string\n",
    "    \n",
    "#     params:\n",
    "#     text_list[list]: list of columns names  \n",
    "#     df [dataframe]: input dataframe \n",
    "#     \"\"\"\n",
    "#     df= df[text_list] \n",
    "#     df = df.fillna('')\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 'title','description','comments'\n",
    "# text_list = ['title','description','comments']\n",
    "# df = columns_req(text_list,df)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of words to remove\n",
    "# remove_tax = [\"gio\",\"fields\",\"test\"]\n",
    "# #list of words to maintain\n",
    "# include_tax = [\"test suite execution\",\"cloning defects\"]\n",
    "\n",
    "# text = \"gio fields test suite execution and test cycle\"\n",
    "# for w in remove_tax: \n",
    "#     if all(w not in phrase for phrase in include_tax): #word in include_tax but not in text -> no tse/cd\n",
    "#         print(w,\"yes\")\n",
    "#     else:\n",
    "#         print(w,\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b505a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #list of words to remove\n",
    "# remove_tax = [\"gio\",\"fields\",\"test\"]\n",
    "# #list of words to maintain\n",
    "# include_tax = [\"test suite execution\",\"test provide method\"]\n",
    "\n",
    "# import re\n",
    "# def custom_tax(text,remove_tax,include_tax):\n",
    "#     for w in remove_tax: #\"gio\",\"fields\",\"test\"\n",
    "#         for phrase in include_tax: #\"test suite execution\",\"provide method\"\n",
    "         \n",
    "#             if w not in phrase: #\"gio\",\"fields\n",
    "#                 pattern = r'\\b'+w+r'\\b'\n",
    "#                 text = re.sub(pattern,' ', text)\n",
    "#             else: #\"test\"\n",
    "#                 if phrase not in text:\n",
    "#                     pattern = r'\\b'+w+r'\\b'\n",
    "#                     text = re.sub(pattern,' ', text)                                             \n",
    "            \n",
    "#     return text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
