{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889562e",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0951373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import glob, os, json\n",
    "import re\n",
    "\n",
    "#user input file path\n",
    "path = 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(path,df=None,date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    Can support incremental aggregation of dataset, by setting arg df as the existing dataframe\n",
    "    Returns a single dataframe.\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    df [dataframe] (optional,default is None): input existing dataframe to merge with new files\n",
    "    date [\"string\"](optional,default is None): user can choose to load only files from specific date in YYYY-MM-DD format\n",
    "    '''\n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    dfs = []\n",
    "\n",
    "    if df is None: #no existing dataframe\n",
    "        \n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "            if date == None:\n",
    "                pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "                \n",
    "            else:\n",
    "#              \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "    \n",
    "            match = re.search(pattern,file)\n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "                for file in file_list:\n",
    "                    with open(file) as f:\n",
    "                        #flatten json into pd dataframe\n",
    "                        json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                        #label which file each row is from \n",
    "                        json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                    dfs.append(json_data)\n",
    "                df = pd.concat(dfs)\n",
    "                \n",
    "    else: #existing dataframe exists and want to append new files to existing dataframe\n",
    "             \n",
    "        for file in filenames:\n",
    "\n",
    "            if file not in df[\"file\"].unique(): #check if file is new - to support merging of new dataset with previously read ones\n",
    "\n",
    "                # search agreed file format pattern in the filename\n",
    "                \n",
    "                if date == None:\n",
    "                    pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "                else:\n",
    "                    pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "                     \n",
    "                match = re.search(pattern,file)\n",
    "\n",
    "                #if match is found\n",
    "                if match:\n",
    "                    json_pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(json_pattern) #list of json files \n",
    "\n",
    "                    for file in file_list:\n",
    "                        with open(file) as f:\n",
    "                            #flatten json into pd dataframe\n",
    "                            json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                            #label which file each row is from \n",
    "                            json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                        dfs.append(json_data)\n",
    "                    new_df = pd.concat(dfs)           \n",
    "                    df=pd.concat([df,new_df])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9aa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c24924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load only files that follow the agreed format, does not choose file by date\n",
    "df = data_loading(path,df=None,date = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16661452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load only files that follow the agreed format, choose file by date\n",
    "# date= \"2021-08-25\"\n",
    "# df = data_loading(path,df=None,date = date)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c03d8",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "### a) Dataframe manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baadc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(df,how,keep,cols_tokeep=None,cols_todrop=None,impute_value=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep or drop columns in dataframe\n",
    "    2) Data impute: Impute or drop NA rows \n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    cols_tokeep [list/None]: list of columns to keep, if there is no list use None\n",
    "    cols_todrop [list/None]: list of columns to drop, if there is no list use None\n",
    "    impute_value [string/None]: value to be imputed (i.e \"\" for empty string). If no value to be imputed but there are \n",
    "                        rows to be dropped use None\n",
    "    how[string]: Drop rows when we have at least one NA or all NA. Choose\n",
    "                      # - \"all\": Drop row with all NA\n",
    "                      # - \"any\": Drop row with at least one NA\n",
    "                  \n",
    "    subset[list/None]: Subset of columns for dropping NA and identifying duplicates, use None if no column to select\n",
    "    keep[string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                        # - \"first\" : Drop duplicates except for the first occurrence. \n",
    "                        # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                        # - False : Drop all duplicates.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select columns or drop unwanted columns\n",
    "    if cols_tokeep != None:\n",
    "        df = df[cols_tokeep]\n",
    "    if cols_todrop != None:\n",
    "        df = df.drop(cols_todrop,axis=1)\n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with user's choice of imputation value\n",
    "    if impute_value != None:\n",
    "        df = df.fillna(impute_value)\n",
    "        print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "        \n",
    "    else: # drop rows with NA values\n",
    "        df= df.dropna(axis=0, how=how,subset=subset)\n",
    "        print(\"Number of null values in df after dropping NA rows:\\n\",df.isnull().sum())\n",
    "        print(\"Shape of df after dropping NA rows:\",df.shape)\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8931eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\"],cols_todrop=None,impute_value=None,subset=[\"title\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2089e",
   "metadata": {},
   "source": [
    "\n",
    "### b) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1540a0a",
   "metadata": {},
   "source": [
    "### 2) Expand contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453da296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def word_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return \" \".join([contractions.fix(word) for word in text.split()])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff07071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "# df[\"desc_cont\"]=  [word_contractions(text) for text in df[\"description\"]]\n",
    "# df[\"comments_cont\"]=  [word_contractions(text) for text in df[\"comments\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47492958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e49df",
   "metadata": {},
   "source": [
    "### 3) Convert all characters into lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01121fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return text.lower() if type(text) == str else text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "# df[\"desc_lower\"]= [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "# df[\"comments_lower\"]= [lowercase(text) for text in df[\"comments_cont\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_lower\",\"desc_lower\",\"comments_lower\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a61d4",
   "metadata": {},
   "source": [
    "### 4) Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3ba0c",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97f0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def stem_words(text,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words, 2 options available: Porter Stemmer or Lancaster Stemmer \n",
    "    params:\n",
    "    text[string]: input string \n",
    "    stemmer_type[string]: input stemming method (\"Porter\" or \"Lancaster\")\n",
    "    \"\"\"\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"title_lower\"]]\n",
    "df1[\"desc_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"desc_lower\"]]\n",
    "df1[\"comments_stem_lan\"]= [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"comments_lower\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_por\"] = [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"title_lower\"]]\n",
    "df1[\"desc_stem_por\"] = [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"desc_lower\"]]\n",
    "df1[\"comments_stem_por\"]= [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"comments_lower\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48f7de",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = df.copy()\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11146a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(column,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words, 2 options available: WordNetLemmatizer or Spacy \n",
    "    params:\n",
    "    column[series]: input series/column to be lemmatized\n",
    "    lemma_type[string]: input lemmatization method (\"WordNet\" or \"Spacy\")\n",
    "    \"\"\"\n",
    "    if lemma_type == \"WordNet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return column.apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    \n",
    "    \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        column = column.apply(lambda text: \" \".join([w.lemma_ for w in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        column = column.apply(lambda text: text.lower() if type(text) == str else text )\n",
    "        \n",
    "        return column\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7092bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2[\"title_lemma_spacy\"] = lemmatize_words(column= df2[\"title_rem\"],lemma_type=\"Spacy\")\n",
    "# df2[\"desc_lemma_spacy\"] = lemmatize_words(column= df2[\"desc_rem\"],lemma_type=\"Spacy\")\n",
    "# df2[\"comments_lemma_spacy\"] = lemmatize_words(column= df2[\"comments_rem\"],lemma_type=\"Spacy\")\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18243072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lemma_word\"] = lemmatize_words(column= df[\"title_rare\"],lemma_type=\"WordNet\")\n",
    "# df2[\"desc_lemma_word\"] = lemmatize_words(column= df2[\"desc_rem\"],lemma_type=\"WordNet\")\n",
    "# df2[\"comments_lemma_word\"] = lemmatize_words(column= df2[\"comments_rem\"],lemma_type=\"WordNet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d27c8",
   "metadata": {},
   "source": [
    "### b) Noise filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661e07",
   "metadata": {},
   "source": [
    "### 1) Remove html tag and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "027213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def remove_htmltag_url(text):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    text [string]: input string\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    #remove html tag\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True) \n",
    "    #remove url\n",
    "    text_clean = re.sub('https?[://%]*\\S+', ' ',text) \n",
    "    return text_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f23e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "# df[\"desc_tag\"]= [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "# df[\"comments_tag\"]= [remove_htmltag_url(text) for text in df[\"comments_lower\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2f91a",
   "metadata": {},
   "source": [
    "### 3) Remove irrelevant characters, punctuation, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02440a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_tag\",\"desc_tag\",\"comments_tag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_irrchar_punc(text,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation\n",
    "    params:\n",
    "    \n",
    "    text[string]: input string \n",
    "    characters[string]: input regex of characters to be removed\n",
    "    \"\"\"\n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        text = re.sub(char, ' ',text) \n",
    "    \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    text = re.sub(r'\\\\+x[\\d\\D][\\d\\D]', ' ',text) \n",
    "    \n",
    "    #Remove special characters and punctuation\n",
    "    text = re.sub('[^\\w\\s]', ' ',text) \n",
    "    text = re.sub(r'_', ' ',text) \n",
    "#     df = df.replace('[^\\w\\s]',' ', regex=True)\n",
    "#     df = df.replace(r\"_\", \" \", regex=True)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a76f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#char=None\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "# df[\"desc_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "# df[\"comments_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d676006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1] #desc before rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27350f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4] #desc rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2235b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#special character removal added by user\n",
    "char = '\\++\\d+'\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=char) for text in df[\"title_tag\"]]\n",
    "df[\"desc_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"desc_tag\"]]\n",
    "df[\"comments_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec85f0",
   "metadata": {},
   "source": [
    "### 3) Remove numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_rem\",\"desc_rem\",\"comments_rem\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c131608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub('\\d+', ' ',text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56627dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "# df[\"desc_num\"]= [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "# df[\"comments_num\"]= [remove_num(text) for text in df[\"comments_rem\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f0d60",
   "metadata": {},
   "source": [
    "### 4) Remove multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_num\",\"desc_num\",\"comments_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multwhitespace(text):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub(' +', ' ',text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "# df[\"desc_white\"]= [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "# df[\"comments_white\"]= [remove_multwhitespace(text) for text in df[\"comments_num\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb117fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e5d5",
   "metadata": {},
   "source": [
    "### 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ca58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_white\",\"desc_white\",\"comments_white\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    text[string]: input string\n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        text = re.sub(pattern,' ', text)\n",
    "                   \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words/phrase to be added to the stop words \n",
    "# extra_sw = ['hsdes',\"testing\"]\n",
    "#list of words/phrase to be removed from stop words\n",
    "# remove_sw = [\"i\",\"am\"]\n",
    "\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "# df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"desc_white\"]]\n",
    "# df[\"comments_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"comments_white\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905229f9",
   "metadata": {},
   "source": [
    "### 5) Remove frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37464615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a389eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freqwords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    column[series]: input column to remove frequent words\n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed from column:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "# df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "# df[\"comments_freq\"] = remove_freqwords(df[\"comments_stop\"],n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada84766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952af778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ef266",
   "metadata": {},
   "source": [
    "### 6) Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50d0e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rarewords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    column[series]: input column to remove rare words\n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the rare words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed from columns:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "        \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "# df[\"desc_rare\"] = remove_rarewords(df[\"desc_stop\"],n)\n",
    "# df[\"comments_rare\"] = remove_rarewords(df[\"comments_stop\"],n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,1] #converting is rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4224f",
   "metadata": {},
   "source": [
    "### c) Custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c45e0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "#remove token method - seperate nltk and split functions \n",
    "def cust_tokenization(column,token_met,token_type,delim =None):\n",
    "    \"\"\"\n",
    "    Custom tokenization, 2 options are available: split() or nltk \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    token_met[\"string\"]: input tokenization method (\"split\" or \"nltk\")\n",
    "    \n",
    "    token_type[\"string\"](use only if token_met= \"nltk\"): type of nltk tokenization\n",
    "    a) token_type = \"WordToken\" tokenizes a string into a list of words\n",
    "    b) token_type = \"SentToken\" tokenizes a string containing sentences into a list of sentences\n",
    "    c) token_type = \"WhiteSpaceToken\" tokenizes a string on whitespace (space, tab, newline)\n",
    "    d) token_type = \"WordPunctTokenizer\" tokenizes a string on punctuations\n",
    "         \n",
    "    delim[\"string\"](use only if token_met = \"split\"): specify delimiter to separate strings,\n",
    "    default delimiter (delim=None) is whitespace,  an alternate option for token_type = \"WhiteSpaceToken\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if token_met == \"split\":\n",
    "        if delim==None:\n",
    "            print(\"Text is split by space\") #default delimiter is space if not specified \n",
    "\n",
    "        else:\n",
    "            print(\"Text is split by:\", delim) #can accept one or more delimiter\n",
    "\n",
    "        return column.apply(lambda text: text.split() if delim==None else text.split(delim))\n",
    "    \n",
    "\n",
    "    if token_met == \"nltk\":\n",
    "    \n",
    "        if token_type == \"WordToken\":\n",
    "            tokenizer = word_tokenize\n",
    "        if token_type == \"SentToken\":\n",
    "            tokenizer = sent_tokenize\n",
    "        if token_type == \"WhiteSpaceToken\":\n",
    "            tokenizer = WhitespaceTokenizer().tokenize\n",
    "        if token_type == \"WordPunctTokenizer\":\n",
    "            tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "        return column.apply(lambda text: tokenizer(text))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe926d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split\n",
    "token_met=\"split\"\n",
    "token_type=None\n",
    "delim = None\n",
    "\n",
    "df[\"title_token\"]= cust_tokenization(column=df[\"title_stop\"],token_met=token_met,token_type=token_type,delim=delim)  \n",
    "df[\"desc_token\"]=  cust_tokenization(column=df[\"desc_stop\"],token_met=token_met,token_type=token_type,delim=delim) \n",
    "df[\"comments_token\"]= cust_tokenization(column=df[\"comments_stop\"],token_met=token_met,token_type=token_type,delim=delim)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1827f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk\n",
    "token_met=\"nltk\"\n",
    "token_type=\"WordToken\"\n",
    "delim = None\n",
    "\n",
    "df[\"title_token_nltk\"]= cust_tokenization(column=df[\"title_stop\"],token_met=token_met,token_type=token_type,delim=delim)  \n",
    "df[\"desc_token_nltk\"]=  cust_tokenization(column=df[\"desc_stop\"],token_met=token_met,token_type=token_type,delim=delim) \n",
    "df[\"comments_token_nltk\"]= cust_tokenization(column=df[\"comments_stop\"],token_met=token_met,token_type=token_type,delim=delim)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013f586",
   "metadata": {},
   "source": [
    "## d) Custom taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4535b2",
   "metadata": {},
   "source": [
    "### i) Configurability for user to provide taxonomy mapping (to remove/remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df897ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def custom_taxo(text,remove_taxo,include_taxo):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for w in remove_taxo:\n",
    "        #row without any item from include_taxo -> replace all remove_taxo items with empty string\n",
    "        if all(phrase not in text for phrase in include_taxo): \n",
    "            pattern = r'\\b'+w+r'\\b'\n",
    "            text = re.sub(pattern,' ', text) \n",
    "        #row with any item from include_taxo -> only replace remove_taxo item that is not in include_taxo\n",
    "        else: \n",
    "            if all(w not in phrase for phrase in include_taxo):\n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text) \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d7fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words to remove\n",
    "remove_taxo = [\"gio\",\"fields\",\"test\"]\n",
    "#list of words to maintain\n",
    "include_taxo = [\"test suite execution\",\"clone defects\"]\n",
    "\n",
    "df[\"title_taxo\"]=  [custom_taxo(text,remove_taxo,include_taxo) for text in df[\"title_stop\"]]\n",
    "# df[\"description_taxo\"]=  [custom_taxo(text,remove_taxo,include_taxo) for text in df[\"desc_stop\"]]\n",
    "# df[\"comments_taxo\"]=  [custom_taxo(text,remove_taxo,include_taxo) for text in df[\"comments_stop\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd346a",
   "metadata": {},
   "source": [
    "### ii)  Named Entity Recognition (Methodology to recommend potential taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb05a",
   "metadata": {},
   "source": [
    "### Train custom NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d34b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "#user to understand requirement - examples \n",
    "def convert_spacy(DATA):\n",
    "    \"\"\"\n",
    "    Convert  data into .spacy format\n",
    "    DATA[]: Train/validation data to be converted to .spacy format\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        \n",
    "    return db\n",
    "\n",
    "    \n",
    "def custom_ner(TRAIN_DATA,VAL_DATA,path):\n",
    "    \"\"\"\n",
    "    Build and save custom NER model in given path. \n",
    "    \n",
    "    \"\"\"\n",
    "    #convert train and validation data into .spacy format\n",
    "    db_train = convert_spacy(TRAIN_DATA) \n",
    "    db_val = convert_spacy(VAL_DATA) \n",
    "    \n",
    "    #save train and validation data in .spacy format in path\n",
    "    db_train.to_disk(path +'train.spacy')\n",
    "    db_val.to_disk(path +'val.spacy')\n",
    "    \n",
    "    print(\"Train and validation converted to .spacy format and saved\")\n",
    "    \n",
    "    #autofill base_config file saved by user from spacy website\n",
    "    !python -m spacy init fill-config base_config.cfg config.cfg\n",
    "    \n",
    "    #Model building and saving in path\n",
    "    !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy\n",
    "    \n",
    "    print(\"Custom NER model built and saved!\")\n",
    "    \n",
    "def check_ents(path,column):\n",
    "    \"\"\"\n",
    "    Check entities after loading best model\n",
    "    \n",
    "    \"\"\"\n",
    "    #Load best model\n",
    "    nlp = spacy.load(path + \"/output/model-best/\")     \n",
    "    print(\"Best model loaded!\")\n",
    "    \n",
    "    entities = []\n",
    "    for text in column.tolist():\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.text+' - '+ent.label_)\n",
    "    print(np.unique(np.array(entities)))        \n",
    "\n",
    "def ner_wrapper(TRAIN_DATA,VAL_DATA,path,column,train_model):  \n",
    "    \"\"\"\n",
    "    User can choose to train the spacy model or load spacy model\n",
    "    params:\n",
    "    TRAIN_DATA[NER format]: train data for model building\n",
    "    VAL_DATA[NER format]: validation data for model building\n",
    "    path[string]: input path to store model. Path has to be the same as base_config.cfg file downloaded from spacy\n",
    "                  website and jupyter notebook.\n",
    "    column[series]: column for entities to be checked\n",
    "    train_model[True/False]: True if want to train model. False to load model (no training)\n",
    "    \"\"\"\n",
    "    if train_model == True:\n",
    "        custom_ner(TRAIN_DATA,VAL_DATA,path)\n",
    "        check_ents(path,column)\n",
    "        \n",
    "    if train_model == False:\n",
    "        check_ents(path,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "TRAIN_DATA = [\n",
    "[\"jchun wai kit is working on this to enable in new tcp\", {\"entities\": [[0, 13, \"NAME\"]]}], \n",
    "[\"siewlita pending release\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi lim chih quanx per our communication i still have one more question\", {\"entities\": [[3, 17, \"NAME\"]]}],\n",
    "[\"yeetheng the auto test trigger after build complete is working fine today\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi jon here is the recipe link weichuan hi can you try to reproduce the issue once more\", {\"entities\": [[3, 6, \"NAME\"],[31, 39, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "VAL_DATA = [\n",
    "[\"wei chuan has updated me with the sample of test execution by automation manual chart\", {\"entities\": [[0, 9, \"NAME\"]]}],\n",
    "[\"subject gio logs and gio installation hi ajay jonathan i just noticed that star is directing all the logs to gio folder\", {\"entities\": [[41, 45, \"NAME\"],[46, 55, \"NAME\"]]}],\n",
    "[\"hi firesh final verdict in jenkins coming as fail even after all the triggered tests are passed\", {\"entities\": [[3, 9, \"NAME\"],[27, 35, \"NAME\"]]}],\n",
    "[\"wai kit below is the requirement needed from gio product defect detection\", {\"entities\": [[0, 7, \"NAME\"]]}],\n",
    "[\"just string field regards robert nowicki\", {\"entities\": [[26, 40, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "#jupyter notebook and base_config.cfg path have to be the same\n",
    "path = \"C:/Users/nchong/\"\n",
    "\n",
    "#load and clean test data\n",
    "df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54740791",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014c2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_lemma_word\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec289e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def feature_extraction(column,ngram_range,ascending,fe_type):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - Bag of words or TF-IDF\n",
    "    \n",
    "    params:\n",
    "    column [series]: column to select\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    \"\"\"\n",
    "    \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == \"tfidf\":\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df,vec_type,vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397a1c45",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "### i ) K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67203a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d545a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "silhouette_avg_list = []\n",
    "n_clusters_list = []\n",
    "dicts = {}\n",
    "\n",
    "def kmeans_clustering(X,vec_type,top_n_terms,n_clusters=None,max_n_clusters=None):\n",
    "    \"\"\"\n",
    "    K- means clustering for unsupervised learning. User can choose either options:\n",
    "    (1) provide the number of clusters or\n",
    "    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest \n",
    "    silhouette score will be chosen. Min number of clusters is fixed as 2\n",
    "    \n",
    "    params:\n",
    "    X[sparse matrix]: sparse matrix obtained from feature extraction function\n",
    "    vec_type: vec_type obtained from feature extraction function\n",
    "    top_n_terms[int]: the top n terms in each cluster to be printed out\n",
    "    n_clusters[None/int]: number of clusters. Choose None for option (2)  \n",
    "    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  \n",
    "    \"\"\"\n",
    "    #user provides the number of clusters \n",
    "    X = X.drop(index='sum')\n",
    "    \n",
    "    if n_clusters != None:\n",
    "        model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "\n",
    "        silhouette_score = metrics.silhouette_score(X, labels,random_state=42)\n",
    "        print(\"Silhouette score for\",n_clusters,\"clusters is\",round(silhouette_score,3))\n",
    "        \n",
    "        \n",
    "    \n",
    "    #user provides the maximum number of clusters \n",
    "    if max_n_clusters != None:\n",
    "        for n_clusters in range(2,max_n_clusters+1): \n",
    "\n",
    "            model = KMeans(n_clusters = n_clusters, random_state=42)\n",
    "            model.fit_predict(X)\n",
    "            labels = model.labels_\n",
    "\n",
    "            silhouette_avg = metrics.silhouette_score(X, labels,random_state=42)\n",
    "            print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", round(silhouette_avg,3))\n",
    "\n",
    "            silhouette_avg_list.append(silhouette_avg)\n",
    "            n_clusters_list.append(n_clusters)\n",
    "\n",
    "\n",
    "        for i in range(len(n_clusters_list)):\n",
    "            dicts[n_clusters_list[i]] = silhouette_avg_list[i]\n",
    "\n",
    "        n_clusters_max = max(dicts,key=dicts.get)\n",
    "        silhouette_avg_max = max(dicts.values())\n",
    "\n",
    "        model = KMeans(n_clusters = n_clusters_max, random_state=42)\n",
    "        model.fit_predict(X)\n",
    "        labels = model.labels_\n",
    "        n_clusters = n_clusters_max\n",
    "        print(\"\\nThe optimal number of clusters selected is\",n_clusters_max,\"with average silhouette_score of\",round(silhouette_avg_max,3),\"\\n\") \n",
    "        \n",
    "    print(\"Top\",top_n_terms,\"terms per cluster:\")\n",
    "    order_centroids = model.cluster_centers_.argsort()[:, ::-1] #sort by descending order\n",
    "    terms = vec_type.get_feature_names()\n",
    "    for i in range(n_clusters):\n",
    "        print(\"Cluster %d:\" % i)\n",
    "        print(['%s' % terms[ind] for ind in order_centroids[i, :top_n_terms]]) #top n terms in each cluster\n",
    "        print(\"\\n\")\n",
    "   \n",
    "               \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd60b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Case 1: user provides the number of clusters ####\n",
    "#feature extraction\n",
    "column = df[\"title_lemma_word\"]\n",
    "ngram_range = (1,1)\n",
    "ascending = False\n",
    "fe_type = \"bagofwords\"\n",
    "X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "#k means clustering\n",
    "df1[\"cluster_case1\"] = kmeans_clustering(X,vec_type,top_n_terms=10,n_clusters=5,max_n_clusters=None)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b623ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### user provides max number of clusters ### \n",
    "#feature extraction\n",
    "column = df[\"title_lemma_word\"]\n",
    "ngram_range = (1,1)\n",
    "ascending = False\n",
    "fe_type = \"bagofwords\"\n",
    "X = feature_extraction(column,ngram_range,ascending,fe_type)[0]\n",
    "vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "\n",
    "#k means clustering\n",
    "df1[\"cluster_case2\"] = kmeans_clustering(X,vec_type,top_n_terms=10,n_clusters=None,max_n_clusters=20)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0885667b",
   "metadata": {},
   "source": [
    "### ii) LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of LDA:\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def lda(vectorized,vec_type,n_components,top_n_terms):\n",
    "    \"\"\"\n",
    "    LDA for unsupervised learning. Select \"bagofwords\" for feature extraction\n",
    "    params:\n",
    "    vectorized: vectorized obtained from feature extraction function\n",
    "    vec_type: vec_type obtained from from feature extraction function\n",
    "    n_components[int]: the number of topics/clusters used in the lda_model\n",
    "    top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "    \"\"\"\n",
    "    # Create object for the LDA class \n",
    "    lda_model = LatentDirichletAllocation(n_components, random_state = 42)  \n",
    "    lda_model.fit(vectorized)\n",
    "    \n",
    "    # Components_ gives us our topic distribution \n",
    "    topic_words = lda_model.components_\n",
    "\n",
    "    # Top n words for a topic\n",
    "\n",
    "    for i,topic in enumerate(topic_words):\n",
    "        print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "        print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    topic_results = lda_model.transform(vectorized) #probabilities of doc belonging to particular topic\n",
    "    \n",
    "# #     Log Likelyhood: Higher the better\n",
    "#     print(\"Log Likelihood: \", lda_model.score(vectorized))\n",
    "\n",
    "#     # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "#     print(\"Perplexity: \", lda_model.perplexity(vectorized))\n",
    "    \n",
    "    return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9d679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user provides number of component and top n terms in each cluster/topic\n",
    "#feature extraction\n",
    "column = df[\"title_lemma_word\"]\n",
    "ngram_range = (1,1)\n",
    "ascending = False\n",
    "fe_type = \"bagofwords\"\n",
    "vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "#LDA\n",
    "df2[\"topic\"] = lda(vectorized,vec_type,n_components=5,top_n_terms=10)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d21c2",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3672ad58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>status</th>\n",
       "      <th>reason</th>\n",
       "      <th>merge_id</th>\n",
       "      <th>problem_area</th>\n",
       "      <th>submitted_date</th>\n",
       "      <th>root_caused_date</th>\n",
       "      <th>description</th>\n",
       "      <th>fix_description</th>\n",
       "      <th>comments</th>\n",
       "      <th>rev</th>\n",
       "      <th>tenant</th>\n",
       "      <th>subject</th>\n",
       "      <th>hierarchy_path</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>record_type</th>\n",
       "      <th>cloned_id</th>\n",
       "      <th>record_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22062762</td>\n",
       "      <td>[USB] Running SSP Traffic on mSLE causes Bandw...</td>\n",
       "      <td>root_caused</td>\n",
       "      <td>awaiting_fix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-04-19 00:48:43</td>\n",
       "      <td>2017-05-25 10:09:10</td>\n",
       "      <td>When executing the Get Port Bandwidth command ...</td>\n",
       "      <td>&lt;p&gt;SIP bugeco:&amp;nbsp;&lt;a href=\"https://hsdes.int...</td>\n",
       "      <td>++++146210633 mghender\\nHi Sara, Is there any ...</td>\n",
       "      <td>9</td>\n",
       "      <td>sip</td>\n",
       "      <td>sighting</td>\n",
       "      <td>/101411699/1016006011/22062762/</td>\n",
       "      <td>1016006011</td>\n",
       "      <td>parent</td>\n",
       "      <td>22062762.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220152061</td>\n",
       "      <td>SPT-H E0: USB2 port missing</td>\n",
       "      <td>rejected</td>\n",
       "      <td>wont_do</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-05-12 00:47:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>sip</td>\n",
       "      <td>sighting</td>\n",
       "      <td>/101411699/1016006011/220152061/</td>\n",
       "      <td>1016006011</td>\n",
       "      <td>parent</td>\n",
       "      <td>220152061.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220421258</td>\n",
       "      <td>[Apple - Basin Falls/KBP-H] - xHCI Blocking S3...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>wont_do</td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>2017-07-06 23:33:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;pre style=\"word-wrap: break-word; white-space...</td>\n",
       "      <td>Failure cannot be reproduced by customer for a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>sip</td>\n",
       "      <td>sighting</td>\n",
       "      <td>/101411699/1016006011/220421258/</td>\n",
       "      <td>1016006011</td>\n",
       "      <td>parent</td>\n",
       "      <td>220421258.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220634430</td>\n",
       "      <td>[CNP_B0]:USB3 Loopback - xhci_debug_device tes...</td>\n",
       "      <td>rejected</td>\n",
       "      <td>cannot_reproduce</td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>2017-08-14 11:33:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p style=\"font-size: 12.18px;\"&gt;&lt;span style=\"fo...</td>\n",
       "      <td>&lt;p&gt;Rejecting this sighting as non-reproducible...</td>\n",
       "      <td>++++146564279 ppmeher\\nFull cutrand log attach...</td>\n",
       "      <td>5</td>\n",
       "      <td>sip</td>\n",
       "      <td>sighting</td>\n",
       "      <td>/101411699/1016006011/220634430/</td>\n",
       "      <td>1016006011</td>\n",
       "      <td>parent</td>\n",
       "      <td>220634430.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220634437</td>\n",
       "      <td>[ICL_A0_PO] Python SV: pci_config_registers ac...</td>\n",
       "      <td>root_caused</td>\n",
       "      <td>awaiting_review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>noise.non-issue</td>\n",
       "      <td>2017-08-14 11:36:26</td>\n",
       "      <td>2017-08-19 00:22:26</td>\n",
       "      <td>&lt;p&gt;Trying Recipe sent by Tamir&lt;/p&gt;&lt;p&gt;pci_confi...</td>\n",
       "      <td>&lt;p&gt;Failure is due to XHCI not coming up so the...</td>\n",
       "      <td>++++136216899 btamir\\nHi There is no informati...</td>\n",
       "      <td>11</td>\n",
       "      <td>sip</td>\n",
       "      <td>sighting</td>\n",
       "      <td>/101411699/1016006011/220634437/</td>\n",
       "      <td>1016006011</td>\n",
       "      <td>parent</td>\n",
       "      <td>220634437.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title       status  \\\n",
       "0   22062762  [USB] Running SSP Traffic on mSLE causes Bandw...  root_caused   \n",
       "1  220152061                        SPT-H E0: USB2 port missing     rejected   \n",
       "2  220421258  [Apple - Basin Falls/KBP-H] - xHCI Blocking S3...     rejected   \n",
       "3  220634430  [CNP_B0]:USB3 Loopback - xhci_debug_device tes...     rejected   \n",
       "4  220634437  [ICL_A0_PO] Python SV: pci_config_registers ac...  root_caused   \n",
       "\n",
       "             reason  merge_id            problem_area       submitted_date  \\\n",
       "0      awaiting_fix       NaN                     NaN  2017-04-19 00:48:43   \n",
       "1           wont_do       NaN                     NaN  2017-05-12 00:47:16   \n",
       "2           wont_do       NaN  noise.cannot_reproduce  2017-07-06 23:33:44   \n",
       "3  cannot_reproduce       NaN  noise.cannot_reproduce  2017-08-14 11:33:10   \n",
       "4   awaiting_review       NaN         noise.non-issue  2017-08-14 11:36:26   \n",
       "\n",
       "      root_caused_date                                        description  \\\n",
       "0  2017-05-25 10:09:10  When executing the Get Port Bandwidth command ...   \n",
       "1                  NaN  <pre style=\"word-wrap: break-word; white-space...   \n",
       "2                  NaN  <pre style=\"word-wrap: break-word; white-space...   \n",
       "3                  NaN  <p style=\"font-size: 12.18px;\"><span style=\"fo...   \n",
       "4  2017-08-19 00:22:26  <p>Trying Recipe sent by Tamir</p><p>pci_confi...   \n",
       "\n",
       "                                     fix_description  \\\n",
       "0  <p>SIP bugeco:&nbsp;<a href=\"https://hsdes.int...   \n",
       "1                                                NaN   \n",
       "2  Failure cannot be reproduced by customer for a...   \n",
       "3  <p>Rejecting this sighting as non-reproducible...   \n",
       "4  <p>Failure is due to XHCI not coming up so the...   \n",
       "\n",
       "                                            comments  rev tenant   subject  \\\n",
       "0  ++++146210633 mghender\\nHi Sara, Is there any ...    9    sip  sighting   \n",
       "1                                                NaN    4    sip  sighting   \n",
       "2                                                NaN    5    sip  sighting   \n",
       "3  ++++146564279 ppmeher\\nFull cutrand log attach...    5    sip  sighting   \n",
       "4  ++++136216899 btamir\\nHi There is no informati...   11    sip  sighting   \n",
       "\n",
       "                     hierarchy_path   parent_id record_type    cloned_id  \\\n",
       "0   /101411699/1016006011/22062762/  1016006011      parent   22062762.0   \n",
       "1  /101411699/1016006011/220152061/  1016006011      parent  220152061.0   \n",
       "2  /101411699/1016006011/220421258/  1016006011      parent  220421258.0   \n",
       "3  /101411699/1016006011/220634430/  1016006011      parent  220634430.0   \n",
       "4  /101411699/1016006011/220634437/  1016006011      parent  220634437.0   \n",
       "\n",
       "   record_index  \n",
       "0             1  \n",
       "1             2  \n",
       "2             3  \n",
       "3             4  \n",
       "4             5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#VICE dataset\n",
    "import pandas as pd\n",
    "path = \"C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/VICE/python_ir/\"\n",
    "df= pd.read_csv(path+\"sip_sighting_usb_duplicate_ai.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6cb6003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "noise.validation_tools                   249\n",
       "noise.duplicated_sighting                178\n",
       "bug.logic.xhci                           164\n",
       "noise.test_content                       126\n",
       "noise.non-issue                          112\n",
       "                                        ... \n",
       "environment.fpga.pciesw_gasket             1\n",
       "environment.board.crb                      1\n",
       "environment.cpu                            1\n",
       "bug.logic.usb3                             1\n",
       "collateral.documentation.visa_sigfile      1\n",
       "Name: problem_area, Length: 69, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"problem_area\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f66af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df before manipulation: (1978, 12)\n",
      "Shape of df after selecting columns: (1978, 2)\n",
      "Number of null values in df:\n",
      " title           0\n",
      "problem_area    0\n",
      "dtype: int64\n",
      "Number of null values in df after dropping NA rows:\n",
      " title           0\n",
      "problem_area    0\n",
      "dtype: int64\n",
      "Shape of df after dropping NA rows: (1978, 2)\n",
      "Number of duplicates in the df: 0\n",
      "Shape of df after manipulation: (1978, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>problem_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Apple - Basin Falls/KBP-H] - xHCI Blocking S3...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CNP_B0]:USB3 Loopback - xhci_debug_device tes...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ICL_A0_PO] Python SV: pci_config_registers ac...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ICL_A0_PO] Not able to get USB3 to train on TC3</td>\n",
       "      <td>environment.3rd_party.device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ICL_A0_PO] Python SV: Not able to read write ...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...</td>\n",
       "      <td>environment.rtl.fpga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>[SVOS] [BUSTER] [TGP-H] device enumeration failed</td>\n",
       "      <td>noise.validation_tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2     [Apple - Basin Falls/KBP-H] - xHCI Blocking S3...   \n",
       "3     [CNP_B0]:USB3 Loopback - xhci_debug_device tes...   \n",
       "4     [ICL_A0_PO] Python SV: pci_config_registers ac...   \n",
       "5      [ICL_A0_PO] Not able to get USB3 to train on TC3   \n",
       "6     [ICL_A0_PO] Python SV: Not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...   \n",
       "2071  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...   \n",
       "2072  [ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...   \n",
       "2076  [SVOS] [BUSTER] [TGP-H] device enumeration failed   \n",
       "2077  [ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...   \n",
       "\n",
       "                      problem_area  \n",
       "2           noise.cannot_reproduce  \n",
       "3           noise.cannot_reproduce  \n",
       "4                  noise.non-issue  \n",
       "5     environment.3rd_party.device  \n",
       "6                  noise.non-issue  \n",
       "...                            ...  \n",
       "2069        noise.cannot_reproduce  \n",
       "2071        noise.cannot_reproduce  \n",
       "2072          environment.rtl.fpga  \n",
       "2076        noise.validation_tools  \n",
       "2077                         other  \n",
       "\n",
       "[1978 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"problem_area\"],cols_todrop=None,impute_value=None,subset=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "357d70c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent words that are removed from column: {('fpga', 400), ('x', 330), ('xhci', 309), ('icl', 288), ('u', 397), ('device', 317), ('test', 419), ('lkf', 386), ('gen', 445), ('usb', 1329)}\n",
      "Rare words that are removed from columns: {('simultaneous', 1), ('writes', 1), ('strm', 1), ('rxpolarity', 1), ('pri', 1), ('ic', 1), ('modify', 1), ('locations', 1), ('tool', 1), ('buster', 1)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>problem_area</th>\n",
       "      <th>title_cont</th>\n",
       "      <th>title_lower</th>\n",
       "      <th>title_tag</th>\n",
       "      <th>title_rem</th>\n",
       "      <th>title_num</th>\n",
       "      <th>title_white</th>\n",
       "      <th>title_stop</th>\n",
       "      <th>title_freq</th>\n",
       "      <th>title_rare</th>\n",
       "      <th>title_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Apple - Basin Falls/KBP-H] - xHCI Blocking S3...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>[Apple - Basin Falls/KBP-H] - xHCI Blocking S3...</td>\n",
       "      <td>[apple - basin falls/kbp-h] - xhci blocking s3...</td>\n",
       "      <td>[apple - basin falls/kbp-h] - xhci blocking s3...</td>\n",
       "      <td>apple   basin falls kbp h    xhci blocking s3...</td>\n",
       "      <td>apple   basin falls kbp h    xhci blocking s ...</td>\n",
       "      <td>apple basin falls kbp h xhci blocking s resum...</td>\n",
       "      <td>apple basin falls kbp h xhci blocking   resum...</td>\n",
       "      <td>apple basin falls kbp h blocking resume warm r...</td>\n",
       "      <td>apple basin falls kbp h blocking resume warm r...</td>\n",
       "      <td>apple basin fall kbp h blocking resume warm re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[CNP_B0]:USB3 Loopback - xhci_debug_device tes...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>[CNP_B0]:USB3 Loopback - xhci_debug_device tes...</td>\n",
       "      <td>[cnp_b0]:usb3 loopback - xhci_debug_device tes...</td>\n",
       "      <td>[cnp_b0]:usb3 loopback - xhci_debug_device tes...</td>\n",
       "      <td>cnp b0  usb3 loopback   xhci debug device tes...</td>\n",
       "      <td>cnp b   usb  loopback   xhci debug device tes...</td>\n",
       "      <td>cnp b usb loopback xhci debug device test fai...</td>\n",
       "      <td>cnp b usb loopback xhci debug device test fai...</td>\n",
       "      <td>cnp b loopback debug fails missed event success</td>\n",
       "      <td>cnp b loopback debug fails missed event success</td>\n",
       "      <td>cnp b loopback debug fails missed event success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ICL_A0_PO] Python SV: pci_config_registers ac...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "      <td>[ICL_A0_PO] Python SV: pci_config_registers ac...</td>\n",
       "      <td>[icl_a0_po] python sv: pci_config_registers ac...</td>\n",
       "      <td>[icl_a0_po] python sv: pci_config_registers ac...</td>\n",
       "      <td>icl a0 po  python sv  pci config registers ac...</td>\n",
       "      <td>icl a  po  python sv  pci config registers ac...</td>\n",
       "      <td>icl a po python sv pci config registers acces...</td>\n",
       "      <td>icl   po python sv pci config registers acces...</td>\n",
       "      <td>po python sv pci config registers access failed</td>\n",
       "      <td>po python sv pci config registers access failed</td>\n",
       "      <td>po python sv pci config register access failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ICL_A0_PO] Not able to get USB3 to train on TC3</td>\n",
       "      <td>environment.3rd_party.device</td>\n",
       "      <td>[ICL_A0_PO] Not able to get USB3 to train on TC3</td>\n",
       "      <td>[icl_a0_po] not able to get usb3 to train on tc3</td>\n",
       "      <td>[icl_a0_po] not able to get usb3 to train on tc3</td>\n",
       "      <td>icl a0 po  not able to get usb3 to train on tc3</td>\n",
       "      <td>icl a  po  not able to get usb  to train on tc</td>\n",
       "      <td>icl a po not able to get usb to train on tc</td>\n",
       "      <td>icl   po   able   get usb   train   tc</td>\n",
       "      <td>po able get train tc</td>\n",
       "      <td>po able get train tc</td>\n",
       "      <td>po able get train tc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[ICL_A0_PO] Python SV: Not able to read write ...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "      <td>[ICL_A0_PO] Python SV: Not able to read write ...</td>\n",
       "      <td>[icl_a0_po] python sv: not able to read write ...</td>\n",
       "      <td>[icl_a0_po] python sv: not able to read write ...</td>\n",
       "      <td>icl a0 po  python sv  not able to read write ...</td>\n",
       "      <td>icl a  po  python sv  not able to read write ...</td>\n",
       "      <td>icl a po python sv not able to read write mgp...</td>\n",
       "      <td>icl   po python sv   able   read write mgphy ...</td>\n",
       "      <td>po python sv able read write mgphy registers p...</td>\n",
       "      <td>po python sv able read write mgphy registers p...</td>\n",
       "      <td>po python sv able read write mgphy register py...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...</td>\n",
       "      <td>[usb3.2][tgp-h][haps-80][e2e][ertl] gen1x2 u3 ...</td>\n",
       "      <td>[usb3.2][tgp-h][haps-80][e2e][ertl] gen1x2 u3 ...</td>\n",
       "      <td>usb3 2  tgp h  haps 80  e2e  ertl  gen1x2 u3 ...</td>\n",
       "      <td>usb     tgp h  haps    e e  ertl  gen x  u  t...</td>\n",
       "      <td>usb tgp h haps e e ertl gen x u test failing ...</td>\n",
       "      <td>usb tgp h haps e e ertl gen x u test failing ...</td>\n",
       "      <td>tgp h haps e e ertl failing seabright link drop</td>\n",
       "      <td>tgp h haps e e ertl failing seabright link drop</td>\n",
       "      <td>tgp h hap e e ertl failing seabright link drop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "      <td>[USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...</td>\n",
       "      <td>[usb3.2][tgp-h][haps-80][e2e][ertl] gen1 and g...</td>\n",
       "      <td>[usb3.2][tgp-h][haps-80][e2e][ertl] gen1 and g...</td>\n",
       "      <td>usb3 2  tgp h  haps 80  e2e  ertl  gen1 and g...</td>\n",
       "      <td>usb     tgp h  haps    e e  ertl  gen  and ge...</td>\n",
       "      <td>usb tgp h haps e e ertl gen and gen x rtd tes...</td>\n",
       "      <td>usb tgp h haps e e ertl gen   gen x rtd tests...</td>\n",
       "      <td>tgp h haps e e ertl rtd tests failing seabrigh...</td>\n",
       "      <td>tgp h haps e e ertl rtd tests failing seabrigh...</td>\n",
       "      <td>tgp h hap e e ertl rtd test failing seabright ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...</td>\n",
       "      <td>environment.rtl.fpga</td>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...</td>\n",
       "      <td>[adp-lp][fpga][rtl0p8] ti phy usb3 port 1 fail...</td>\n",
       "      <td>[adp-lp][fpga][rtl0p8] ti phy usb3 port 1 fail...</td>\n",
       "      <td>adp lp  fpga  rtl0p8  ti phy usb3 port 1 fail...</td>\n",
       "      <td>adp lp  fpga  rtl p   ti phy usb  port   fail...</td>\n",
       "      <td>adp lp fpga rtl p ti phy usb port fails enume...</td>\n",
       "      <td>adp lp fpga rtl p ti phy usb port fails enume...</td>\n",
       "      <td>adp lp rtl p ti phy port fails enumeration win...</td>\n",
       "      <td>adp lp rtl p ti phy port fails enumeration win...</td>\n",
       "      <td>adp lp rtl p ti phy port fails enumeration win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>[SVOS] [BUSTER] [TGP-H] device enumeration failed</td>\n",
       "      <td>noise.validation_tools</td>\n",
       "      <td>[SVOS] [BUSTER] [TGP-H] device enumeration failed</td>\n",
       "      <td>[svos] [buster] [tgp-h] device enumeration failed</td>\n",
       "      <td>[svos] [buster] [tgp-h] device enumeration failed</td>\n",
       "      <td>svos   buster   tgp h  device enumeration failed</td>\n",
       "      <td>svos   buster   tgp h  device enumeration failed</td>\n",
       "      <td>svos buster tgp h device enumeration failed</td>\n",
       "      <td>svos buster tgp h device enumeration failed</td>\n",
       "      <td>svos buster tgp h enumeration failed</td>\n",
       "      <td>svos tgp h enumeration failed</td>\n",
       "      <td>svos tgp h enumeration failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...</td>\n",
       "      <td>other</td>\n",
       "      <td>[ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...</td>\n",
       "      <td>[adp-lp][fpga][rtl0p8] usb2 dual port traffic ...</td>\n",
       "      <td>[adp-lp][fpga][rtl0p8] usb2 dual port traffic ...</td>\n",
       "      <td>adp lp  fpga  rtl0p8  usb2 dual port traffic ...</td>\n",
       "      <td>adp lp  fpga  rtl p   usb  dual port traffic ...</td>\n",
       "      <td>adp lp fpga rtl p usb dual port traffic hangs...</td>\n",
       "      <td>adp lp fpga rtl p usb dual port traffic hangs...</td>\n",
       "      <td>adp lp rtl p dual port traffic hangs port cont...</td>\n",
       "      <td>adp lp rtl p dual port traffic hangs port cont...</td>\n",
       "      <td>adp lp rtl p dual port traffic hang port contr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title  \\\n",
       "2     [Apple - Basin Falls/KBP-H] - xHCI Blocking S3...   \n",
       "3     [CNP_B0]:USB3 Loopback - xhci_debug_device tes...   \n",
       "4     [ICL_A0_PO] Python SV: pci_config_registers ac...   \n",
       "5      [ICL_A0_PO] Not able to get USB3 to train on TC3   \n",
       "6     [ICL_A0_PO] Python SV: Not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...   \n",
       "2071  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...   \n",
       "2072  [ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...   \n",
       "2076  [SVOS] [BUSTER] [TGP-H] device enumeration failed   \n",
       "2077  [ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...   \n",
       "\n",
       "                      problem_area  \\\n",
       "2           noise.cannot_reproduce   \n",
       "3           noise.cannot_reproduce   \n",
       "4                  noise.non-issue   \n",
       "5     environment.3rd_party.device   \n",
       "6                  noise.non-issue   \n",
       "...                            ...   \n",
       "2069        noise.cannot_reproduce   \n",
       "2071        noise.cannot_reproduce   \n",
       "2072          environment.rtl.fpga   \n",
       "2076        noise.validation_tools   \n",
       "2077                         other   \n",
       "\n",
       "                                             title_cont  \\\n",
       "2     [Apple - Basin Falls/KBP-H] - xHCI Blocking S3...   \n",
       "3     [CNP_B0]:USB3 Loopback - xhci_debug_device tes...   \n",
       "4     [ICL_A0_PO] Python SV: pci_config_registers ac...   \n",
       "5      [ICL_A0_PO] Not able to get USB3 to train on TC3   \n",
       "6     [ICL_A0_PO] Python SV: Not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1x2 u3 ...   \n",
       "2071  [USB3.2][TGP-H][HAPS-80][E2E][ERTL] Gen1 and G...   \n",
       "2072  [ADP-LP][FPGA][RTL0p8] TI Phy USB3 port 1 fail...   \n",
       "2076  [SVOS] [BUSTER] [TGP-H] device enumeration failed   \n",
       "2077  [ADP-LP][FPGA][RTL0p8] USB2 dual port traffic ...   \n",
       "\n",
       "                                            title_lower  \\\n",
       "2     [apple - basin falls/kbp-h] - xhci blocking s3...   \n",
       "3     [cnp_b0]:usb3 loopback - xhci_debug_device tes...   \n",
       "4     [icl_a0_po] python sv: pci_config_registers ac...   \n",
       "5      [icl_a0_po] not able to get usb3 to train on tc3   \n",
       "6     [icl_a0_po] python sv: not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069  [usb3.2][tgp-h][haps-80][e2e][ertl] gen1x2 u3 ...   \n",
       "2071  [usb3.2][tgp-h][haps-80][e2e][ertl] gen1 and g...   \n",
       "2072  [adp-lp][fpga][rtl0p8] ti phy usb3 port 1 fail...   \n",
       "2076  [svos] [buster] [tgp-h] device enumeration failed   \n",
       "2077  [adp-lp][fpga][rtl0p8] usb2 dual port traffic ...   \n",
       "\n",
       "                                              title_tag  \\\n",
       "2     [apple - basin falls/kbp-h] - xhci blocking s3...   \n",
       "3     [cnp_b0]:usb3 loopback - xhci_debug_device tes...   \n",
       "4     [icl_a0_po] python sv: pci_config_registers ac...   \n",
       "5      [icl_a0_po] not able to get usb3 to train on tc3   \n",
       "6     [icl_a0_po] python sv: not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069  [usb3.2][tgp-h][haps-80][e2e][ertl] gen1x2 u3 ...   \n",
       "2071  [usb3.2][tgp-h][haps-80][e2e][ertl] gen1 and g...   \n",
       "2072  [adp-lp][fpga][rtl0p8] ti phy usb3 port 1 fail...   \n",
       "2076  [svos] [buster] [tgp-h] device enumeration failed   \n",
       "2077  [adp-lp][fpga][rtl0p8] usb2 dual port traffic ...   \n",
       "\n",
       "                                              title_rem  \\\n",
       "2      apple   basin falls kbp h    xhci blocking s3...   \n",
       "3      cnp b0  usb3 loopback   xhci debug device tes...   \n",
       "4      icl a0 po  python sv  pci config registers ac...   \n",
       "5       icl a0 po  not able to get usb3 to train on tc3   \n",
       "6      icl a0 po  python sv  not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069   usb3 2  tgp h  haps 80  e2e  ertl  gen1x2 u3 ...   \n",
       "2071   usb3 2  tgp h  haps 80  e2e  ertl  gen1 and g...   \n",
       "2072   adp lp  fpga  rtl0p8  ti phy usb3 port 1 fail...   \n",
       "2076   svos   buster   tgp h  device enumeration failed   \n",
       "2077   adp lp  fpga  rtl0p8  usb2 dual port traffic ...   \n",
       "\n",
       "                                              title_num  \\\n",
       "2      apple   basin falls kbp h    xhci blocking s ...   \n",
       "3      cnp b   usb  loopback   xhci debug device tes...   \n",
       "4      icl a  po  python sv  pci config registers ac...   \n",
       "5       icl a  po  not able to get usb  to train on tc    \n",
       "6      icl a  po  python sv  not able to read write ...   \n",
       "...                                                 ...   \n",
       "2069   usb     tgp h  haps    e e  ertl  gen x  u  t...   \n",
       "2071   usb     tgp h  haps    e e  ertl  gen  and ge...   \n",
       "2072   adp lp  fpga  rtl p   ti phy usb  port   fail...   \n",
       "2076   svos   buster   tgp h  device enumeration failed   \n",
       "2077   adp lp  fpga  rtl p   usb  dual port traffic ...   \n",
       "\n",
       "                                            title_white  \\\n",
       "2      apple basin falls kbp h xhci blocking s resum...   \n",
       "3      cnp b usb loopback xhci debug device test fai...   \n",
       "4      icl a po python sv pci config registers acces...   \n",
       "5          icl a po not able to get usb to train on tc    \n",
       "6      icl a po python sv not able to read write mgp...   \n",
       "...                                                 ...   \n",
       "2069   usb tgp h haps e e ertl gen x u test failing ...   \n",
       "2071   usb tgp h haps e e ertl gen and gen x rtd tes...   \n",
       "2072   adp lp fpga rtl p ti phy usb port fails enume...   \n",
       "2076        svos buster tgp h device enumeration failed   \n",
       "2077   adp lp fpga rtl p usb dual port traffic hangs...   \n",
       "\n",
       "                                             title_stop  \\\n",
       "2      apple basin falls kbp h xhci blocking   resum...   \n",
       "3      cnp b usb loopback xhci debug device test fai...   \n",
       "4      icl   po python sv pci config registers acces...   \n",
       "5               icl   po   able   get usb   train   tc    \n",
       "6      icl   po python sv   able   read write mgphy ...   \n",
       "...                                                 ...   \n",
       "2069   usb tgp h haps e e ertl gen x u test failing ...   \n",
       "2071   usb tgp h haps e e ertl gen   gen x rtd tests...   \n",
       "2072   adp lp fpga rtl p ti phy usb port fails enume...   \n",
       "2076        svos buster tgp h device enumeration failed   \n",
       "2077   adp lp fpga rtl p usb dual port traffic hangs...   \n",
       "\n",
       "                                             title_freq  \\\n",
       "2     apple basin falls kbp h blocking resume warm r...   \n",
       "3       cnp b loopback debug fails missed event success   \n",
       "4       po python sv pci config registers access failed   \n",
       "5                                  po able get train tc   \n",
       "6     po python sv able read write mgphy registers p...   \n",
       "...                                                 ...   \n",
       "2069    tgp h haps e e ertl failing seabright link drop   \n",
       "2071  tgp h haps e e ertl rtd tests failing seabrigh...   \n",
       "2072  adp lp rtl p ti phy port fails enumeration win...   \n",
       "2076               svos buster tgp h enumeration failed   \n",
       "2077  adp lp rtl p dual port traffic hangs port cont...   \n",
       "\n",
       "                                             title_rare  \\\n",
       "2     apple basin falls kbp h blocking resume warm r...   \n",
       "3       cnp b loopback debug fails missed event success   \n",
       "4       po python sv pci config registers access failed   \n",
       "5                                  po able get train tc   \n",
       "6     po python sv able read write mgphy registers p...   \n",
       "...                                                 ...   \n",
       "2069    tgp h haps e e ertl failing seabright link drop   \n",
       "2071  tgp h haps e e ertl rtd tests failing seabrigh...   \n",
       "2072  adp lp rtl p ti phy port fails enumeration win...   \n",
       "2076                      svos tgp h enumeration failed   \n",
       "2077  adp lp rtl p dual port traffic hangs port cont...   \n",
       "\n",
       "                                            title_clean  \n",
       "2     apple basin fall kbp h blocking resume warm re...  \n",
       "3       cnp b loopback debug fails missed event success  \n",
       "4        po python sv pci config register access failed  \n",
       "5                                  po able get train tc  \n",
       "6     po python sv able read write mgphy register py...  \n",
       "...                                                 ...  \n",
       "2069     tgp h hap e e ertl failing seabright link drop  \n",
       "2071  tgp h hap e e ertl rtd test failing seabright ...  \n",
       "2072  adp lp rtl p ti phy port fails enumeration win...  \n",
       "2076                      svos tgp h enumeration failed  \n",
       "2077  adp lp rtl p dual port traffic hang port contr...  \n",
       "\n",
       "[1978 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data preprocessing\n",
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=None,remove_sw=None) for text in df[\"title_white\"]]\n",
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_freq\"],n)\n",
    "df[\"title_clean\"] = lemmatize_words(column= df[\"title_rare\"],lemma_type=\"WordNet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "370679f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_clean</th>\n",
       "      <th>problem_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple basin fall kbp h blocking resume warm re...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cnp b loopback debug fails missed event success</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>po python sv pci config register access failed</td>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>po able get train tc</td>\n",
       "      <td>environment.3rd_party.device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>po python sv able read write mgphy register py...</td>\n",
       "      <td>noise.non-issue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>tgp h hap e e ertl failing seabright link drop</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>tgp h hap e e ertl rtd test failing seabright ...</td>\n",
       "      <td>noise.cannot_reproduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>adp lp rtl p ti phy port fails enumeration win...</td>\n",
       "      <td>environment.rtl.fpga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>svos tgp h enumeration failed</td>\n",
       "      <td>noise.validation_tools</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>adp lp rtl p dual port traffic hang port contr...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            title_clean  \\\n",
       "2     apple basin fall kbp h blocking resume warm re...   \n",
       "3       cnp b loopback debug fails missed event success   \n",
       "4        po python sv pci config register access failed   \n",
       "5                                  po able get train tc   \n",
       "6     po python sv able read write mgphy register py...   \n",
       "...                                                 ...   \n",
       "2069     tgp h hap e e ertl failing seabright link drop   \n",
       "2071  tgp h hap e e ertl rtd test failing seabright ...   \n",
       "2072  adp lp rtl p ti phy port fails enumeration win...   \n",
       "2076                      svos tgp h enumeration failed   \n",
       "2077  adp lp rtl p dual port traffic hang port contr...   \n",
       "\n",
       "                      problem_area  \n",
       "2           noise.cannot_reproduce  \n",
       "3           noise.cannot_reproduce  \n",
       "4                  noise.non-issue  \n",
       "5     environment.3rd_party.device  \n",
       "6                  noise.non-issue  \n",
       "...                            ...  \n",
       "2069        noise.cannot_reproduce  \n",
       "2071        noise.cannot_reproduce  \n",
       "2072          environment.rtl.fpga  \n",
       "2076        noise.validation_tools  \n",
       "2077                         other  \n",
       "\n",
       "[1978 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"title_clean\",\"problem_area\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non negative matrix factorization\n",
    "# df3 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b06ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.decomposition import NMF\n",
    "\n",
    "# def nmf(vectorized,vec_type,n_components,top_n_terms):\n",
    "#     \"\"\"\n",
    "#     Non-negative matrix factorization for unsupervised learning.\n",
    "#     params:\n",
    "#     vectorized: vectorized obtained from feature extraction function\n",
    "#     vec_type: vec_type obtained from from feature extraction function\n",
    "#     n_components[int]: the number of topics/clusters used in NMF\n",
    "#     top_n_terms[int]: the top n terms in each topic/cluster to be printed out\n",
    "#     \"\"\"\n",
    "#     # Create object for the NMF class \n",
    "#     nmf_model = NMF(n_components,random_state=42)\n",
    "#     nmf_model.fit(vectorized)\n",
    "    \n",
    "#     # Components_ gives us our topic distribution \n",
    "#     topic_words = nmf_model.components_\n",
    "\n",
    "#     # Top n words for a topic\n",
    "\n",
    "#     for i,topic in enumerate(topic_words):\n",
    "#         print(f\"The top {top_n_terms} words for topic #{i}\")\n",
    "#         print([vec_type.get_feature_names()[index] for index in topic.argsort()[-top_n_terms:]])\n",
    "#         print(\"\\n\")\n",
    "        \n",
    "#     topic_results = nmf_model.transform(vectorized) \n",
    "    \n",
    "#     return topic_results.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #user provides number of component and top n terms in each cluster/topic\n",
    "# #feature extraction\n",
    "# column = df3[\"title_lemma_word\"]\n",
    "# ngram_range = (1,1)\n",
    "# ascending = False\n",
    "# fe_type = \"bagofwords\"\n",
    "# vec_type = feature_extraction(column,ngram_range,ascending,fe_type)[1]\n",
    "# vectorized = feature_extraction(column,ngram_range,ascending,fe_type)[2]\n",
    "\n",
    "# #NMF\n",
    "# df3[\"topic\"] = nmf(vectorized,vec_type,n_components=17,top_n_terms=10)\n",
    "# df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eea3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert and save train/validation data as .spacy\n",
    "# out_path = \"C:/Users/nchong/\"\n",
    "# db_train = convert_spacy(TRAIN_DATA)\n",
    "# db_train.to_disk(out_path +'train.spacy') # save the docbin object\n",
    "# db_val = convert_spacy(VAL_DATA)\n",
    "# db_val.to_disk(out_path +'val.spacy') # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89862c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model\n",
    "# nlp1 = spacy.load(\"C:/Users/nchong/output/model-best/\") #load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43084cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp1(\"waikitcx hi arisha please provide us the\") # input sample text\n",
    "\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_ents(text):\n",
    "#     doc= nlp1(text)\n",
    "#     if doc.ents:\n",
    "#         for ent in doc.ents:\n",
    "#             return(ent.text+' - '+ent.label_)\n",
    "#     else:\n",
    "#         return('No named entities found.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
