{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8889562e",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0951373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import pandas as pd\n",
    "import glob, os, json\n",
    "import re\n",
    "\n",
    "#user input file path\n",
    "path = 'C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/Sample json output/HSD ES Raw Data/team1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497319eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(path,df=None,date=None):\n",
    "    '''\n",
    "    Load only files that follow agreed filename format, merge files as single dataframe.\n",
    "    Can support incremental aggregation of dataset, by setting arg df as the existing dataframe\n",
    "    Returns a single dataframe.\n",
    "    \n",
    "    params:\n",
    "    path [string]: path of the files, without filename\n",
    "    df [dataframe] (optional,default is None): input existing dataframe to merge with new files\n",
    "    date [\"string\"](optional,default is None): user can choose to load only files from specific date in YYYY-MM-DD format\n",
    "    '''\n",
    "    filenames = os.listdir(path)\n",
    "    file_list=[]\n",
    "    dfs = []\n",
    "\n",
    "    if df is None: #no existing dataframe\n",
    "        \n",
    "        for file in filenames:\n",
    "            # search agreed file format pattern in the filename\n",
    "            if date == None:\n",
    "                pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "                \n",
    "            else:\n",
    "#              \n",
    "                pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "    \n",
    "            match = re.search(pattern,file)\n",
    "            #if match is found\n",
    "            if match:\n",
    "                pattern = os.path.join(path, file) #join path with file name\n",
    "                file_list.append(pattern) #list of json files that follow the agreed filename\n",
    "\n",
    "                for file in file_list:\n",
    "                    with open(file) as f:\n",
    "                        #flatten json into pd dataframe\n",
    "                        json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                        #label which file each row is from \n",
    "                        json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                    dfs.append(json_data)\n",
    "                df = pd.concat(dfs)\n",
    "                \n",
    "    else: #existing dataframe exists and want to append new files to existing dataframe\n",
    "             \n",
    "        for file in filenames:\n",
    "\n",
    "            if file not in df[\"file\"].unique(): #check if file is new - to support merging of new dataset with previously read ones\n",
    "\n",
    "                # search agreed file format pattern in the filename\n",
    "                \n",
    "                if date == None:\n",
    "                    pattern = r\"^\\(\\d{4}-\\d{2}-\\d{1,2}\\)\\d+\\_\\D+\\_\\d+\\.json$\"\n",
    "\n",
    "                else:\n",
    "                    pattern = r\"\\(\"+date+r\"\\)\\d+\\_\\D+\\_\\d+\\.json\"\n",
    "                     \n",
    "                match = re.search(pattern,file)\n",
    "\n",
    "                #if match is found\n",
    "                if match:\n",
    "                    json_pattern = os.path.join(path, file) #join path with file name\n",
    "                    file_list.append(json_pattern) #list of json files \n",
    "\n",
    "                    for file in file_list:\n",
    "                        with open(file) as f:\n",
    "                            #flatten json into pd dataframe\n",
    "                            json_data = pd.json_normalize(json.loads(f.read()))\n",
    "                            #label which file each row is from \n",
    "                            json_data['file'] = file.rsplit(\"/\", 1)[-1]\n",
    "\n",
    "                        dfs.append(json_data)\n",
    "                    new_df = pd.concat(dfs)           \n",
    "                    df=pd.concat([df,new_df])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d9aa175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(2021-08-25)1_firstSet_1.json',\n",
       " '(2021-08-25)3_secondSet_1.json',\n",
       " '(2021-10-11)3_secondSet_1.json',\n",
       " 'data_3cols.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c24924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>comments</th>\n",
       "      <th>updated_date</th>\n",
       "      <th>hierarchy_id</th>\n",
       "      <th>rev</th>\n",
       "      <th>tenant</th>\n",
       "      <th>subject</th>\n",
       "      <th>is_current</th>\n",
       "      <th>hierarchy_path</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>record_type</th>\n",
       "      <th>row_num</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1308651592</td>\n",
       "      <td>provide method to update GIO fields from git r...</td>\n",
       "      <td>Please provide a way to update GIO fields from...</td>\n",
       "      <td>++++1562123662 fbakhda\\nHi @Panceac, Cornel Eu...</td>\n",
       "      <td>2021-07-21 12:30:31.387</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308651592/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>1</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1308671310</td>\n",
       "      <td>Test suite execution terminates before executi...</td>\n",
       "      <td>&lt;p&gt;Test suite execution finished before execut...</td>\n",
       "      <td>++++1361513318 cmoala\\nsys_tsdval@GL-IAF1-V-S0...</td>\n",
       "      <td>2021-05-04 09:30:00.320</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308671310/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>2</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1308673361</td>\n",
       "      <td>Cloning defects from another test cycle is not...</td>\n",
       "      <td>&lt;p&gt;I am trying to clone defects from another t...</td>\n",
       "      <td>++++1361514315 cmoala\\nObserved that only impl...</td>\n",
       "      <td>2021-05-20 11:47:18.927</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1308673361/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>3</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507656633</td>\n",
       "      <td>[Testing Only] this is enhancement only</td>\n",
       "      <td>Retest some function again.</td>\n",
       "      <td></td>\n",
       "      <td>2020-03-13 10:16:18.703</td>\n",
       "      <td></td>\n",
       "      <td>31</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1507656633/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>4</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507656638</td>\n",
       "      <td>[Testing Only] this is consultation only</td>\n",
       "      <td>enter the support needed at here ...</td>\n",
       "      <td>++++1661488832 prajput\\nHSDES testing. Please ...</td>\n",
       "      <td>2020-06-01 09:49:55.913</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/1507656638/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>5</td>\n",
       "      <td>(2021-08-25)1_firstSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>22012641037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;div&gt;&lt;span style=\"font-size: 12.18px;\"&gt;Hello,&amp;...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-26 13:19:20.430</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012641037/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>900</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>22012645565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-05-20 13:03:09.327</td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012645565/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>901</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>22012704243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;div&gt;The schedule test suite allow for the use...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-04-26 10:04:12.410</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012704243/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>902</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>22012765885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Hi Gio Team,&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Thank you f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-06-30 00:35:58.927</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22012765885/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>903</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>22013190829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Converting to enhancement...&lt;/p&gt;&lt;p&gt;&lt;br /&gt;&lt;/...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-13 15:41:16.507</td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>iot_platf</td>\n",
       "      <td>support</td>\n",
       "      <td>1</td>\n",
       "      <td>/1201559436/1208431055/22013190829/</td>\n",
       "      <td>1208431055</td>\n",
       "      <td>parent</td>\n",
       "      <td>904</td>\n",
       "      <td>(2021-10-11)3_secondSet_1.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5424 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                              title  \\\n",
       "0     1308651592  provide method to update GIO fields from git r...   \n",
       "1     1308671310  Test suite execution terminates before executi...   \n",
       "2     1308673361  Cloning defects from another test cycle is not...   \n",
       "3     1507656633            [Testing Only] this is enhancement only   \n",
       "4     1507656638           [Testing Only] this is consultation only   \n",
       "..           ...                                                ...   \n",
       "899  22012641037                                                NaN   \n",
       "900  22012645565                                                NaN   \n",
       "901  22012704243                                                NaN   \n",
       "902  22012765885                                                NaN   \n",
       "903  22013190829                                                NaN   \n",
       "\n",
       "                                           description  \\\n",
       "0    Please provide a way to update GIO fields from...   \n",
       "1    <p>Test suite execution finished before execut...   \n",
       "2    <p>I am trying to clone defects from another t...   \n",
       "3                          Retest some function again.   \n",
       "4                 enter the support needed at here ...   \n",
       "..                                                 ...   \n",
       "899  <div><span style=\"font-size: 12.18px;\">Hello,&...   \n",
       "900  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "901  <div>The schedule test suite allow for the use...   \n",
       "902  <p>Hi Gio Team,</p><p><br /></p><p>Thank you f...   \n",
       "903  <p>Converting to enhancement...</p><p><br /></...   \n",
       "\n",
       "                                              comments  \\\n",
       "0    ++++1562123662 fbakhda\\nHi @Panceac, Cornel Eu...   \n",
       "1    ++++1361513318 cmoala\\nsys_tsdval@GL-IAF1-V-S0...   \n",
       "2    ++++1361514315 cmoala\\nObserved that only impl...   \n",
       "3                                                        \n",
       "4    ++++1661488832 prajput\\nHSDES testing. Please ...   \n",
       "..                                                 ...   \n",
       "899                                                NaN   \n",
       "900                                                NaN   \n",
       "901                                                NaN   \n",
       "902                                                NaN   \n",
       "903                                                NaN   \n",
       "\n",
       "                updated_date hierarchy_id rev     tenant  subject is_current  \\\n",
       "0    2021-07-21 12:30:31.387                8  iot_platf  support          1   \n",
       "1    2021-05-04 09:30:00.320               11  iot_platf  support          1   \n",
       "2    2021-05-20 11:47:18.927                9  iot_platf  support          1   \n",
       "3    2020-03-13 10:16:18.703               31  iot_platf  support          1   \n",
       "4    2020-06-01 09:49:55.913               19  iot_platf  support          1   \n",
       "..                       ...          ...  ..        ...      ...        ...   \n",
       "899  2021-03-26 13:19:20.430               11  iot_platf  support          1   \n",
       "900  2021-05-20 13:03:09.327               11  iot_platf  support          1   \n",
       "901  2021-04-26 10:04:12.410                9  iot_platf  support          1   \n",
       "902  2021-06-30 00:35:58.927               14  iot_platf  support          1   \n",
       "903  2021-07-13 15:41:16.507               16  iot_platf  support          1   \n",
       "\n",
       "                          hierarchy_path   parent_id record_type row_num  \\\n",
       "0     /1201559436/1208431055/1308651592/  1208431055      parent       1   \n",
       "1     /1201559436/1208431055/1308671310/  1208431055      parent       2   \n",
       "2     /1201559436/1208431055/1308673361/  1208431055      parent       3   \n",
       "3     /1201559436/1208431055/1507656633/  1208431055      parent       4   \n",
       "4     /1201559436/1208431055/1507656638/  1208431055      parent       5   \n",
       "..                                   ...         ...         ...     ...   \n",
       "899  /1201559436/1208431055/22012641037/  1208431055      parent     900   \n",
       "900  /1201559436/1208431055/22012645565/  1208431055      parent     901   \n",
       "901  /1201559436/1208431055/22012704243/  1208431055      parent     902   \n",
       "902  /1201559436/1208431055/22012765885/  1208431055      parent     903   \n",
       "903  /1201559436/1208431055/22013190829/  1208431055      parent     904   \n",
       "\n",
       "                               file  \n",
       "0     (2021-08-25)1_firstSet_1.json  \n",
       "1     (2021-08-25)1_firstSet_1.json  \n",
       "2     (2021-08-25)1_firstSet_1.json  \n",
       "3     (2021-08-25)1_firstSet_1.json  \n",
       "4     (2021-08-25)1_firstSet_1.json  \n",
       "..                              ...  \n",
       "899  (2021-10-11)3_secondSet_1.json  \n",
       "900  (2021-10-11)3_secondSet_1.json  \n",
       "901  (2021-10-11)3_secondSet_1.json  \n",
       "902  (2021-10-11)3_secondSet_1.json  \n",
       "903  (2021-10-11)3_secondSet_1.json  \n",
       "\n",
       "[5424 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load only files that follow the agreed format, does not choose file by date\n",
    "df = data_loading(path,df=None,date = None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16661452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load only files that follow the agreed format, choose file by date\n",
    "# date= \"2021-08-25\"\n",
    "# df = data_loading(path,df=None,date = date)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c03d8",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "### a) Dataframe manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baadc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_manipulation(df,how,keep,cols_tokeep=None,cols_todrop=None,impute_value=None,subset=None):\n",
    "    \"\"\"\n",
    "    1) Column selection: Keep or drop columns in dataframe\n",
    "    2) Data impute: Impute or drop NA rows \n",
    "    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    cols_tokeep [list/None]: list of columns to keep, if there is no list use None\n",
    "    cols_todrop [list/None]: list of columns to drop, if there is no list use None\n",
    "    impute_value [string/None]: value to be imputed (i.e \"\" for empty string). If no value to be imputed but there are \n",
    "                        rows to be dropped use None\n",
    "    how[string]: Drop rows when we have at least one NA or all NA. Choose\n",
    "                      # - \"all\": Drop row with all NA\n",
    "                      # - \"any\": Drop row with at least one NA\n",
    "                  \n",
    "    subset[list/None]: Subset of columns for dropping NA and identifying duplicates, use None if no column to select\n",
    "    keep[string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence\n",
    "                        # - \"first\" : Drop duplicates except for the first occurrence. \n",
    "                        # - \"last\" : Drop duplicates except for the last occurrence. \n",
    "                        # - False : Drop all duplicates.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Shape of df before manipulation:\",df.shape)\n",
    "\n",
    "    #Column selection - user can select columns or drop unwanted columns\n",
    "    if cols_tokeep != None:\n",
    "        df = df[cols_tokeep]\n",
    "    if cols_todrop != None:\n",
    "        df = df.drop(cols_todrop,axis=1)\n",
    "    print(\"Shape of df after selecting columns:\",df.shape)\n",
    "\n",
    "    #---Data impute - user can impute or drop rows with NA,freq of null values before & after manipulation returned---#\n",
    "    print(\"Number of null values in df:\\n\",df.isnull().sum())\n",
    "  \n",
    "\n",
    "    # impute NA values with user's choice of imputation value\n",
    "    if impute_value != None:\n",
    "        df = df.fillna(impute_value)\n",
    "        print(\"Number of null values in df after NA imputation:\\n\",df.isnull().sum())\n",
    "        \n",
    "    else: # drop rows with NA values\n",
    "        df= df.dropna(axis=0, how=how,subset=subset)\n",
    "        print(\"Number of null values in df after dropping NA rows:\\n\",df.isnull().sum())\n",
    "        print(\"Shape of df after dropping NA rows:\",df.shape)\n",
    "\n",
    "    #---------Data duplication cleaning--------#\n",
    "    print(\"Number of duplicates in the df:\", df.duplicated().sum())\n",
    "\n",
    "    #drop duplicates\n",
    "    df = df.drop_duplicates(subset=subset, keep=keep)\n",
    "\n",
    "    print(\"Shape of df after manipulation:\",df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8931eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df before manipulation: (5424, 15)\n",
      "Shape of df after selecting columns: (5424, 1)\n",
      "Number of null values in df:\n",
      " title    2712\n",
      "dtype: int64\n",
      "Number of null values in df after dropping NA rows:\n",
      " title    0\n",
      "dtype: int64\n",
      "Shape of df after dropping NA rows: (2712, 1)\n",
      "Number of duplicates in the df: 1815\n",
      "Shape of df after manipulation: (897, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>provide method to update GIO fields from git r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test suite execution terminates before executi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cloning defects from another test cycle is not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Testing Only] this is enhancement only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Testing Only] this is consultation only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Import GC Time Global Domain Artifact in GIO f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>[KPI_Metric] Phase-2: Extract kpi metric trend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Ability to clone Schedule Test Suites from oth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>[KPI_Metric] Enhance KPI feature to plot graph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>Is there a way to pull all test cases for a pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>897 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title\n",
       "0    provide method to update GIO fields from git r...\n",
       "1    Test suite execution terminates before executi...\n",
       "2    Cloning defects from another test cycle is not...\n",
       "3              [Testing Only] this is enhancement only\n",
       "4             [Testing Only] this is consultation only\n",
       "..                                                 ...\n",
       "899  Import GC Time Global Domain Artifact in GIO f...\n",
       "900  [KPI_Metric] Phase-2: Extract kpi metric trend...\n",
       "901  Ability to clone Schedule Test Suites from oth...\n",
       "902  [KPI_Metric] Enhance KPI feature to plot graph...\n",
       "903  Is there a way to pull all test cases for a pr...\n",
       "\n",
       "[897 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "df = df_manipulation(df,how=\"any\",keep=\"first\",cols_tokeep=[\"title\"],cols_todrop=None,impute_value=None,subset=[\"title\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2089e",
   "metadata": {},
   "source": [
    "\n",
    "### b) Text Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1540a0a",
   "metadata": {},
   "source": [
    "### 2) Expand contractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453da296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "def word_contractions(text):\n",
    "    \"\"\"\n",
    "    Expand word contractions (i.e. \"isn't\" to \"is not\")\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return \" \".join([contractions.fix(word) for word in text.split()])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff07071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_cont\"] = [word_contractions(text) for text in df[\"title\"]]\n",
    "df[\"desc_cont\"]=  [word_contractions(text) for text in df[\"description\"]]\n",
    "df[\"comments_cont\"]=  [word_contractions(text) for text in df[\"comments\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b563c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47492958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[149,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e49df",
   "metadata": {},
   "source": [
    "### 3) Convert all characters into lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01121fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    \"\"\"\n",
    "    Convert all characters to lower case\n",
    "    param:\n",
    "    text[string]: input string \n",
    "    \"\"\"\n",
    "    return text.lower() if type(text) == str else text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_lower\"] = [lowercase(text) for text in df[\"title_cont\"]]\n",
    "df[\"desc_lower\"]= [lowercase(text) for text in df[\"desc_cont\"]]\n",
    "df[\"comments_lower\"]= [lowercase(text) for text in df[\"comments_cont\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be73186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_lower\",\"desc_lower\",\"comments_lower\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699a61d4",
   "metadata": {},
   "source": [
    "### 4) Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3ba0c",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "def stem_words(text,stemmer_type):\n",
    "    \"\"\"\n",
    "    Stemming words, 2 options available: Porter Stemmer or Lancaster Stemmer \n",
    "    params:\n",
    "    text[string]: input string \n",
    "    stemmer_type[string]: input stemming method (\"Porter\" or \"Lancaster\")\n",
    "    \"\"\"\n",
    "    if stemmer_type == \"Porter\":\n",
    "        stemmer = PorterStemmer()\n",
    "    if stemmer_type == \"Lancaster\":\n",
    "        stemmer=LancasterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f0a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"title_lower\"]]\n",
    "df1[\"desc_stem_lan\"] = [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"desc_lower\"]]\n",
    "df1[\"comments_stem_lan\"]= [stem_words(text,stemmer_type = \"Lancaster\") for text in df1[\"comments_lower\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"title_stem_por\"] = [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"title_lower\"]]\n",
    "df1[\"desc_stem_por\"] = [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"desc_lower\"]]\n",
    "df1[\"comments_stem_por\"]= [stem_words(text,stemmer_type = \"Porter\") for text in df1[\"comments_lower\"]]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48f7de",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ba3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11146a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(column,lemma_type):\n",
    "    \"\"\"\n",
    "    Lemmatize words, 2 options available: WordNetLemmatizer or Spacy \n",
    "    params:\n",
    "    column[series]: input series/column to be lemmatized\n",
    "    lemma_type[string]: input lemmatization method (\"WordNet\" or \"Spacy\")\n",
    "    \"\"\"\n",
    "    if lemma_type == \"WordNet\":\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return column.apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    \n",
    "    \n",
    "    if lemma_type == \"Spacy\":\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        column = column.apply(lambda text: \" \".join([w.lemma_ for w in nlp(text)]))\n",
    "        #convert to lower case as spacy will convert pronouns to upper case\n",
    "        column = column.apply(lambda text: text.lower() if type(text) == str else text )\n",
    "        \n",
    "        return column\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7092bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"title_lemma_spacy\"] = lemmatize_words(column= df2[\"title_rem\"],lemma_type=\"Spacy\")\n",
    "df2[\"desc_lemma_spacy\"] = lemmatize_words(column= df2[\"desc_rem\"],lemma_type=\"Spacy\")\n",
    "df2[\"comments_lemma_spacy\"] = lemmatize_words(column= df2[\"comments_rem\"],lemma_type=\"Spacy\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18243072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"title_lemma_word\"] = lemmatize_words(column= df2[\"title_rem\"],lemma_type=\"WordNet\")\n",
    "df2[\"desc_lemma_word\"] = lemmatize_words(column= df2[\"desc_rem\"],lemma_type=\"WordNet\")\n",
    "df2[\"comments_lemma_word\"] = lemmatize_words(column= df2[\"comments_rem\"],lemma_type=\"WordNet\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d27c8",
   "metadata": {},
   "source": [
    "### b) Noise filtering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661e07",
   "metadata": {},
   "source": [
    "### 1) Remove html tag and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027213da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "def remove_htmltag_url(text):\n",
    "    \"\"\"\n",
    "    Remove html tag and url\n",
    "    params:\n",
    "    text [string]: input string\n",
    "    \n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    pd.options.mode.chained_assignment = None \n",
    "    #remove html tag\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text(separator= \" \",strip=True) \n",
    "    #remove url\n",
    "    text_clean = re.sub('https?[://%]*\\S+', ' ',text) \n",
    "    return text_clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f23e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_tag\"] = [remove_htmltag_url(text) for text in df[\"title_lower\"]]\n",
    "df[\"desc_tag\"]= [remove_htmltag_url(text) for text in df[\"desc_lower\"]]\n",
    "df[\"comments_tag\"]= [remove_htmltag_url(text) for text in df[\"comments_lower\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273bee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2f91a",
   "metadata": {},
   "source": [
    "### 3) Remove irrelevant characters, punctuation, special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02440a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_tag\",\"desc_tag\",\"comments_tag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_irrchar_punc(text,char=None):\n",
    "    \"\"\"\n",
    "    Remove irrelevant characters and punctuation\n",
    "    params:\n",
    "    \n",
    "    text[string]: input string \n",
    "    characters[string]: input regex of characters to be removed\n",
    "    \"\"\"\n",
    "    if char != None:\n",
    "        #Remove special characters given by user\n",
    "        text = re.sub(char, ' ',text) \n",
    "    \n",
    "    # Remove utf-8 literals (i.e. \\\\xe2\\\\x80\\\\x8)\n",
    "    text = re.sub(r'\\\\+x[\\d\\D][\\d\\D]', ' ',text) \n",
    "    \n",
    "    #Remove special characters and punctuation\n",
    "    text = re.sub('[^\\w\\s]', ' ',text) \n",
    "    text = re.sub(r'_', ' ',text) \n",
    "#     df = df.replace('[^\\w\\s]',' ', regex=True)\n",
    "#     df = df.replace(r\"_\", \" \", regex=True)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('C:/Users/nchong/OneDrive - Intel Corporation/Documents/Debug Similarity Analytics and Bucketization Framework/General/'+'data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a76f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#char=None\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=None) for text in df[\"title_tag\"]]\n",
    "df[\"desc_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"desc_tag\"]]\n",
    "df[\"comments_rem\"]= [remove_irrchar_punc(text,char=None) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d676006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1] #desc before rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27350f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4] #desc rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2235b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#special character removal added by user\n",
    "char = '\\++\\d+'\n",
    "df[\"title_rem\"] = [remove_irrchar_punc(text,char=char) for text in df[\"title_tag\"]]\n",
    "df[\"desc_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"desc_tag\"]]\n",
    "df[\"comments_rem\"]= [remove_irrchar_punc(text,char=char) for text in df[\"comments_tag\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec85f0",
   "metadata": {},
   "source": [
    "### 3) Remove numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f8e33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_rem\",\"desc_rem\",\"comments_rem\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    \"\"\"\n",
    "    Remove numeric data\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub('\\d+', ' ',text) \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56627dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_num\"] = [remove_num(text) for text in df[\"title_rem\"]]\n",
    "df[\"desc_num\"]= [remove_num(text) for text in df[\"desc_rem\"]]\n",
    "df[\"comments_num\"]= [remove_num(text) for text in df[\"comments_rem\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f0d60",
   "metadata": {},
   "source": [
    "### 4) Remove multiple whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_num\",\"desc_num\",\"comments_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multwhitespace(text):\n",
    "    \"\"\"\n",
    "    Remove multiple white spaces\n",
    "    params:\n",
    "    text[string]: input string \n",
    "    \n",
    "    \"\"\"\n",
    "    text = re.sub(' +', ' ',text) \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"title_white\"] = [remove_multwhitespace(text) for text in df[\"title_num\"]]\n",
    "df[\"desc_white\"]= [remove_multwhitespace(text) for text in df[\"desc_num\"]]\n",
    "df[\"comments_white\"]= [remove_multwhitespace(text) for text in df[\"comments_num\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb117fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97e5d5",
   "metadata": {},
   "source": [
    "### 4) Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ca58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e13a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_white\",\"desc_white\",\"comments_white\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text,extra_sw=None,remove_sw=None):\n",
    "    \"\"\"\n",
    "    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  \n",
    "    params:\n",
    "    text[string]: input string\n",
    "    extra_sw [list] (optional): list of words/phrase to be added to the stop words \n",
    "    remove_sw [list] (optional): list of words to be removed from the stop words \n",
    "    \"\"\"\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    \n",
    "    #default list of stopwords\n",
    "    if extra_sw == None and remove_sw==None:\n",
    "        all_stopwords = all_stopwords\n",
    "        \n",
    "    # add more stopwords\n",
    "    elif remove_sw == None:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        \n",
    "    # remove stopwords from existing sw list\n",
    "    elif extra_sw == None:\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "        \n",
    "    # remove and add stopwords to existing sw list\n",
    "    else:\n",
    "        all_stopwords.extend(extra_sw) #add to existing stop words list\n",
    "        all_stopwords = [e for e in all_stopwords if e not in remove_sw] #remove from existing stop words list\n",
    "         \n",
    "  \n",
    "    for w in all_stopwords:\n",
    "        pattern = r'\\b'+w+r'\\b'\n",
    "        text = re.sub(pattern,' ', text)\n",
    "                   \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words/phrase to be added to the stop words \n",
    "extra_sw = ['hsdes',\"testing\"]\n",
    "#list of words/phrase to be removed from stop words\n",
    "remove_sw = [\"i\",\"am\"]\n",
    "arg1 = extra_sw\n",
    "arg2 = remove_sw\n",
    "\n",
    "df[\"title_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"title_white\"]]\n",
    "df[\"desc_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"desc_white\"]]\n",
    "df[\"comments_stop\"]=  [remove_stopwords(text,extra_sw=arg1,remove_sw=arg2) for text in df[\"comments_white\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905229f9",
   "metadata": {},
   "source": [
    "### 5) Remove frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37464615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a389eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_freqwords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n frequent words\n",
    "    params:\n",
    "    column[series]: input column to remove frequent words\n",
    "    n [integer]: input number of frequent words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the frequent words             \n",
    "    FREQWORDS = set([w for (w, wc) in cnt.most_common(n)])\n",
    "    \n",
    "    print(\"Frequent words that are removed from column:\", set([(w, wc) for (w, wc) in cnt.most_common(n)]))\n",
    "    \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in FREQWORDS]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5dd44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_freq\"] = remove_freqwords(df[\"title_stop\"],n)\n",
    "df[\"desc_freq\"] = remove_freqwords(df[\"desc_stop\"],n)\n",
    "df[\"comments_freq\"] = remove_freqwords(df[\"comments_stop\"],n)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada84766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952af778",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ef266",
   "metadata": {},
   "source": [
    "### 6) Remove rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rarewords(column,n):\n",
    "    \"\"\"\n",
    "    Remove n rare words\n",
    "    params:\n",
    "    column[series]: input column to remove rare words\n",
    "    n [integer]: input number of rare words to be removed\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for text in column.values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "           \n",
    "    #custom function to remove the rare words             \n",
    "    RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n-1:-1]])\n",
    "    \n",
    "    print(\"Rare words that are removed from columns:\", set([(w,wc) for (w, wc) in cnt.most_common()[:-n-1:-1]]))\n",
    "        \n",
    "    return column.apply(lambda text: \" \".join([word for word in str(text).split() if word not in RAREWORDS]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff0508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=10\n",
    "df[\"title_rare\"] = remove_rarewords(df[\"title_stop\"],n)\n",
    "df[\"desc_rare\"] = remove_rarewords(df[\"desc_stop\"],n)\n",
    "df[\"comments_rare\"] = remove_rarewords(df[\"comments_stop\"],n)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5fd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,1] #converting is rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cba934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[903,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4224f",
   "metadata": {},
   "source": [
    "### c) Custom tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0821f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45e0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import re\n",
    "#remove token method - seperate nltk and split functions \n",
    "def cust_tokenization(column,token_met,token_type,delim =None):\n",
    "    \"\"\"\n",
    "    Custom tokenization, 2 options are available: split() or nltk \n",
    "    params:\n",
    "    df [dataframe]: input dataframe \n",
    "    token_met[\"string\"]: input tokenization method (\"split\" or \"nltk\")\n",
    "    \n",
    "    token_type[\"string\"](use only if token_met= \"nltk\"): type of nltk tokenization\n",
    "    a) token_type = \"WordToken\" tokenizes a string into a list of words\n",
    "    b) token_type = \"SentToken\" tokenizes a string containing sentences into a list of sentences\n",
    "    c) token_type = \"WhiteSpaceToken\" tokenizes a string on whitespace (space, tab, newline)\n",
    "    d) token_type = \"WordPunctTokenizer\" tokenizes a string on punctuations\n",
    "         \n",
    "    delim[\"string\"](use only if token_met = \"split\"): specify delimiter to separate strings,\n",
    "    default delimiter (delim=None) is whitespace,  an alternate option for token_type = \"WhiteSpaceToken\"\n",
    "    \n",
    "    \"\"\"\n",
    "    if token_met == \"split\":\n",
    "        if delim==None:\n",
    "            print(\"Text is split by space\") #default delimiter is space if not specified \n",
    "\n",
    "        else:\n",
    "            print(\"Text is split by:\", delim) #can accept one or more delimiter\n",
    "\n",
    "        return column.apply(lambda text: text.split() if delim==None else text.split(delim))\n",
    "    \n",
    "\n",
    "    if token_met == \"nltk\":\n",
    "    \n",
    "        if token_type == \"WordToken\":\n",
    "            tokenizer = word_tokenize\n",
    "        if token_type == \"SentToken\":\n",
    "            tokenizer = sent_tokenize\n",
    "        if token_type == \"WhiteSpaceToken\":\n",
    "            tokenizer = WhitespaceTokenizer().tokenize\n",
    "        if token_type == \"WordPunctTokenizer\":\n",
    "            tokenizer = WordPunctTokenizer().tokenize\n",
    "\n",
    "        return column.apply(lambda text: tokenizer(text))\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe926d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use split\n",
    "token_met=\"split\"\n",
    "token_type=None\n",
    "delim = None\n",
    "\n",
    "df[\"title_token\"]= cust_tokenization(column=df[\"title_stop\"],token_met=token_met,token_type=token_type,delim=delim)  \n",
    "df[\"desc_token\"]=  cust_tokenization(column=df[\"desc_stop\"],token_met=token_met,token_type=token_type,delim=delim) \n",
    "df[\"comments_token\"]= cust_tokenization(column=df[\"comments_stop\"],token_met=token_met,token_type=token_type,delim=delim)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1827f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nltk\n",
    "token_met=\"nltk\"\n",
    "token_type=\"WordToken\"\n",
    "delim = None\n",
    "\n",
    "df[\"title_token_nltk\"]= cust_tokenization(column=df[\"title_stop\"],token_met=token_met,token_type=token_type,delim=delim)  \n",
    "df[\"desc_token_nltk\"]=  cust_tokenization(column=df[\"desc_stop\"],token_met=token_met,token_type=token_type,delim=delim) \n",
    "df[\"comments_token_nltk\"]= cust_tokenization(column=df[\"comments_stop\"],token_met=token_met,token_type=token_type,delim=delim)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5013f586",
   "metadata": {},
   "source": [
    "## d) Custom taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4535b2",
   "metadata": {},
   "source": [
    "### i) Configurability for user to provide taxonomy mapping (to remove/remain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df897ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#rename tax to taxo\n",
    "def custom_tax(text,remove_tax,include_tax):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for w in remove_tax:\n",
    "        #row without any item from include_tax -> replace all remove_tax items with empty string\n",
    "        if all(phrase not in text for phrase in include_tax): \n",
    "            pattern = r'\\b'+w+r'\\b'\n",
    "            text = re.sub(pattern,' ', text) \n",
    "        #row with any item from include_tax -> only replace remove_tax item that is not in include_tax\n",
    "        else: \n",
    "            if all(w not in phrase for phrase in include_tax):\n",
    "                pattern = r'\\b'+w+r'\\b'\n",
    "                text = re.sub(pattern,' ', text) \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70a6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of words to remove\n",
    "remove_tax = [\"gio\",\"fields\",\"test\"]\n",
    "#list of words to maintain\n",
    "include_tax = [\"test suite execution\",\"clone defects\"]\n",
    "\n",
    "df[\"title_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"title_stop\"]]\n",
    "df[\"description_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"desc_stop\"]]\n",
    "df[\"comments_tax\"]=  [custom_tax(text,remove_tax,include_tax) for text in df[\"comments_stop\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd346a",
   "metadata": {},
   "source": [
    "### ii)  Named Entity Recognition (Methodology to recommend potential taxonomy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bbb05a",
   "metadata": {},
   "source": [
    "### Train custom NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d34b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"title_stop\",\"desc_stop\",\"comments_stop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import numpy as np\n",
    "#user to understand requirement - examples \n",
    "def convert_spacy(DATA):\n",
    "    \"\"\"\n",
    "    Convert  data into .spacy format\n",
    "    DATA[]: Train/validation data to be converted to .spacy format\n",
    "    \"\"\"\n",
    "    nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "    db = DocBin() # create a DocBin object\n",
    "\n",
    "    for text, annot in tqdm(DATA): # data in previous format\n",
    "        doc = nlp.make_doc(text) # create doc object from text\n",
    "        ents = []\n",
    "        for start, end, label in annot[\"entities\"]: # add character indexes\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span is None:\n",
    "                print(\"Skipping entity\")\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents # label the text with the ents\n",
    "        db.add(doc)\n",
    "        \n",
    "    return db\n",
    "\n",
    "    \n",
    "def custom_ner(TRAIN_DATA,VAL_DATA,path):\n",
    "    \"\"\"\n",
    "    Build and save custom NER model in given path. \n",
    "    \n",
    "    \"\"\"\n",
    "    #convert train and validation data into .spacy format\n",
    "    db_train = convert_spacy(TRAIN_DATA) \n",
    "    db_val = convert_spacy(VAL_DATA) \n",
    "    \n",
    "    #save train and validation data in .spacy format in path\n",
    "    db_train.to_disk(path +'train.spacy')\n",
    "    db_val.to_disk(path +'val.spacy')\n",
    "    \n",
    "    print(\"Train and validation converted to .spacy format and saved\")\n",
    "    \n",
    "    #autofill base_config file saved by user from spacy website\n",
    "    !python -m spacy init fill-config base_config.cfg config.cfg\n",
    "    \n",
    "    #Model building and saving in path\n",
    "    !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy\n",
    "    \n",
    "    print(\"Custom NER model built and saved!\")\n",
    "    \n",
    "def check_ents(path,column):\n",
    "    \"\"\"\n",
    "    Check entities after loading best model\n",
    "    \n",
    "    \"\"\"\n",
    "    #Load best model\n",
    "    nlp = spacy.load(path + \"/output/model-best/\")     \n",
    "    print(\"Best model loaded!\")\n",
    "    \n",
    "    entities = []\n",
    "    for text in column.tolist():\n",
    "        doc = nlp(text)\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent.text+' - '+ent.label_)\n",
    "    print(np.unique(np.array(entities)))        \n",
    "\n",
    "def ner_wrapper(TRAIN_DATA,VAL_DATA,path,column,train_model):  \n",
    "    \"\"\"\n",
    "    User can choose to train the spacy model or load spacy model\n",
    "    params:\n",
    "    TRAIN_DATA[NER format]: train data for model building\n",
    "    VAL_DATA[NER format]: validation data for model building\n",
    "    path[string]: input path to store model. Path has to be the same as base_config.cfg file downloaded from spacy\n",
    "                  website and jupyter notebook.\n",
    "    column[series]: column for entities to be checked\n",
    "    train_model[True/False]: True if want to train model. False to load model (no training)\n",
    "    \"\"\"\n",
    "    if train_model == True:\n",
    "        custom_ner(TRAIN_DATA,VAL_DATA,path)\n",
    "        check_ents(path,column)\n",
    "        \n",
    "    if train_model == False:\n",
    "        check_ents(path,column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train data\n",
    "TRAIN_DATA = [\n",
    "[\"jchun wai kit is working on this to enable in new tcp\", {\"entities\": [[0, 13, \"NAME\"]]}], \n",
    "[\"siewlita pending release\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi lim chih quanx per our communication i still have one more question\", {\"entities\": [[3, 17, \"NAME\"]]}],\n",
    "[\"yeetheng the auto test trigger after build complete is working fine today\", {\"entities\": [[0, 8, \"NAME\"]]}],\n",
    "[\"hi jon here is the recipe link weichuan hi can you try to reproduce the issue once more\", {\"entities\": [[3, 6, \"NAME\"],[31, 39, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "VAL_DATA = [\n",
    "[\"wei chuan has updated me with the sample of test execution by automation manual chart\", {\"entities\": [[0, 9, \"NAME\"]]}],\n",
    "[\"subject gio logs and gio installation hi ajay jonathan i just noticed that star is directing all the logs to gio folder\", {\"entities\": [[41, 45, \"NAME\"],[46, 55, \"NAME\"]]}],\n",
    "[\"hi firesh final verdict in jenkins coming as fail even after all the triggered tests are passed\", {\"entities\": [[3, 9, \"NAME\"],[27, 35, \"NAME\"]]}],\n",
    "[\"wai kit below is the requirement needed from gio product defect detection\", {\"entities\": [[0, 7, \"NAME\"]]}],\n",
    "[\"just string field regards robert nowicki\", {\"entities\": [[26, 40, \"NAME\"]]}]\n",
    "]\n",
    "\n",
    "#jupyter notebook and base_config.cfg path have to be the same\n",
    "path = \"C:/Users/nchong/\"\n",
    "\n",
    "#load and clean test data\n",
    "df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfcfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_wrapper(TRAIN_DATA,VAL_DATA,path,column=df_test[\"comments\"],train_model=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54740791",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec289e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def feature_extraction(column,ngram_range,ascending,fe_type):\n",
    "    \"\"\"\n",
    "    Feature extraction methods - Bag of words or TF-IDF\n",
    "    \n",
    "    params:\n",
    "    column [series]: column to select\n",
    "    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted\n",
    "                                       - ngram_range of (1, 1) means only unigrams, \n",
    "                                       - ngram_range of (1, 2) means unigrams and bigrams, \n",
    "                                       - ngram_range of (2, 2) means only bigram\n",
    "    ascending [True/False/None]: - None (words arranged in alphabetical order)\n",
    "                                 - True(words arranged in ascending order of sum), \n",
    "                                 - False(words arranged in descending order of sum)                               \n",
    "    fe_type[string]: Feature extraction type: Choose \"bagofwords\" or \"tfidf\" method\n",
    "    \"\"\"\n",
    "    \n",
    "    if fe_type == \"bagofwords\":\n",
    "        vec_type = CountVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0).astype(int)\n",
    "\n",
    "    if fe_type == \"tfidf\":\n",
    "        vec_type = TfidfVectorizer(ngram_range=ngram_range, analyzer='word')\n",
    "        vectorized = vec_type.fit_transform(column)\n",
    "        df = pd.DataFrame(vectorized.toarray(), columns=vec_type.get_feature_names())\n",
    "        df.loc['sum'] = df.sum(axis=0)\n",
    "    \n",
    "    if ascending != None:\n",
    "            \n",
    "        df = df.sort_values(by ='sum', axis = 1,ascending=ascending)\n",
    "    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5282542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df.iloc[:3,0]\n",
    "ngram_range = (1,1)\n",
    "ascending = None\n",
    "fe_type = \"bagofwords\"\n",
    "feature_extraction(column,ngram_range,ascending,fe_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df.iloc[:3,0]\n",
    "ngram_range = (1,1)\n",
    "ascending = True\n",
    "fe_type = \"tfidf\"\n",
    "feature_extraction(column,ngram_range,ascending,fe_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eea3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert and save train/validation data as .spacy\n",
    "# out_path = \"C:/Users/nchong/\"\n",
    "# db_train = convert_spacy(TRAIN_DATA)\n",
    "# db_train.to_disk(out_path +'train.spacy') # save the docbin object\n",
    "# db_val = convert_spacy(VAL_DATA)\n",
    "# db_val.to_disk(out_path +'val.spacy') # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89862c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./val.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best model\n",
    "# nlp1 = spacy.load(\"C:/Users/nchong/output/model-best/\") #load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43084cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp1(\"waikitcx hi arisha please provide us the\") # input sample text\n",
    "\n",
    "# spacy.displacy.render(doc, style=\"ent\", jupyter=True) # display in Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f0cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_ents(text):\n",
    "#     doc= nlp1(text)\n",
    "#     if doc.ents:\n",
    "#         for ent in doc.ents:\n",
    "#             return(ent.text+' - '+ent.label_)\n",
    "#     else:\n",
    "#         return('No named entities found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be688da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df_test = pd.read_excel(\"C:/Users/nchong/test.xlsx\",index_col=0)\n",
    "# df_test = df_manipulation(df_test,how=\"any\",keep=\"first\",cols_tokeep=[\"title\",\"description\",\"comments\"],cols_todrop=None,impute_value=\"\",subset=None)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
