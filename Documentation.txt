###---------------- A) Overview --------------------- ###
Consists of 3 modules: 1) Data loading, 2) Data Pre-processing, 3) ML

Framework will load the json data in the data loading module, followed by data preprocessing to clean and remove noise from the text data. With the cleaned data, user can choose to use 
a) unsupervised learning if user wants to classify text data into clusters/groups. Similar texts will be classified into the same cluster. Output will return the text and the cluster number that
   corresponds to the text. Methods that are available: K-means clustering (default), Latent Dirichlet Allocation (optional), Non Negative Matrix Factorization(optional)

b) supervised learning if the data has a target/label for prediction. The text data will be split into train and test. Model building will be done using the train data according to the algorithm
   chosen and evaluated on the test data. Output will return the overall accuracy/classification report(i.e. precision,support,recall,f1-score)/confusion matrix. Algorithms that are available:
   Random Forest(default), Support Vector Machine(optional), Naive Bayes(optional)

c) similarity metrics if user wants to compare the similarity between texts. Methods that are available: Cosine Similarity(default), Jaccard Similarity(optional)

###---------------- B) Details on modules and parameters used ---------------------###

#------1) Data Loading Module------#
Load only json files that follow the agreed filename format, merge files as single dataframe. User can choose to 
    a) Load all json files following the agreed filename format
    b) Load only json files from specific dates by adding the start and stop dates (Note: Both start_date and
    stop_date must be used together)
    
    params:
    path [string]: path of the files, without filename
    
    start_date[None/string in YYYY-MM-DD format](optional,default is None): 
    User can choose to load files starting from start_date
    - None: no start_date is provided, all files are loaded
    - string in YYYY-MM-DD format: files starting from start_date will be loaded
    
    stop_date[None/string in YYYY-MM-DD format](optional,default is None): 
    User can choose to load files until stop_date
    - None: no stop_date is provided, all files are loaded
    - string in YYYY-MM-DD format: files until stop_date will be loaded


#------2) Data Preprocessing Module------#
Remove noise and clean the text data. Consists of dataframe manipulation, text normalization, noise filtering, feature extraction

Functions available in the module:
  
A)  df_manipulation 
    DataFrame Manipulation  
    1) Column selection: Keep columns in dataframe
    2) Data impute: Impute NA rows with empty string
    3) Data duplication cleaning: Drop all duplicates or drop all duplicates except for the first/last occurrence
    params:
    df [dataframe]: input dataframe  
    how[None/string]: Drop rows when we have at least one NA or all NA. Choose
                      # - None : NA imputed with empty string
                      # - "all": Drop row with all NA
                      # - "any": Drop row with at least one NA
    col_selection [None/list]: - None [Default]: Keep all columns in dataframe 
                               - List: List of columns to keep in dataframe                      
                                 
    keep[None/string/False]: Choose to drop all duplicates or drop all duplicates except for the first/last occurrence
                      # - None[DEFAULT] : Drop duplicates except for the first occurrence. 
                      # - "last" : Drop duplicates except for the last occurrence. 
                      # - False : Drop all duplicates.                 
    subset[list/None]: list: Subset of columns for dropping NA and identifying duplicates, 
                       None[DEFAULT]: if no column to select

B) df_filterrows 
    User can choose to keep or drop row(s) in a column by providing the list of elements in the row(s)
    params:
    df [dataframe]: input dataframe 
    col[string]: input column name in which the row(s) within are to be kept/dropped
    keep_list[list/None]: list - input list of rows to be kept, None - when using drop_list
    drop_list[list/None]: list - input list of rows to be dropped, None - when using keep_list
    Example: To keep/drop the rows ['hw.sa', 'fw.qcode'] in the column â€œcomponent_affected"

C)  word_contractions
    Expand word contractions (i.e. "isn't" to "is not")
    params:
    df [dataframe]: input dataframe 

D)  lowercase
    Convert all characters to lower case
    params:
    df [dataframe]: input dataframe 

E)  remove_htmltag_url
    Remove html tag and url
    params:
    df [dataframe]: input dataframe 

F) custom_remtaxo
    User provides taxonomy to be removed from the text. 
    a) user wants to remove taxonomies only -> input a list of taxonomies or regex to be removed in remove_taxo 
    b) user wants to remove taxonomies but wants the same taxonomy to remain in certain phrases 
    (i.e remove "test" from text but want "test" to remain in "test cycle") -> input a list of taxonomies or regex to be removed in remove_taxo 
    and list of phrases for the taxonomy to remain in include_taxo
    params:
    df [dataframe]: input dataframe
    remove_taxo[list/regex]: list of taxonomies or regex(i.e. r'test \w+') to be removed from text 
    include_taxo[list/None](optional): list of phrases for the taxonomy to remain in 

G) custom_keeptaxo
    User provides taxonomy to be kept in the text, the rest of the taxonomy will be omitted. User can choose to do one 
    or multi stage filtering on the taxonomy
    df[DataFrame]: input dataframe
    keep_taxo[regex/list of regex]: use regex/list of regex to filter the taxonomy to keep
        regex : only one regex in keep_taxo to filter the taxonomy; i.e "[\w+\.]+\w+@intel.com"
        list of regex: use list of regex to filter the taxonomy for multi stage filter 
        i.e. ["[\w+\.]+\w+@intel.com","@intel.com","@intel","intel"]

H)  remove_irrchar_punc
    Remove irrelevant characters and punctuation. Optional: User can specify special characters to be removed in regex format  
    params:    
    df [dataframe]: input dataframe 
    char[string/None]: input regex of characters to be removed

I)  remove_num
    Remove numeric data
    params:
    df [dataframe]: input dataframe 

J)  remove_multwhitespace
    Remove multiple white spaces
    params:
    df [dataframe]: input dataframe 

K)  remove_stopwords
    Removes English stopwords. Optional: user can add own stopwords or remove words from English stopwords  
    params:
    df [dataframe]: input dataframe 
    extra_sw [list] (optional): list of words/phrase to be added to the stop words 
    remove_sw [list] (optional): list of words to be removed from the stop words 

L)  remove_freqwords
    Remove n frequent words
    params:
    df [dataframe]: input dataframe 
    n [integer]: input number of frequent words to be removed

M)  remove_rarewords
    Remove n rare words
    params:
    df [dataframe]: input dataframe 
    n [integer]: input number of rare words to be removed

N)  stem_words
    Stemming words. Default option is Porter Stemmer, alternative option is Lancaster Stemmer 
    params:
    df [dataframe]: input dataframe 
    stemmer_type[None/string]: input stemming method 
                                - None for Porter Stemmer
                                - "Lancaster" for Lancaster Stemmer 

O)  lemmatize_words
    Lemmatize words: Default option is WordNetLemmatizer, alternative option is Spacy 
    params:
    column[series]: input series/column to be lemmatized
    lemma_type[None/string]: input lemmatization method
                            - None for WordNetLemmatizer
                            - "Spacy" for Spacy    

P)  feature_extraction 
    Feature extraction methods - TF-IDF[DEFAULT] or Bag of words
    params:
    column [series/DataFrame]: column selected for feature extraction 
                        - series: only one column is selected for feature extraction (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected for feature extraction (e.g. df[["title_clean","desc_clean"]])
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                       - [DEFAULT] ngram_range of (1, 1) means only unigrams, 
                                       - ngram_range of (1, 2) means unigrams and bigrams, 
                                       - ngram_range of (2, 2) means only bigram
    ascending [True/False/None]: - [DEFAULT] None (words arranged in alphabetical order)
                                 - True(words arranged in ascending order of sum), 
                                 - False(words arranged in descending order of sum)                               
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method    


#------3) ML Module------#
Consists of unsupervised learning, supervised learning and similarity metric methods.

a) Unsupervised learning

i)  kmeans_clustering (default method)
    K- means clustering for unsupervised learning. User can choose either options:
    (1) provide the number of clusters or
    (2) provide the max number of clusters for kmeans to iterate through, the optimal number of clusters with highest 
    silhouette score will be chosen. Min number of clusters is fixed as 2
    Returns:
    a) Top no of terms(top_n_terms) associated with each cluster 
    b) Dataframe with ID,raw text and cluster (in interface.py)
    
    params:
    column [series/DataFrame]: column(s) selected for clustering 
                        - series: only one column is selected for clustering (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected for clustering (e.g. df[["title_clean","desc_clean"]])
    outpath[string]: path to write the outputs
    top_n_terms[int]: the top n terms in each cluster to be printed out
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                   - [DEFAULT] ngram_range of (1, 1) means only unigrams, 
                                   - ngram_range of (1, 2) means unigrams and bigrams, 
                                   - ngram_range of (2, 2) means only bigram
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method    
    n_clusters[None/int]: number of clusters. Choose None for option (2)  
    max_n_clusters[None/int]: max number of clusters. Choose None for option (1)  
    token_pattern[regex/None]: None: default regexp select tokens of 2 or more alphanumeric characters 
                               (punctuation is completely ignored and always treated as a token separator).
                               regex: Regular expression denoting what constitutes a â€œtoken"

ii) lda (optional)
    LDA for unsupervised learning. Bag of words is selected for feature extraction
    Returns:
    a) Top no of terms(top_n_terms) associated with each cluster 
    b) Dataframe with ID,raw text and cluster (in interface.py)    
    params:
    column [series/DataFrame]: column(s) selected for lda
                        - series: only one column is selected for lda (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected for lda (e.g. df[["title_clean","desc_clean"]])
    outpath[string]: path to write the outputs
    n_components[int]: the number of topics/clusters used in the lda_model
    top_n_terms[int]: the top n terms in each topic/cluster to be printed out
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                   - [default] ngram_range of (1, 1) means only unigrams, 
                                   - ngram_range of (1, 2) means unigrams and bigrams, 
                                   - ngram_range of (2, 2) means only bigram
    token_pattern[regex/None]: None: default regexp select tokens of 2 or more alphanumeric characters 
                               (punctuation is completely ignored and always treated as a token separator).
                               regex: Regular expression denoting what constitutes a â€œtoken"


iii)nmf (optional)  
    Non-negative matrix factorization for unsupervised learning.
    Returns:
    a) Top no of terms(top_n_terms) associated with each cluster 
    b) Dataframe with ID,raw text and cluster (in interface.py)
    params:
    column [series/DataFrame]: column(s) selected for NMF 
                        - series: only one column is selected for NMF (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected for NMF (e.g. df[["title_clean","desc_clean"]])
    outpath[string]: path to write the outputs
    n_components[int]: the number of topics/clusters used in NMF
    top_n_terms[int]: the top n terms in each topic/cluster to be printed out
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method    
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                   - [default] ngram_range of (1, 1) means only unigrams, 
                                   - ngram_range of (1, 2) means unigrams and bigrams, 
                                   - ngram_range of (2, 2) means only bigram
    token_pattern[regex/None]: None: default regexp select tokens of 2 or more alphanumeric characters 
                               (punctuation is completely ignored and always treated as a token separator).
                               regex: Regular expression denoting what constitutes a â€œtoken"


b) Supervised learning
i) supervised_lng
   Consists of 3 supervised machine learning methods: RandomForest (Default), Naive Bayes(optional, SVM (optional)
   Returns .joblib model, overall accuracy, classification report, confusion matrix                                                                                       
   params:
   df[DataFrame]: input dataframe
   user_outpath[string]: path to write output for user
   target[string]: label of data
   test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. 
                          If int, represents the absolute number of test samples.
   ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                       -[DEFAULT] ngram_range of (1, 1) means only unigrams, 
                                       - ngram_range of (1, 2) means unigrams and bigrams, 
                                       - ngram_range of (2, 2) means only bigram
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method    
   model_type[None/string]: Choose ML algorithm 
                            - None (Default algorithm is Random Forest)
                            - 'NB'(To choose Naive Bayes as ML algorithm), 
                            - 'SVM'(To choose Support Vector Machine as ML algorithm)
   ascend[True/False/None]:  - None (Default: Confusion matrix is arranged in alphabetical order)
                              - True(Confusion matrix arranged in ascending order of accuracy % per label), 
                              - False(Confusion matrix arranged in descending order of accuracy % per label)      

ii) deep_lng
    Deep learning method: MultiLayer Perceptron
    Returns .joblib model, overall accuracy, classification report, confusion matrix
    params:
    df[DataFrame]: input dataframe
    user_outpath[string]: path to write output for user
    target[string]: label of data
    test_size[float/int]: If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. 
                          If int, represents the absolute number of test samples.
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                       - ngram_range of (1, 1) means only unigrams, 
                                       - ngram_range of (1, 2) means unigrams and bigrams, 
                                       - ngram_range of (2, 2) means only bigram
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method    
    hidden_layer_sizes[tuple],default(None) = (100): To set the number of layers and the number of nodes.
                                               Each element in the tuple represents the number of nodes,
                                               length of tuple denotes the total number of hidden layers in the network
    activation["identity", "logistic", "tanh","relu"], default(None)= "relu": Activation function for the hidden layer.
    solver["lbfgs", "sgd", "adam"], default(None):"adam": The solver for weight optimization.
    learning_rate["constant", "invscaling", "adaptive"], default(None)= "constant": Learning rate schedule for weight updates
    max_iter[int], default(None)= 200: Maximum number of iterations. The solver iterates until convergence or this number of iterations.
    ascend [True/False/None]: - None (Default: Confusion matrix is arranged in alphabetical order)
                                 - True(Confusion matrix arranged in ascending order of accuracy % per label), 
                                 - False(Confusion matrix arranged in descending order of accuracy % per label)                            

c) Similarity Metrics
i) cosinesimilarity
    Compute the cosine similarity between rows of texts. User can 
    a) fix number of rows for comparison, each row will be taken as base and compared with the rest
    b) fix one row as base, comparison will be done with all the other rows
    Returns:
    Dataframe with base index/row, index/row, similarity score, text 
    params:    
    column[series/DataFrame]: column(s) of text for row wise similarity comparison
                        - series: only one column is selected (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected(e.g. df[["title_clean","desc_clean"]])  
    user_outpath[string]: path to write output for user
    threshold[None/float]: cut off value for the cosine similarity, only texts with values above or equal to threshold
                           will be printed
                        - None: Default threhold is 0.5
                        - float: any value between 0 and 1 
    total_rows[None/int]: Number of rows for comparison, choose None for option b 
    base_row[None/int]: Row fixed as base, choose None for option a 
    ngram_range [tuple(min_n, max_n)]: The lower and upper boundary of the range of n-values for different n-grams to be extracted
                                       -[DEFAULT] ngram_range of (1, 1) means only unigrams, 
                                       - ngram_range of (1, 2) means unigrams and bigrams, 
                                       - ngram_range of (2, 2) means only bigram
    fe_type[string/None]: Feature extraction type: Choose "bagofwords" for bow or None for default tfidf method      
    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)
                                 - True(words arranged in ascending order of sum), 
                                 - False(words arranged in descending order of sum)
  
ii) jaccard_similarity
    Compute the jaccard similarity between texts. User can 
    a) fix number of rows for comparison, each row will be taken as base and compared with the rest
    b) fix one row as base, comparison will be done with all the other rows
    Returns:
    Dataframe with base index/row, index/row, similarity score, text
    params:
    column[series/DataFrame]: column(s) of text for row wise similarity comparison
                        - series: only one column is selected (e.g. df["title_clean"])
                        - DataFrame: more than one column is selected(e.g. df[["title_clean","desc_clean"]]) 
    user_outpath[string]: path to write output for user
    threshold[None/float]: cut off value for the jaccard similarity, only texts with values above or equal to threshold
                           will be printed
                        - None: Default threhold is 0.5
                        - float: any value between 0 and 1 
    total_rows[None/int]: Number of rows for comparison, choose None for option b 
    base_row[None/int]: Row fixed as base, choose None for option a 
    ascending [True/False/None]: - [default] None (words arranged in alphabetical order)
                                 - True(words arranged in ascending order of sum), 
                                 - False(words arranged in descending order of sum)  